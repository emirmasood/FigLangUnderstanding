{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"gpuType":"T4"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU","widgets":{"application/vnd.jupyter.widget-state+json":{"c94c49bf5dca4e4caee8df37d3a8a394":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_8c2f8c3e116848eeb54e0b955f8dfd32","IPY_MODEL_9dfedeba1ecd41d3843aec98b4aad192","IPY_MODEL_41d8f01151a041daa60abb9acf7f9e44"],"layout":"IPY_MODEL_2dca123f15d944658021fb9bda04f7d3"}},"8c2f8c3e116848eeb54e0b955f8dfd32":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_46d6534a813543a6a3cde4f998ee8549","placeholder":"​","style":"IPY_MODEL_b76e507b035b4c159a8fe12c1c01f10f","value":"tokenizer_config.json: 100%"}},"9dfedeba1ecd41d3843aec98b4aad192":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_e92d12006b244545aa7edbe51067e706","max":25,"min":0,"orientation":"horizontal","style":"IPY_MODEL_a68f987461864dd98677a14150fc7838","value":25}},"41d8f01151a041daa60abb9acf7f9e44":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_39676294dc004421a22e66a1fcbe2caa","placeholder":"​","style":"IPY_MODEL_7f0847ad60064d109b92d771a4b0dbcf","value":" 25.0/25.0 [00:00&lt;00:00, 2.71kB/s]"}},"2dca123f15d944658021fb9bda04f7d3":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"46d6534a813543a6a3cde4f998ee8549":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"b76e507b035b4c159a8fe12c1c01f10f":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"e92d12006b244545aa7edbe51067e706":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"a68f987461864dd98677a14150fc7838":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"39676294dc004421a22e66a1fcbe2caa":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"7f0847ad60064d109b92d771a4b0dbcf":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"8b9dac4cbe234704ad7f77e9578c2655":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_4ee4762afac94577bdcc259f22286ad1","IPY_MODEL_6f6fdc3e296c4c7ba25a771e942b6714","IPY_MODEL_160485e1dc634920a205b392a396e7d1"],"layout":"IPY_MODEL_eef27e558f3542449195eec33c1b7e40"}},"4ee4762afac94577bdcc259f22286ad1":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_761a897316704e89a00996d0fd733821","placeholder":"​","style":"IPY_MODEL_90dd5089c7674bea9c3059ffa960195a","value":"config.json: 100%"}},"6f6fdc3e296c4c7ba25a771e942b6714":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_2da1ab38872449eca6a71616b4014b11","max":481,"min":0,"orientation":"horizontal","style":"IPY_MODEL_d86a3e64d4444dc6babe946bf075a89f","value":481}},"160485e1dc634920a205b392a396e7d1":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_87a06d0b6cdb442ba283bfa47ff91906","placeholder":"​","style":"IPY_MODEL_c77167756489491098f485a468014872","value":" 481/481 [00:00&lt;00:00, 43.9kB/s]"}},"eef27e558f3542449195eec33c1b7e40":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"761a897316704e89a00996d0fd733821":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"90dd5089c7674bea9c3059ffa960195a":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"2da1ab38872449eca6a71616b4014b11":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"d86a3e64d4444dc6babe946bf075a89f":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"87a06d0b6cdb442ba283bfa47ff91906":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"c77167756489491098f485a468014872":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"40c891e0faac4a02b09d9f058d066bda":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_2c3e7ae00b464da18e811dc7fa9ab027","IPY_MODEL_f5ee4af0bcfd435899737648e3c55c13","IPY_MODEL_5e15ee0cd4514d16b9f478fe5ce48597"],"layout":"IPY_MODEL_e80c4aab2488428281bbcf2e0d68118a"}},"2c3e7ae00b464da18e811dc7fa9ab027":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_d6e5411763a947bd98801f9c9da1c1ee","placeholder":"​","style":"IPY_MODEL_1116d9266efa4dc88848916f56bc1532","value":"vocab.json: 100%"}},"f5ee4af0bcfd435899737648e3c55c13":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_d0955954439f42d0a3376e6332cad65d","max":898823,"min":0,"orientation":"horizontal","style":"IPY_MODEL_539b11acd22c4070a1cb0cf0f24c6c92","value":898823}},"5e15ee0cd4514d16b9f478fe5ce48597":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_4ef0dcd2d4ab4d60a768efb6563fa527","placeholder":"​","style":"IPY_MODEL_8816092e7abb4c4b9cd6e2f9d03dcc68","value":" 899k/899k [00:00&lt;00:00, 1.46MB/s]"}},"e80c4aab2488428281bbcf2e0d68118a":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"d6e5411763a947bd98801f9c9da1c1ee":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"1116d9266efa4dc88848916f56bc1532":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"d0955954439f42d0a3376e6332cad65d":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"539b11acd22c4070a1cb0cf0f24c6c92":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"4ef0dcd2d4ab4d60a768efb6563fa527":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"8816092e7abb4c4b9cd6e2f9d03dcc68":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"afd6356d025748569962fad4f5f4a13b":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_66cc007e1b744751acdf862c4236b431","IPY_MODEL_b05bd2d6d14043d8b6c063126d20a83b","IPY_MODEL_a2818597e0d04cc18e193959ad9f66f4"],"layout":"IPY_MODEL_aaffe9b72ceb434a8bf9091c4bf831c9"}},"66cc007e1b744751acdf862c4236b431":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_c0c958e29efb44b0b96964e8f9e1cd99","placeholder":"​","style":"IPY_MODEL_e7f305a8e7c645bbaa83c67c670b2da6","value":"merges.txt: 100%"}},"b05bd2d6d14043d8b6c063126d20a83b":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_94a124ae3f124d00aa09ea750bd43a9d","max":456318,"min":0,"orientation":"horizontal","style":"IPY_MODEL_c7254b7d5f964f5489e148c002fde347","value":456318}},"a2818597e0d04cc18e193959ad9f66f4":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_fec4089d20e9420daf8e43e500159f9a","placeholder":"​","style":"IPY_MODEL_15d318ff62084c11b5e7383bfce14189","value":" 456k/456k [00:00&lt;00:00, 784kB/s]"}},"aaffe9b72ceb434a8bf9091c4bf831c9":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"c0c958e29efb44b0b96964e8f9e1cd99":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"e7f305a8e7c645bbaa83c67c670b2da6":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"94a124ae3f124d00aa09ea750bd43a9d":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"c7254b7d5f964f5489e148c002fde347":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"fec4089d20e9420daf8e43e500159f9a":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"15d318ff62084c11b5e7383bfce14189":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"344190806c9c4b28a6c62c13af134a12":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_1eb3c8e889fc43d398df2d43e2071753","IPY_MODEL_e9d86d3f53c7487b9009b11b35b92ba3","IPY_MODEL_48e9a41fecfc458884c25e64a5befee7"],"layout":"IPY_MODEL_2b1e4f4570844bd689e3cd5b1d13471f"}},"1eb3c8e889fc43d398df2d43e2071753":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_97cc99a694494024b10ae84f58b04855","placeholder":"​","style":"IPY_MODEL_35102bfe19fa4a658b272e1b63b2e415","value":"tokenizer.json: 100%"}},"e9d86d3f53c7487b9009b11b35b92ba3":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_658274319e294b55a190ea78d9a277b2","max":1355863,"min":0,"orientation":"horizontal","style":"IPY_MODEL_fb961a366dd047db936004f0be110530","value":1355863}},"48e9a41fecfc458884c25e64a5befee7":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_549aa5dc1c2446109f5457a999b65237","placeholder":"​","style":"IPY_MODEL_c8aec08919914735b207afb3558b93d0","value":" 1.36M/1.36M [00:00&lt;00:00, 2.57MB/s]"}},"2b1e4f4570844bd689e3cd5b1d13471f":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"97cc99a694494024b10ae84f58b04855":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"35102bfe19fa4a658b272e1b63b2e415":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"658274319e294b55a190ea78d9a277b2":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"fb961a366dd047db936004f0be110530":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"549aa5dc1c2446109f5457a999b65237":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"c8aec08919914735b207afb3558b93d0":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"cbfc6f752a5b46eeb25992f04724f7c0":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_bfebb93bb62a447ea83dc7ae587d2835","IPY_MODEL_4d3912f180b5492f9caff2128fe901c6","IPY_MODEL_2baade8cb423411c8d03038bee3bb104"],"layout":"IPY_MODEL_d673ec4e8cb54d47a2d42e9f6bd3cd63"}},"bfebb93bb62a447ea83dc7ae587d2835":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_71dbe99615234e088e7836cb84fc9cd8","placeholder":"​","style":"IPY_MODEL_d53d691456b54d59a05d917e0eb4bc6e","value":"model.safetensors: 100%"}},"4d3912f180b5492f9caff2128fe901c6":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_ff80d61eeb3b4a6c8b5da2512f702b7d","max":498818054,"min":0,"orientation":"horizontal","style":"IPY_MODEL_f00ea1093cd64fe1be032cc4fd952bd2","value":498818054}},"2baade8cb423411c8d03038bee3bb104":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_8dba97264e2d48c5b21f55d1c32480ec","placeholder":"​","style":"IPY_MODEL_d29401ccd6ad486385199aae5b2e149d","value":" 499M/499M [00:05&lt;00:00, 110MB/s]"}},"d673ec4e8cb54d47a2d42e9f6bd3cd63":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"71dbe99615234e088e7836cb84fc9cd8":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"d53d691456b54d59a05d917e0eb4bc6e":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"ff80d61eeb3b4a6c8b5da2512f702b7d":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"f00ea1093cd64fe1be032cc4fd952bd2":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"8dba97264e2d48c5b21f55d1c32480ec":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"d29401ccd6ad486385199aae5b2e149d":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}}}}},"cells":[{"cell_type":"markdown","metadata":{"id":"97009911"},"source":["# Phase 3 - RoBERTa Extension - Mixture of Adapters\n","\n","## Goal\n","To train and evaluate a Mixture-of-Adapters (MoE) model on figurative language tasks (Sentiment, Sarcasm), analyzing performance across different English varieties.\n","\n","## Dataset + Evaluation Protocol\n","*   **Tasks**: Sentiment Analysis, Sarcasm Detection.\n","*   **Varieties**: en-AU (Australia), en-IN (India), en-UK (United Kingdom), etc.\n","*   **Sources**: Google, Reddit, Twitter, etc.\n","*   **Data**: Loaded via `index_settings.csv` and `index_testsets.csv`.\n","\n","## Notebook Workflow\n","1.  **Setup**: Imports, Drive mount, Seeds.\n","2.  **Config**: Define hyperparameters (MoE, LR, Epochs).\n","3.  **Data Loading**: Parse index CSVs.\n","4.  **Model**: Define RoBERTa + MoE Adapters + Router.\n","5.  **Training**: Two-stage protocol (Stage 1: Pooled Pretraining, Stage 2: Specialized Adaptation).\n","6.  **Evaluation**: Compute F1/Acc, compare vs CE baseline, analyze errors.\n","7.  **Visualization**: Plot heatmaps and locale-specific bars.\n","\n","## Outputs\n","*   **Metrics**: `models/.../metrics/moe_metrics_all.csv`\n","*   **Predictions**: `models/.../predictions/moe_predictions_all.csv`\n","*   **Plots**: `models/.../figures/*.png`\n","*   **Checkpoints**: `models/.../checkpoints/*.pt`\n","\n","## Reproduction\n","*   **Seed**: 42\n","*   **Model**: `roberta-base`\n","*   **Config**: Defined in Step 2 (`CFG`)."]},{"cell_type":"markdown","metadata":{"id":"1acd2569"},"source":["## Step 1 — Setup\n","*   **Purpose**: Initialize environment, imports, random seeds, and drive paths.\n","*   **Inputs**: Google Drive path (`/content/drive/MyDrive/DNLP`).\n","*   **Outputs**: `DEVICE`, `BASE` path.\n","*   **Assumptions**: Mounts Google Drive if available."]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"RJAemAArRMWV","executionInfo":{"status":"ok","timestamp":1769811092894,"user_tz":-60,"elapsed":46452,"user":{"displayName":"DNLP Project","userId":"02984253218627728392"}},"outputId":"c2ce4eb6-ecbc-4b78-dbdd-92189778a01f"},"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive\n","✅ Added SRC_DIR to sys.path: /content/drive/MyDrive/DNLP/src\n","BASE  = /content/drive/MyDrive/DNLP\n","DEVICE= cuda\n"]}],"source":["# ==== Cell 1: Setup ====\n","import os, random, json, time, math\n","from pathlib import Path\n","\n","import numpy as np\n","import pandas as pd\n","\n","import torch\n","import torch.nn as nn\n","import torch.nn.functional as F\n","from torch.utils.data import Dataset, DataLoader\n","\n","from transformers import AutoTokenizer, AutoModel\n","from sklearn.metrics import accuracy_score, precision_recall_fscore_support\n","\n","# (Colab) mount drive if available\n","try:\n","    from google.colab import drive\n","    if not Path(\"/content/drive\").exists():\n","        drive.mount(\"/content/drive\")\n","except Exception as e:\n","    print(\"Colab drive mount skipped:\", repr(e))\n","\n","# ---------\n","# Paths\n","# ---------\n","BASE = Path(\"/content/drive/MyDrive/DNLP\")\n","assert BASE.exists(), f\"BASE not found: {BASE}\"\n","\n","# Add src to path if exists (hybrid style)\n","import sys\n","SRC_DIR = BASE / \"src\"\n","if SRC_DIR.exists():\n","    sys.path.append(str(SRC_DIR))\n","    print(\"✅ Added SRC_DIR to sys.path:\", SRC_DIR)\n","\n","# ---------\n","# Repro\n","# ---------\n","def set_seed(seed=42):\n","    random.seed(seed)\n","    np.random.seed(seed)\n","    torch.manual_seed(seed)\n","    torch.cuda.manual_seed_all(seed)\n","\n","set_seed(42)\n","\n","DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n","print(\"BASE  =\", BASE)\n","print(\"DEVICE=\", DEVICE)"]},{"cell_type":"markdown","metadata":{"id":"88d88a61"},"source":["## Step 2 — Config + run folders\n","*   **Purpose**: Define configuration dictionary (hyperparameters) and create output directories.\n","*   **Inputs**: `BASE` path.\n","*   **Outputs**: `CFG` dict, directories (`CKPT_DIR`, `MET_DIR`, etc.).\n","*   **Assumptions**: Uses `roberta-base`."]},{"cell_type":"code","source":["# ==== Cell 2: Config + run folders ====\n","\n","RUN_NAME = f\"roberta_extension_mixture_of_adapters\"\n","\n","RUN_DIR = BASE / \"models\" / RUN_NAME\n","CKPT_DIR = RUN_DIR / \"checkpoints\"\n","MET_DIR  = RUN_DIR / \"metrics\"\n","PRD_DIR  = RUN_DIR / \"predictions\"\n","PLT_DIR  = RUN_DIR / \"figures\"\n","ANA_DIR  = RUN_DIR / \"analysis\"\n","\n","for d in [CKPT_DIR, MET_DIR, PRD_DIR, PLT_DIR, ANA_DIR]:\n","    d.mkdir(parents=True, exist_ok=True)\n","\n","CFG = {\n","    # Model\n","    \"MODEL_NAME\": \"roberta-base\",\n","    \"MAX_LEN\": 256,\n","\n","    # Train\n","    \"BATCH_SIZE\": 16,\n","    \"LR\": 2e-5,\n","    \"WEIGHT_DECAY\": 0.01,\n","    \"EPOCHS\": 6,\n","    \"PATIENCE\": 2,\n","    \"WARMUP_RATIO\": 0.06,\n","    \"GRAD_ACCUM\": 1,\n","    \"USE_AMP\": True,\n","    \"NUM_WORKERS\": 2,\n","\n","    # MoE (mixture of adapters)\n","    \"N_EXPERTS\": 3,\n","    \"ADAPTER_BOTTLENECK\": 128,\n","    \"ROUTER_HIDDEN\": 128,\n","    \"ADAPTER_DROPOUT\": 0.10,\n","\n","    # Regularizers to prevent collapse\n","    \"LOAD_BAL_W\": 0.02,     # small\n","    \"ENTROPY_W\": 0.01,      # small, encourages higher entropy (we subtract it from loss)\n","\n","    # Optional (keep off unless you explicitly want it)\n","    \"EXPERT_L2_REG\": 0.0,\n","    \"ROUTER_SUP_W\": 0.0,              # keep OFF (you moved away from it)\n","    \"ROUTER_LABEL_SMOOTH\": 0.05,\n","    \"ROUTER_SUP_DROPOUT_P\": 0.30,\n","\n","    # Decision rule (match CE baseline)\n","    \"FIXED_THRESHOLD\": 0.5,\n","\n","    # Two-stage protocol toggles\n","    \"FREEZE_ROUTER_STAGE2\": True,\n","    \"FREEZE_BACKBONE_STAGE2\": False,  # set True if you want faster but potentially weaker\n","}\n","\n","VARIANT = \"roberta_extension_mixture_of_adapters\"\n","\n","print(\"RUN_DIR =\", RUN_DIR)\n","print(\"VARIANT =\", VARIANT)\n","print(\"CFG:\", json.dumps(CFG, indent=2))"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"igEoLZlcTpiT","executionInfo":{"status":"ok","timestamp":1769811095533,"user_tz":-60,"elapsed":379,"user":{"displayName":"DNLP Project","userId":"02984253218627728392"}},"outputId":"156dcf32-6ebc-47d1-bc00-ee9457b6e526"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["RUN_DIR = /content/drive/MyDrive/DNLP/models/roberta_extension_mixture_of_adapters\n","VARIANT = roberta_extension_mixture_of_adapters\n","CFG: {\n","  \"MODEL_NAME\": \"roberta-base\",\n","  \"MAX_LEN\": 256,\n","  \"BATCH_SIZE\": 16,\n","  \"LR\": 2e-05,\n","  \"WEIGHT_DECAY\": 0.01,\n","  \"EPOCHS\": 6,\n","  \"PATIENCE\": 2,\n","  \"WARMUP_RATIO\": 0.06,\n","  \"GRAD_ACCUM\": 1,\n","  \"USE_AMP\": true,\n","  \"NUM_WORKERS\": 2,\n","  \"N_EXPERTS\": 3,\n","  \"ADAPTER_BOTTLENECK\": 128,\n","  \"ROUTER_HIDDEN\": 128,\n","  \"ADAPTER_DROPOUT\": 0.1,\n","  \"LOAD_BAL_W\": 0.02,\n","  \"ENTROPY_W\": 0.01,\n","  \"EXPERT_L2_REG\": 0.0,\n","  \"ROUTER_SUP_W\": 0.0,\n","  \"ROUTER_LABEL_SMOOTH\": 0.05,\n","  \"ROUTER_SUP_DROPOUT_P\": 0.3,\n","  \"FIXED_THRESHOLD\": 0.5,\n","  \"FREEZE_ROUTER_STAGE2\": true,\n","  \"FREEZE_BACKBONE_STAGE2\": false\n","}\n"]}]},{"cell_type":"markdown","metadata":{"id":"f5ffaf3e"},"source":["## Step 3 — Load indices for a task\n","*   **Purpose**: Load train/test split metadata (CSV paths, settings) for specific tasks.\n","*   **Inputs**: `data/processed/{task}/index_settings.csv` and `index_testsets.csv`.\n","*   **Outputs**: `settings_df` (train settings), `testsets_df` (test sets).\n","*   **Assumptions**: CSV files exist in the standard processed data structure."]},{"cell_type":"code","source":["# ==== Cell 3: Load indices for a task (index_settings + index_testsets) ====\n","\n","def resolve_csv(p: str) -> Path:\n","    p = Path(p)\n","    return p if p.is_absolute() else (BASE / p)\n","\n","def load_task_indices(task: str):\n","    task_dir = BASE / \"data\" / \"processed\" / task\n","    INDEX_SETTINGS = task_dir / \"index_settings.csv\"\n","    INDEX_TESTSETS = task_dir / \"testsets\" / \"index_testsets.csv\"\n","\n","    assert INDEX_SETTINGS.exists(), f\"Missing: {INDEX_SETTINGS}\"\n","    assert INDEX_TESTSETS.exists(), f\"Missing: {INDEX_TESTSETS}\"\n","\n","    settings_df = pd.read_csv(INDEX_SETTINGS)\n","    testsets_df = pd.read_csv(INDEX_TESTSETS)\n","\n","    settings_df.columns = [c.strip().lower() for c in settings_df.columns]\n","    testsets_df.columns  = [c.strip().lower() for c in testsets_df.columns]\n","\n","    # compat renames\n","    if \"setting\" in settings_df.columns and \"train_setting\" not in settings_df.columns:\n","        settings_df.rename(columns={\"setting\": \"train_setting\"}, inplace=True)\n","    if \"csv\" in testsets_df.columns and \"test_csv\" not in testsets_df.columns:\n","        testsets_df.rename(columns={\"csv\": \"test_csv\"}, inplace=True)\n","\n","    need_s = {\"train_setting\",\"train_csv\",\"val_csv\"}\n","    need_t = {\"test_setting\",\"test_csv\"}\n","    assert need_s.issubset(set(settings_df.columns)), f\"{task}: settings_df missing {need_s - set(settings_df.columns)}\"\n","    assert need_t.issubset(set(testsets_df.columns)), f\"{task}: testsets_df missing {need_t - set(testsets_df.columns)}\"\n","\n","    settings_df[\"train_csv_abs\"] = settings_df[\"train_csv\"].apply(resolve_csv)\n","    settings_df[\"val_csv_abs\"]   = settings_df[\"val_csv\"].apply(resolve_csv)\n","    testsets_df[\"test_csv_abs\"]  = testsets_df[\"test_csv\"].apply(resolve_csv)\n","\n","    # sanity\n","    for p in settings_df[\"train_csv_abs\"].tolist() + settings_df[\"val_csv_abs\"].tolist() + testsets_df[\"test_csv_abs\"].tolist():\n","        assert Path(p).exists(), f\"Missing CSV: {p}\"\n","\n","    return settings_df, testsets_df\n","\n","# quick peek\n","for t in [\"sentiment\", \"sarcasm\"]:\n","    s_df, te_df = load_task_indices(t)\n","    print(f\"\\n[{t}] settings:\", len(s_df), \"testsets:\", len(te_df))\n","    print(\"settings head:\", s_df[\"train_setting\"].head(6).tolist())\n","    print(\"testsets head:\", te_df[\"test_setting\"].head(6).tolist())"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"r1SPM6K1UDSV","executionInfo":{"status":"ok","timestamp":1769811105761,"user_tz":-60,"elapsed":8834,"user":{"displayName":"DNLP Project","userId":"02984253218627728392"}},"outputId":"50297d82-6319-485c-b335-1744719cecd9"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["\n","[sentiment] settings: 5 testsets: 6\n","settings head: ['Google', 'Reddit', 'TRAIN_en-AU', 'TRAIN_en-IN', 'TRAIN_en-UK']\n","testsets head: ['TEST_FULL', 'TEST_Google', 'TEST_Reddit', 'TEST_en-AU', 'TEST_en-IN', 'TEST_en-UK']\n","\n","[sarcasm] settings: 4 testsets: 5\n","settings head: ['FULL', 'TRAIN_en-AU', 'TRAIN_en-IN', 'TRAIN_en-UK']\n","testsets head: ['TEST_FULL', 'TEST_Reddit', 'TEST_en-AU', 'TEST_en-IN', 'TEST_en-UK']\n"]}]},{"cell_type":"markdown","metadata":{"id":"484f7d66"},"source":["## Step 4 — Load CE artifacts\n","*   **Purpose**: Load pre-computed Cross-Entropy (baseline) metrics/predictions for comparison.\n","*   **Inputs**: `models/roberta_baseline_ce` metrics and prediction CSVs.\n","*   **Outputs**: `metrics_ce`, `preds_ce` (DataFrames).\n","*   **Assumptions**: Baseline run has been completed and files exist."]},{"cell_type":"code","source":["# ==== Cell 4: Load CE artifacts (optional but recommended) ====\n","\n","CE_RUN_DIR = BASE / \"models\" / \"roberta_baseline_ce\"\n","ce_met_path = CE_RUN_DIR / \"metrics\" / \"roberta_ce_metrics_all.csv\"\n","ce_prd_path = CE_RUN_DIR / \"predictions\" / \"roberta_ce_predictions_all.csv\"\n","\n","metrics_ce = None\n","preds_ce = None\n","\n","if ce_met_path.exists() and ce_prd_path.exists():\n","    metrics_ce = pd.read_csv(ce_met_path)\n","    preds_ce   = pd.read_csv(ce_prd_path)\n","    metrics_ce.columns = [c.strip().lower() for c in metrics_ce.columns]\n","    preds_ce.columns   = [c.strip().lower() for c in preds_ce.columns]\n","    print(\"✅ Loaded CE metrics/preds:\", ce_met_path.name, ce_prd_path.name)\n","else:\n","    print(\"⚠️ CE metrics/preds not found. Delta + CE-vs-MoE error analysis will be skipped.\")\n","    print(\"Expected:\", ce_met_path)\n","    print(\"          \", ce_prd_path)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"0eml5vlzUgNs","executionInfo":{"status":"ok","timestamp":1769811109112,"user_tz":-60,"elapsed":3343,"user":{"displayName":"DNLP Project","userId":"02984253218627728392"}},"outputId":"13604559-2f42-46b5-8a15-73447d348ec1"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["✅ Loaded CE metrics/preds: roberta_ce_metrics_all.csv roberta_ce_predictions_all.csv\n"]}]},{"cell_type":"markdown","metadata":{"id":"b17f98a9"},"source":["## Step 5 — Dataset + Loader\n","*   **Purpose**: Define the `TextDS` dataset class and `make_loader` function for tokenization and batching.\n","*   **Inputs**: CSV paths containing text and labels.\n","*   **Outputs**: `TextDS` class, `DataLoader` instances.\n","*   **Assumptions**: Input CSVs contain columns for `text`, `label`, and optionally `variety` info."]},{"cell_type":"code","source":["# ==== Cell 5: Dataset + Loader ====\n","\n","tok = AutoTokenizer.from_pretrained(CFG[\"MODEL_NAME\"], use_fast=True)\n","\n","def infer_text_col(df):\n","    for c in [\"text\", \"text_clean\", \"text_norm\", \"sentence\", \"content\"]:\n","        if c in df.columns:\n","            return c\n","    raise ValueError(f\"No text col found. Columns={list(df.columns)[:40]}\")\n","\n","def infer_label_col(df):\n","    for c in [\"label\", \"y\", \"gold\", \"target\"]:\n","        if c in df.columns:\n","            return c\n","    raise ValueError(f\"No label col found. Columns={list(df.columns)[:40]}\")\n","\n","def infer_variety_cols(df):\n","    # Prefer explicit id + name if available\n","    id_col = None\n","    name_col = None\n","    for c in [\"variety_id\"]:\n","        if c in df.columns: id_col = c\n","    for c in [\"variety\", \"variety_name\", \"variety_code\"]:\n","        if c in df.columns: name_col = c\n","    return id_col, name_col\n","\n","def map_variety_to_id(v):\n","    # Standardize common forms to {0,1,2} = {en-AU, en-IN, en-UK}\n","    s = str(v).strip()\n","    s = s.replace(\"TEST_\", \"\").replace(\"TRAIN_\", \"\")\n","    s = s.replace(\"_\", \"-\")\n","    s = s.replace(\"en-\", \"en-\")\n","    mp = {\n","        \"en-AU\": 0, \"AU\": 0, \"En-AU\": 0, \"en-au\": 0,\n","        \"en-IN\": 1, \"IN\": 1, \"En-IN\": 1, \"en-in\": 1,\n","        \"en-UK\": 2, \"UK\": 2, \"En-UK\": 2, \"en-uk\": 2,\n","    }\n","    return mp.get(s, -1), s\n","\n","class TextDS(Dataset):\n","    def __init__(self, csv_path: Path, max_len: int):\n","        self.df = pd.read_csv(csv_path)\n","        self.df.columns = [c.strip().lower() for c in self.df.columns]\n","\n","        self.text_col = infer_text_col(self.df)\n","        self.label_col = infer_label_col(self.df)\n","        self.vid_col, self.vname_col = infer_variety_cols(self.df)\n","\n","        self.texts  = self.df[self.text_col].astype(str).tolist()\n","        self.labels = self.df[self.label_col].astype(int).values\n","\n","        # row_id for joins\n","        if \"row_id\" in self.df.columns:\n","            self.row_id = self.df[\"row_id\"].astype(int).values\n","        else:\n","            self.row_id = np.arange(len(self.df), dtype=int)\n","\n","        # variety id + name\n","        self.variety_id = None\n","        self.variety_name = None\n","\n","        if self.vid_col is not None and np.issubdtype(self.df[self.vid_col].dtype, np.number):\n","            self.variety_id = self.df[self.vid_col].astype(int).values\n","            if self.vname_col is not None:\n","                self.variety_name = self.df[self.vname_col].astype(str).values\n","            else:\n","                # best-effort name\n","                inv = {0:\"en-AU\", 1:\"en-IN\", 2:\"en-UK\"}\n","                self.variety_name = np.array([inv.get(int(x), \"UNK\") for x in self.variety_id], dtype=object)\n","        elif self.vname_col is not None:\n","            vraw = self.df[self.vname_col].astype(str).values\n","            vids, vnames = [], []\n","            for x in vraw:\n","                vid, vn = map_variety_to_id(x)\n","                vids.append(vid); vnames.append(vn)\n","            self.variety_id = np.array(vids, dtype=int)\n","            self.variety_name = np.array(vnames, dtype=object)\n","        else:\n","            # still runnable, but router won't learn variety routing\n","            self.variety_id = np.full(len(self.df), -1, dtype=int)\n","            self.variety_name = np.full(len(self.df), \"UNK\", dtype=object)\n","\n","        self.max_len = max_len\n","\n","    def __len__(self):\n","        return len(self.labels)\n","\n","    def __getitem__(self, i):\n","        t = self.texts[i]\n","        enc = tok(\n","            t,\n","            truncation=True,\n","            padding=\"max_length\",\n","            max_length=self.max_len,\n","            return_tensors=\"pt\",\n","        )\n","        enc = {k: v.squeeze(0) for k, v in enc.items()}\n","        y   = int(self.labels[i])\n","        vid = int(self.variety_id[i])\n","        rid = int(self.row_id[i])\n","        vnm = str(self.variety_name[i])\n","        return enc, y, vid, rid, vnm, t\n","\n","def make_loader(csv_path: Path, shuffle: bool):\n","    ds = TextDS(csv_path, max_len=CFG[\"MAX_LEN\"])\n","    ld = DataLoader(\n","        ds,\n","        batch_size=CFG[\"BATCH_SIZE\"],\n","        shuffle=shuffle,\n","        num_workers=CFG[\"NUM_WORKERS\"],\n","        pin_memory=(DEVICE==\"cuda\"),\n","    )\n","    return ds, ld"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":301,"referenced_widgets":["c94c49bf5dca4e4caee8df37d3a8a394","8c2f8c3e116848eeb54e0b955f8dfd32","9dfedeba1ecd41d3843aec98b4aad192","41d8f01151a041daa60abb9acf7f9e44","2dca123f15d944658021fb9bda04f7d3","46d6534a813543a6a3cde4f998ee8549","b76e507b035b4c159a8fe12c1c01f10f","e92d12006b244545aa7edbe51067e706","a68f987461864dd98677a14150fc7838","39676294dc004421a22e66a1fcbe2caa","7f0847ad60064d109b92d771a4b0dbcf","8b9dac4cbe234704ad7f77e9578c2655","4ee4762afac94577bdcc259f22286ad1","6f6fdc3e296c4c7ba25a771e942b6714","160485e1dc634920a205b392a396e7d1","eef27e558f3542449195eec33c1b7e40","761a897316704e89a00996d0fd733821","90dd5089c7674bea9c3059ffa960195a","2da1ab38872449eca6a71616b4014b11","d86a3e64d4444dc6babe946bf075a89f","87a06d0b6cdb442ba283bfa47ff91906","c77167756489491098f485a468014872","40c891e0faac4a02b09d9f058d066bda","2c3e7ae00b464da18e811dc7fa9ab027","f5ee4af0bcfd435899737648e3c55c13","5e15ee0cd4514d16b9f478fe5ce48597","e80c4aab2488428281bbcf2e0d68118a","d6e5411763a947bd98801f9c9da1c1ee","1116d9266efa4dc88848916f56bc1532","d0955954439f42d0a3376e6332cad65d","539b11acd22c4070a1cb0cf0f24c6c92","4ef0dcd2d4ab4d60a768efb6563fa527","8816092e7abb4c4b9cd6e2f9d03dcc68","afd6356d025748569962fad4f5f4a13b","66cc007e1b744751acdf862c4236b431","b05bd2d6d14043d8b6c063126d20a83b","a2818597e0d04cc18e193959ad9f66f4","aaffe9b72ceb434a8bf9091c4bf831c9","c0c958e29efb44b0b96964e8f9e1cd99","e7f305a8e7c645bbaa83c67c670b2da6","94a124ae3f124d00aa09ea750bd43a9d","c7254b7d5f964f5489e148c002fde347","fec4089d20e9420daf8e43e500159f9a","15d318ff62084c11b5e7383bfce14189","344190806c9c4b28a6c62c13af134a12","1eb3c8e889fc43d398df2d43e2071753","e9d86d3f53c7487b9009b11b35b92ba3","48e9a41fecfc458884c25e64a5befee7","2b1e4f4570844bd689e3cd5b1d13471f","97cc99a694494024b10ae84f58b04855","35102bfe19fa4a658b272e1b63b2e415","658274319e294b55a190ea78d9a277b2","fb961a366dd047db936004f0be110530","549aa5dc1c2446109f5457a999b65237","c8aec08919914735b207afb3558b93d0"]},"id":"fM4m_nhDUpiS","executionInfo":{"status":"ok","timestamp":1769811115643,"user_tz":-60,"elapsed":6517,"user":{"displayName":"DNLP Project","userId":"02984253218627728392"}},"outputId":"03cc784b-975e-45c7-d872-891db1d118ad"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.12/dist-packages/huggingface_hub/utils/_auth.py:94: UserWarning: \n","The secret `HF_TOKEN` does not exist in your Colab secrets.\n","To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n","You will be able to reuse this secret in all of your notebooks.\n","Please note that authentication is recommended but still optional to access public models or datasets.\n","  warnings.warn(\n"]},{"output_type":"display_data","data":{"text/plain":["tokenizer_config.json:   0%|          | 0.00/25.0 [00:00<?, ?B/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"c94c49bf5dca4e4caee8df37d3a8a394"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["config.json:   0%|          | 0.00/481 [00:00<?, ?B/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"8b9dac4cbe234704ad7f77e9578c2655"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["vocab.json:   0%|          | 0.00/899k [00:00<?, ?B/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"40c891e0faac4a02b09d9f058d066bda"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["merges.txt:   0%|          | 0.00/456k [00:00<?, ?B/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"afd6356d025748569962fad4f5f4a13b"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["tokenizer.json:   0%|          | 0.00/1.36M [00:00<?, ?B/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"344190806c9c4b28a6c62c13af134a12"}},"metadata":{}}]},{"cell_type":"markdown","metadata":{"id":"b5385eec"},"source":["## Step 6 — Metrics\n","*   **Purpose**: Define helper functions to calculate Accuracy, Precision, Recall, and F1.\n","*   **Inputs**: Ground truth labels and predicted probabilities.\n","*   **Outputs**: Dictionary of metric scores.\n","*   **Assumptions**: Uses a fixed threshold (e.g., 0.5) for binary classification."]},{"cell_type":"code","source":["# ==== Cell 6: Metrics (fixed thr=0.5) ====\n","\n","def compute_metrics(y_true, y_pred):\n","    acc = accuracy_score(y_true, y_pred)\n","    p, r, f1, _ = precision_recall_fscore_support(\n","        y_true, y_pred, average=\"macro\", zero_division=0\n","    )\n","    return {\"acc\": float(acc), \"precision\": float(p), \"recall\": float(r), \"macro_f1\": float(f1)}\n","\n","def metrics_from_probs(y_true, prob, thr=None):\n","    thr = CFG[\"FIXED_THRESHOLD\"] if thr is None else float(thr)\n","    y_pred = (prob >= thr).astype(int)\n","    m = compute_metrics(y_true, y_pred)\n","    return m, thr, y_pred"],"metadata":{"id":"-nCr8ySXUtdV"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"39a27c62"},"source":["## Step 7 — MoE-of-Adapters model\n","*   **Purpose**: Define the `RobertaMoEAdapters` architecture (Backbone + Router + Experts).\n","*   **Inputs**: Pretrained `roberta-base`, config parameters (n_experts, bottleneck).\n","*   **Outputs**: PyTorch model class.\n","*   **Assumptions**: Uses a mixture of bottleneck adapters managed by a router."]},{"cell_type":"code","source":["# ==== Cell 7: MoE-of-Adapters model ====\n","\n","class BottleneckAdapter(nn.Module):\n","    def __init__(self, dim, bottleneck=128, dropout=0.1):\n","        super().__init__()\n","        self.down = nn.Linear(dim, bottleneck)\n","        self.up   = nn.Linear(bottleneck, dim)\n","        self.drop = nn.Dropout(dropout)\n","        self.act  = nn.ReLU()\n","\n","    def forward(self, x):\n","        z = self.down(x)\n","        z = self.act(z)\n","        z = self.drop(z)\n","        z = self.up(z)\n","        return z  # delta\n","\n","class Router(nn.Module):\n","    def __init__(self, dim, n_experts, hidden=128, dropout=0.1):\n","        super().__init__()\n","        self.net = nn.Sequential(\n","            nn.Linear(dim, hidden),\n","            nn.Tanh(),\n","            nn.Dropout(dropout),\n","            nn.Linear(hidden, n_experts),\n","        )\n","\n","    def forward(self, x):\n","        return self.net(x)  # logits\n","\n","class RobertaMoEAdapters(nn.Module):\n","    def __init__(self, model_name, n_experts=3, bottleneck=128, router_hidden=128, dropout=0.1, num_labels=2):\n","        super().__init__()\n","        self.backbone = AutoModel.from_pretrained(model_name)\n","        dim = self.backbone.config.hidden_size\n","\n","        self.router = Router(dim, n_experts=n_experts, hidden=router_hidden, dropout=dropout)\n","        self.experts = nn.ModuleList([\n","            BottleneckAdapter(dim, bottleneck=bottleneck, dropout=dropout)\n","            for _ in range(n_experts)\n","        ])\n","        self.classifier = nn.Linear(dim, num_labels)\n","\n","    def forward(self, input_ids, attention_mask):\n","        out = self.backbone(input_ids=input_ids, attention_mask=attention_mask)\n","        h = out.last_hidden_state[:, 0, :]  # CLS\n","\n","        logits_router = self.router(h)                 # [B,E]\n","        w = torch.softmax(logits_router, dim=-1)       # [B,E]\n","\n","        deltas = []\n","        for ex in self.experts:\n","            deltas.append(ex(h))                       # [B,D]\n","        deltas = torch.stack(deltas, dim=1)            # [B,E,D]\n","\n","        mix_delta = (w.unsqueeze(-1) * deltas).sum(dim=1)  # [B,D]\n","        rep = h + mix_delta\n","        logits = self.classifier(rep)                  # [B,2]\n","\n","        return logits, w, deltas, logits_router"],"metadata":{"id":"UKlIhfKlU22T"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"09e09758"},"source":["## Step 8 — Loss + Optim + Warmup\n","*   **Purpose**: Define loss functions (CE, Load Balancing, Entropy), Optimizer, and Scheduler.\n","*   **Inputs**: Hyperparameters from `CFG`, class weights from data.\n","*   **Outputs**: Optimizer, Loss functions, LR scheduler helper.\n","*   **Assumptions**: Includes regularization to prevent router collapse."]},{"cell_type":"code","source":["# ==== Cell 8: Loss + Optim + Warmup ====\n","\n","def compute_class_weights_from_csv(train_csv: Path, label_col=\"label\"):\n","    df = pd.read_csv(train_csv)\n","    df.columns = [c.strip().lower() for c in df.columns]\n","    assert label_col in df.columns, f\"{train_csv} missing '{label_col}'\"\n","    y = df[label_col].astype(int).values\n","    n = len(y)\n","    n1 = int((y == 1).sum())\n","    n0 = int((y == 0).sum())\n","    n0 = max(n0, 1)\n","    n1 = max(n1, 1)\n","    w0 = n / (2.0 * n0)\n","    w1 = n / (2.0 * n1)\n","    return torch.tensor([w0, w1], dtype=torch.float32), (n0, n1, n)\n","\n","def load_balance_loss(w):\n","    # encourage mean usage ~ uniform\n","    E = w.size(1)\n","    target = 1.0 / E\n","    mean_w = w.mean(dim=0)  # [E]\n","    return ((mean_w - target) ** 2).sum()\n","\n","def entropy_value(w, eps=1e-9):\n","    # entropy per sample, averaged (higher = more spread)\n","    return (-(w * torch.log(w + eps)).sum(dim=1)).mean()\n","\n","def expert_l2_reg(deltas):\n","    # encourage experts to be different (pairwise MSE) – optional\n","    E = deltas.size(1)\n","    if E <= 1:\n","        return deltas.new_tensor(0.0)\n","    reg = 0.0\n","    cnt = 0\n","    for i in range(E):\n","        for j in range(i+1, E):\n","            reg = reg + F.mse_loss(deltas[:, i, :], deltas[:, j, :])\n","            cnt += 1\n","    return reg / max(cnt, 1)\n","\n","def make_optimizer(model):\n","    no_decay = [\"bias\", \"LayerNorm.weight\"]\n","    params = [\n","        {\"params\": [p for n,p in model.named_parameters() if p.requires_grad and not any(nd in n for nd in no_decay)], \"weight_decay\": CFG[\"WEIGHT_DECAY\"]},\n","        {\"params\": [p for n,p in model.named_parameters() if p.requires_grad and any(nd in n for nd in no_decay)], \"weight_decay\": 0.0},\n","    ]\n","    return torch.optim.AdamW(params, lr=CFG[\"LR\"])\n","\n","def linear_warmup(step, total_steps, warmup_ratio):\n","    warmup_steps = int(total_steps * warmup_ratio)\n","    if warmup_steps <= 0:\n","        return 1.0\n","    return min(1.0, step / warmup_steps)"],"metadata":{"id":"adW9YW1qU8CF"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"94e36c20"},"source":["## Step 9 — predict_probs\n","*   **Purpose**: Inference loop to generate probabilities and gather metadata (labels, variety IDs).\n","*   **Inputs**: Trained `model` and `loader`.\n","*   **Outputs**: Arrays of probabilities, true labels, variety IDs, and texts."]},{"cell_type":"code","source":["# ==== Cell 9: predict_probs ====\n","\n","@torch.no_grad()\n","def predict_probs(model, loader):\n","    model.eval()\n","    probs, ys, vids, rids, vnames, texts = [], [], [], [], [], []\n","    for enc, y, vid, rid, vnm, t in loader:\n","        enc = {k: v.to(DEVICE) for k,v in enc.items()}\n","        logits, w, deltas, logits_router = model(enc[\"input_ids\"], enc[\"attention_mask\"])\n","        p1 = torch.softmax(logits, dim=-1)[:, 1].detach().cpu().numpy()\n","\n","        probs.append(p1)\n","        ys.append(np.array(y, dtype=int))\n","        vids.append(np.array(vid, dtype=int))\n","        rids.append(np.array(rid, dtype=int))\n","        vnames += list(vnm)\n","        texts  += list(t)\n","\n","    return (\n","        np.concatenate(probs),\n","        np.concatenate(ys),\n","        np.concatenate(vids),\n","        np.concatenate(rids),\n","        np.array(vnames, dtype=object),\n","        texts,\n","    )"],"metadata":{"id":"n-dTOLMNVBCf"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"58f29806"},"source":["## Step 10 — Train one setting\n","*   **Purpose**: Main training routine for a single setting. Supports Stage 1 (Pool) and Stage 2 (Adaptation).\n","*   **Inputs**: Task, train/val CSVs, init checkpoint, freeze toggles.\n","*   **Outputs**: Trained model checkpoint (`.pt`).\n","*   **Assumptions**: Uses `RobertaMoEAdapters` and saves best model by Val F1."]},{"cell_type":"code","source":["# ==== Cell 10: Train one setting (supports init_state + router freeze) ====\n","\n","def set_requires_grad(module, flag: bool):\n","    for p in module.parameters():\n","        p.requires_grad = flag\n","\n","def train_one_setting_moe(\n","    task: str,\n","    train_setting: str,\n","    train_csv: Path,\n","    val_csv: Path,\n","    save_name: str,\n","    init_ckpt: Path = None,\n","    freeze_router: bool = False,\n","    freeze_backbone: bool = False,\n","):\n","    ckpt_path = CKPT_DIR / f\"{save_name}.pt\"\n","    if ckpt_path.exists():\n","        print(f\"✅ CKPT exists, skipping train: {ckpt_path.name}\")\n","        return ckpt_path\n","\n","    tr_ds, tr_ld = make_loader(train_csv, shuffle=True)\n","    va_ds, va_ld = make_loader(val_csv, shuffle=False)\n","\n","    class_wts_cpu, (n0, n1, n) = compute_class_weights_from_csv(train_csv)\n","    class_wts = class_wts_cpu.to(DEVICE)\n","    print(f\"[{task} | {train_setting}] train label counts: n0={n0} n1={n1} n={n} | class_wts=[{class_wts_cpu[0]:.3f},{class_wts_cpu[1]:.3f}]\")\n","\n","    model = RobertaMoEAdapters(\n","        CFG[\"MODEL_NAME\"],\n","        n_experts=CFG[\"N_EXPERTS\"],\n","        bottleneck=CFG[\"ADAPTER_BOTTLENECK\"],\n","        router_hidden=CFG[\"ROUTER_HIDDEN\"],\n","        dropout=CFG[\"ADAPTER_DROPOUT\"],\n","    ).to(DEVICE)\n","\n","    # init from FULL ckpt if provided\n","    init_meta = {\"init_from\": None}\n","    if init_ckpt is not None:\n","        blob = torch.load(init_ckpt, map_location=\"cpu\")\n","        model.load_state_dict(blob[\"state_dict\"], strict=True)\n","        init_meta[\"init_from\"] = str(init_ckpt)\n","        print(\"✅ Initialized from:\", init_ckpt.name)\n","\n","    # freeze toggles for stage2\n","    if freeze_router:\n","        set_requires_grad(model.router, False)\n","    if freeze_backbone:\n","        set_requires_grad(model.backbone, False)\n","\n","    opt = make_optimizer(model)\n","\n","    total_steps = (len(tr_ld) * CFG[\"EPOCHS\"]) // max(CFG[\"GRAD_ACCUM\"], 1)\n","    scaler = torch.amp.GradScaler(\"cuda\", enabled=(CFG[\"USE_AMP\"] and DEVICE==\"cuda\"))\n","\n","    best_val_f1 = -1.0\n","    best_state = None\n","    bad_epochs = 0\n","    step = 0\n","\n","    for ep in range(1, CFG[\"EPOCHS\"] + 1):\n","        model.train()\n","        losses = []\n","\n","        opt.zero_grad(set_to_none=True)\n","\n","        for it, batch in enumerate(tr_ld, start=1):\n","            enc, y, vid, rid, vnm, t = batch\n","            enc = {k: v.to(DEVICE) for k,v in enc.items()}\n","            y = torch.as_tensor(y, dtype=torch.long, device=DEVICE)\n","\n","            with torch.amp.autocast(\"cuda\", enabled=(CFG[\"USE_AMP\"] and DEVICE==\"cuda\")):\n","                logits, w, deltas, logits_router = model(enc[\"input_ids\"], enc[\"attention_mask\"])\n","\n","                # weighted CE\n","                loss_ce = F.cross_entropy(logits, y, weight=class_wts)\n","\n","                # regularizers\n","                loss_lb = load_balance_loss(w)\n","                ent = entropy_value(w)  # higher is better\n","\n","                loss = loss_ce + CFG[\"LOAD_BAL_W\"] * loss_lb - CFG[\"ENTROPY_W\"] * ent\n","\n","                if CFG[\"EXPERT_L2_REG\"] > 0:\n","                    loss = loss + CFG[\"EXPERT_L2_REG\"] * expert_l2_reg(deltas)\n","\n","                # NOTE: router supervision intentionally OFF here (ROUTER_SUP_W=0)\n","\n","                loss = loss / max(CFG[\"GRAD_ACCUM\"], 1)\n","\n","            scaler.scale(loss).backward()\n","\n","            if it % CFG[\"GRAD_ACCUM\"] == 0:\n","                # warmup LR\n","                step += 1\n","                lr_scale = linear_warmup(step, total_steps, CFG[\"WARMUP_RATIO\"])\n","                for g in opt.param_groups:\n","                    g[\"lr\"] = CFG[\"LR\"] * lr_scale\n","\n","                scaler.step(opt)\n","                scaler.update()\n","                opt.zero_grad(set_to_none=True)\n","\n","            losses.append(float(loss.detach().cpu().item()))\n","\n","        # ---- Validation (fixed thr=0.5) ----\n","        prob, y_true, v_true, rid_true, vnm_true, texts = predict_probs(model, va_ld)\n","        m, thr_use, _ = metrics_from_probs(y_true, prob, thr=CFG[\"FIXED_THRESHOLD\"])\n","        val_f1 = m[\"macro_f1\"]\n","\n","        print(f\"[{task} | {train_setting} | {VARIANT}] EP{ep} loss={np.mean(losses):.4f} valF1@0.5={val_f1:.4f}\")\n","\n","        if val_f1 > best_val_f1:\n","            best_val_f1 = val_f1\n","            best_state = {k: v.detach().cpu() for k,v in model.state_dict().items()}\n","            bad_epochs = 0\n","        else:\n","            bad_epochs += 1\n","            if bad_epochs >= CFG[\"PATIENCE\"]:\n","                print(\"Early stopping.\")\n","                break\n","\n","    assert best_state is not None, \"Training failed: best_state is None\"\n","\n","    torch.save({\n","        \"state_dict\": best_state,\n","        \"best_val_f1\": float(best_val_f1),\n","        \"cfg\": CFG,\n","        \"task\": task,\n","        \"train_setting\": train_setting,\n","        \"variant\": VARIANT,\n","        \"threshold\": float(CFG[\"FIXED_THRESHOLD\"]),\n","        \"freeze_router\": bool(freeze_router),\n","        \"freeze_backbone\": bool(freeze_backbone),\n","        **init_meta,\n","    }, ckpt_path)\n","\n","    print(\"✅ Saved:\", ckpt_path)\n","    return ckpt_path"],"metadata":{"id":"fCFRgfvuVFO6"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"5b77c6cc"},"source":["## Step 11 — Evaluate a checkpoint\n","*   **Purpose**: Evaluate a saved model on all associated test sets and breakdown performance by variety.\n","*   **Inputs**: Checkpoint path, testsets DataFrame.\n","*   **Outputs**: DataFrames for metrics, predictions, and per-variety stats."]},{"cell_type":"code","source":["# ==== Cell 11: Evaluate a checkpoint on all testsets (with per-variety breakdown) ====\n","\n","def load_model_from_ckpt(ckpt_path: Path):\n","    blob = torch.load(ckpt_path, map_location=\"cpu\")\n","    model = RobertaMoEAdapters(\n","        CFG[\"MODEL_NAME\"],\n","        n_experts=CFG[\"N_EXPERTS\"],\n","        bottleneck=CFG[\"ADAPTER_BOTTLENECK\"],\n","        router_hidden=CFG[\"ROUTER_HIDDEN\"],\n","        dropout=CFG[\"ADAPTER_DROPOUT\"],\n","    ).to(DEVICE)\n","    model.load_state_dict(blob[\"state_dict\"], strict=True)\n","    model.eval()\n","    return model, blob\n","\n","def eval_on_testsets(task: str, train_setting: str, ckpt_path: Path, testsets_df: pd.DataFrame):\n","    model, blob = load_model_from_ckpt(ckpt_path)\n","\n","    all_met = []\n","    all_prd = []\n","    all_pervar = []\n","\n","    for _, r in testsets_df.iterrows():\n","        test_setting = r[\"test_setting\"]\n","        test_csv = r[\"test_csv_abs\"]\n","\n","        te_ds, te_ld = make_loader(test_csv, shuffle=False)\n","        prob, y_true, v_true, rid_true, vnm_true, texts = predict_probs(model, te_ld)\n","\n","        m, thr_use, y_pred = metrics_from_probs(y_true, prob, thr=CFG[\"FIXED_THRESHOLD\"])\n","\n","        # ---- global metrics row (match baseline schema) ----\n","        mrow = {\n","            \"task\": task,\n","            \"train_setting\": train_setting,\n","            \"variant\": VARIANT,\n","            \"test_setting\": test_setting,\n","            \"split\": \"test\",\n","            \"n\": int(len(y_true)),\n","            **m,\n","            \"threshold_type\": \"fixed0.5\",\n","            \"threshold\": float(thr_use),\n","        }\n","        all_met.append(mrow)\n","\n","        # ---- predictions ----\n","        dfp = pd.DataFrame({\n","            \"task\": task,\n","            \"train_setting\": train_setting,\n","            \"variant\": VARIANT,\n","            \"test_setting\": test_setting,\n","            \"row_id\": rid_true,\n","            \"label\": y_true,\n","            \"prob\": prob,\n","            \"pred\": y_pred,\n","            \"threshold\": float(thr_use),\n","            \"variety_id\": v_true,\n","            \"variety_name\": vnm_true,\n","            \"text\": texts,\n","        })\n","        all_prd.append(dfp)\n","\n","        # ---- per-variety breakdown inside this testset ----\n","        for vid in sorted(set(v_true.tolist())):\n","            mask = (v_true == vid)\n","            if mask.sum() == 0:\n","                continue\n","            mv, _, ypv = metrics_from_probs(y_true[mask], prob[mask], thr=thr_use)\n","            vname = str(pd.Series(vnm_true[mask]).mode().iloc[0]) if mask.sum() > 0 else \"UNK\"\n","            all_pervar.append({\n","                \"task\": task,\n","                \"train_setting\": train_setting,\n","                \"variant\": VARIANT,\n","                \"test_setting\": test_setting,\n","                \"split\": \"test\",\n","                \"variety_id\": int(vid),\n","                \"variety_name\": vname,\n","                \"n\": int(mask.sum()),\n","                **mv,\n","                \"threshold_type\": \"fixed0.5\",\n","                \"threshold\": float(thr_use),\n","            })\n","\n","    return pd.DataFrame(all_met), pd.concat(all_prd, ignore_index=True), pd.DataFrame(all_pervar)"],"metadata":{"id":"HfyWjG4EVH15"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"3935892b"},"source":["## Step 12 — Execution Loop (Stage 1 + Stage 2)\n","*   **Purpose**: Orchestrate the 2-stage training process for all tasks (Sentiment, Sarcasm).\n","    1.  **Stage 1**: Train on pooled data (FULL) to get a general model.\n","    2.  **Stage 2**: Adapt the general model to specific settings (e.g., en-AU, en-IN) with frozen components.\n","*   **Inputs**: `TASKS` list, Indices.\n","*   **Outputs**: Final merged metrics/predictions CSVs."]},{"cell_type":"code","source":["# ==== Cell 12: Build pooled stage-1 if FULL missing, then adapt all settings (NO DUPLICATE FULL) ====\n","\n","TASKS = [\"sentiment\", \"sarcasm\"]\n","\n","TMP_POOL_DIR = RUN_DIR / \"tmp_pooled\"\n","TMP_POOL_DIR.mkdir(parents=True, exist_ok=True)\n","\n","def _read_df(csv_path: Path) -> pd.DataFrame:\n","    df = pd.read_csv(csv_path)\n","    df.columns = [c.strip().lower() for c in df.columns]\n","    return df\n","\n","def build_pooled_train_val(task: str, settings_df: pd.DataFrame, pool_from=(\"Google\",\"Reddit\")):\n","    \"\"\"\n","    Creates synthetic pooled train/val CSVs for stage-1 init.\n","    Prefer pooling from Google+Reddit if both exist; otherwise pool from ALL available settings.\n","    De-duplicates by row_id.\n","    Returns (pool_train_csv, pool_val_csv, pool_setting_name, pool_sources)\n","    \"\"\"\n","    available = settings_df[\"train_setting\"].astype(str).tolist()\n","    pool = [s for s in pool_from if s in available]\n","    if len(pool) == 0:\n","        pool = available  # fallback: pool everything\n","\n","    train_dfs, val_dfs = [], []\n","    for s in pool:\n","        r = settings_df[settings_df[\"train_setting\"].astype(str) == s].iloc[0]\n","        train_dfs.append(_read_df(Path(r[\"train_csv_abs\"])))\n","        val_dfs.append(_read_df(Path(r[\"val_csv_abs\"])))\n","\n","    tr_all = pd.concat(train_dfs, ignore_index=True)\n","    va_all = pd.concat(val_dfs, ignore_index=True)\n","\n","    # De-dup by row_id (preferred)\n","    if \"row_id\" in tr_all.columns and \"row_id\" in va_all.columns:\n","        tr_all = tr_all.drop_duplicates(subset=[\"row_id\"]).reset_index(drop=True)\n","        va_all = va_all.drop_duplicates(subset=[\"row_id\"]).reset_index(drop=True)\n","    else:\n","        key = [c for c in [\"text\",\"label\",\"task\",\"variety_id\",\"variety_name\",\"source\",\"source_name\"] if c in tr_all.columns]\n","        tr_all = tr_all.drop_duplicates(subset=key if key else None).reset_index(drop=True)\n","        va_all = va_all.drop_duplicates(subset=key if key else None).reset_index(drop=True)\n","\n","    pool_setting = \"FULL\"  # synthetic label (not in settings_df)\n","    out_dir = TMP_POOL_DIR / task\n","    out_dir.mkdir(parents=True, exist_ok=True)\n","\n","    pool_train_csv = out_dir / f\"{pool_setting}_train.csv\"\n","    pool_val_csv   = out_dir / f\"{pool_setting}_val.csv\"\n","    tr_all.to_csv(pool_train_csv, index=False)\n","    va_all.to_csv(pool_val_csv, index=False)\n","\n","    return pool_train_csv, pool_val_csv, pool_setting, pool\n","\n","def find_full_like_existing(settings_df: pd.DataFrame):\n","    ts = settings_df[\"train_setting\"].astype(str)\n","    # exact FULL\n","    m = ts.str.upper().eq(\"FULL\")\n","    if m.any():\n","        return settings_df[m].iloc[0], \"exact_FULL\"\n","    # common variants\n","    aliases = {\"TRAIN_FULL\", \"FULL_TRAIN\", \"FULLTRAIN\", \"ALL\", \"COMBINED\", \"TRAIN_ALL\", \"TRAIN_COMBINED\"}\n","    m = ts.str.upper().isin(aliases)\n","    if m.any():\n","        return settings_df[m].iloc[0], \"alias_match\"\n","    # contains FULL\n","    m = ts.str.upper().str.contains(\"FULL\", na=False)\n","    if m.any():\n","        return settings_df[m].iloc[0], \"contains_FULL\"\n","    return None, None\n","\n","ALL_METRICS, ALL_PREDS, ALL_PERVAR = [], [], []\n","\n","for task in TASKS:\n","    settings_df, testsets_df = load_task_indices(task)\n","\n","    print(\"\\n==============================\")\n","    print(f\"RUN TASK: {task}\")\n","    print(\"Available train_settings:\", settings_df[\"train_setting\"].astype(str).tolist())\n","\n","    # -------------------------\n","    # Stage 1: pooled pretrain (existing FULL or synthetic FULL)\n","    # -------------------------\n","    full_row, how = find_full_like_existing(settings_df)\n","\n","    stage1_is_existing_row = (full_row is not None)\n","\n","    if stage1_is_existing_row:\n","        stage1_name = str(full_row[\"train_setting\"])\n","        pool_train_csv = Path(full_row[\"train_csv_abs\"])\n","        pool_val_csv   = Path(full_row[\"val_csv_abs\"])\n","        pool_sources   = [stage1_name]\n","        print(f\"✅ [{task}] Using existing pooled setting: '{stage1_name}' ({how})\")\n","    else:\n","        pool_train_csv, pool_val_csv, stage1_name, pool_sources = build_pooled_train_val(\n","            task, settings_df, pool_from=(\"Google\",\"Reddit\")\n","        )\n","        print(f\"✅ [{task}] Built synthetic '{stage1_name}' from: {pool_sources}\")\n","        print(\"   train_csv:\", pool_train_csv.name, \"rows=\", len(pd.read_csv(pool_train_csv)))\n","        print(\"   val_csv  :\", pool_val_csv.name,   \"rows=\", len(pd.read_csv(pool_val_csv)))\n","\n","    full_ckpt_name = f\"{task}__{stage1_name}__{VARIANT}__stage1_poolpretrain\"\n","    full_ckpt = train_one_setting_moe(\n","        task=task,\n","        train_setting=stage1_name,\n","        train_csv=pool_train_csv,\n","        val_csv=pool_val_csv,\n","        save_name=full_ckpt_name,\n","        init_ckpt=None,\n","        freeze_router=False,\n","        freeze_backbone=False,\n","    )\n","\n","    # Evaluate stage-1 pooled model\n","    met_full, prd_full, pv_full = eval_on_testsets(task, stage1_name, full_ckpt, testsets_df)\n","    ALL_METRICS.append(met_full)\n","    ALL_PREDS.append(prd_full)\n","    ALL_PERVAR.append(pv_full)\n","\n","    # -------------------------\n","    # Stage 2: adapt each REAL setting from pooled init\n","    #   - If stage1 is an existing row (e.g., sarcasm FULL), skip it to avoid duplicates.\n","    # -------------------------\n","    if stage1_is_existing_row:\n","        stage2_df = settings_df.drop(index=full_row.name).copy()\n","        print(f\"✅ [{task}] Stage-2: adapt {len(stage2_df)} real settings (skipping '{stage1_name}')\")\n","    else:\n","        stage2_df = settings_df.copy()\n","        print(f\"✅ [{task}] Stage-2: adapt {len(stage2_df)} real settings\")\n","\n","    for _, row in stage2_df.iterrows():\n","        train_setting = str(row[\"train_setting\"])\n","        train_csv = Path(row[\"train_csv_abs\"])\n","        val_csv   = Path(row[\"val_csv_abs\"])\n","\n","        save_name = f\"{task}__{train_setting}__{VARIANT}__stage2_fromPOOL_routerFz\"\n","        ckpt_path = train_one_setting_moe(\n","            task=task,\n","            train_setting=train_setting,\n","            train_csv=train_csv,\n","            val_csv=val_csv,\n","            save_name=save_name,\n","            init_ckpt=full_ckpt,\n","            freeze_router=bool(CFG[\"FREEZE_ROUTER_STAGE2\"]),\n","            freeze_backbone=bool(CFG[\"FREEZE_BACKBONE_STAGE2\"]),\n","        )\n","\n","        met_df, prd_df, pv_df = eval_on_testsets(task, train_setting, ckpt_path, testsets_df)\n","        ALL_METRICS.append(met_df)\n","        ALL_PREDS.append(prd_df)\n","        ALL_PERVAR.append(pv_df)\n","\n","# -------------------------\n","# Save combined artifacts\n","# -------------------------\n","metrics_moe = pd.concat(ALL_METRICS, ignore_index=True)\n","preds_moe   = pd.concat(ALL_PREDS, ignore_index=True)\n","pervar_moe  = pd.concat(ALL_PERVAR, ignore_index=True)\n","\n","met_path = MET_DIR / \"moe_metrics_all.csv\"\n","prd_path = PRD_DIR / \"moe_predictions_all.csv\"\n","pv_path  = MET_DIR / \"moe_pervar_metrics_all.csv\"\n","\n","metrics_moe.to_csv(met_path, index=False)\n","preds_moe.to_csv(prd_path, index=False)\n","pervar_moe.to_csv(pv_path, index=False)\n","\n","print(\"\\n✅ Saved:\", met_path)\n","print(\"✅ Saved:\", prd_path)\n","print(\"✅ Saved:\", pv_path)\n","\n","display(metrics_moe.head(20))"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":1000,"referenced_widgets":["cbfc6f752a5b46eeb25992f04724f7c0","bfebb93bb62a447ea83dc7ae587d2835","4d3912f180b5492f9caff2128fe901c6","2baade8cb423411c8d03038bee3bb104","d673ec4e8cb54d47a2d42e9f6bd3cd63","71dbe99615234e088e7836cb84fc9cd8","d53d691456b54d59a05d917e0eb4bc6e","ff80d61eeb3b4a6c8b5da2512f702b7d","f00ea1093cd64fe1be032cc4fd952bd2","8dba97264e2d48c5b21f55d1c32480ec","d29401ccd6ad486385199aae5b2e149d"]},"id":"pyVtbQezVKcf","executionInfo":{"status":"ok","timestamp":1769813841890,"user_tz":-60,"elapsed":2716137,"user":{"displayName":"DNLP Project","userId":"02984253218627728392"}},"outputId":"51d4baf9-2093-46b6-d2ae-072c7e67a639"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["\n","==============================\n","RUN TASK: sentiment\n","Available train_settings: ['Google', 'Reddit', 'TRAIN_en-AU', 'TRAIN_en-IN', 'TRAIN_en-UK']\n","✅ [sentiment] Built synthetic 'FULL' from: ['Google', 'Reddit']\n","   train_csv: FULL_train.csv rows= 7093\n","   val_csv  : FULL_val.csv rows= 1773\n","[sentiment | FULL] train label counts: n0=3579 n1=3514 n=7093 | class_wts=[0.991,1.009]\n"]},{"output_type":"display_data","data":{"text/plain":["model.safetensors:   0%|          | 0.00/499M [00:00<?, ?B/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"cbfc6f752a5b46eeb25992f04724f7c0"}},"metadata":{}},{"output_type":"stream","name":"stderr","text":["Some weights of RobertaModel were not initialized from the model checkpoint at roberta-base and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"]},{"output_type":"stream","name":"stdout","text":["[sentiment | FULL | roberta_extension_mixture_of_adapters] EP1 loss=0.3633 valF1@0.5=0.8917\n","[sentiment | FULL | roberta_extension_mixture_of_adapters] EP2 loss=0.1980 valF1@0.5=0.8909\n","[sentiment | FULL | roberta_extension_mixture_of_adapters] EP3 loss=0.1322 valF1@0.5=0.8855\n","Early stopping.\n","✅ Saved: /content/drive/MyDrive/DNLP/models/roberta_extension_mixture_of_adapters/checkpoints/sentiment__FULL__roberta_extension_mixture_of_adapters__stage1_poolpretrain.pt\n"]},{"output_type":"stream","name":"stderr","text":["Some weights of RobertaModel were not initialized from the model checkpoint at roberta-base and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"]},{"output_type":"stream","name":"stdout","text":["✅ [sentiment] Stage-2: adapt 5 real settings from pooled init\n","[sentiment | Google] train label counts: n0=900 n1=2629 n=3529 | class_wts=[1.961,0.671]\n"]},{"output_type":"stream","name":"stderr","text":["Some weights of RobertaModel were not initialized from the model checkpoint at roberta-base and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"]},{"output_type":"stream","name":"stdout","text":["✅ Initialized from: sentiment__FULL__roberta_extension_mixture_of_adapters__stage1_poolpretrain.pt\n","[sentiment | Google | roberta_extension_mixture_of_adapters] EP1 loss=0.1919 valF1@0.5=0.9083\n","[sentiment | Google | roberta_extension_mixture_of_adapters] EP2 loss=0.1236 valF1@0.5=0.8913\n","[sentiment | Google | roberta_extension_mixture_of_adapters] EP3 loss=0.1033 valF1@0.5=0.8832\n","Early stopping.\n","✅ Saved: /content/drive/MyDrive/DNLP/models/roberta_extension_mixture_of_adapters/checkpoints/sentiment__Google__roberta_extension_mixture_of_adapters__stage2_fromPOOL_routerFz.pt\n"]},{"output_type":"stream","name":"stderr","text":["Some weights of RobertaModel were not initialized from the model checkpoint at roberta-base and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"]},{"output_type":"stream","name":"stdout","text":["[sentiment | Reddit] train label counts: n0=2679 n1=885 n=3564 | class_wts=[0.665,2.014]\n"]},{"output_type":"stream","name":"stderr","text":["Some weights of RobertaModel were not initialized from the model checkpoint at roberta-base and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"]},{"output_type":"stream","name":"stdout","text":["✅ Initialized from: sentiment__FULL__roberta_extension_mixture_of_adapters__stage1_poolpretrain.pt\n","[sentiment | Reddit | roberta_extension_mixture_of_adapters] EP1 loss=0.3059 valF1@0.5=0.8268\n","[sentiment | Reddit | roberta_extension_mixture_of_adapters] EP2 loss=0.1910 valF1@0.5=0.8204\n","[sentiment | Reddit | roberta_extension_mixture_of_adapters] EP3 loss=0.1153 valF1@0.5=0.8199\n","Early stopping.\n","✅ Saved: /content/drive/MyDrive/DNLP/models/roberta_extension_mixture_of_adapters/checkpoints/sentiment__Reddit__roberta_extension_mixture_of_adapters__stage2_fromPOOL_routerFz.pt\n"]},{"output_type":"stream","name":"stderr","text":["Some weights of RobertaModel were not initialized from the model checkpoint at roberta-base and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"]},{"output_type":"stream","name":"stdout","text":["[sentiment | TRAIN_en-AU] train label counts: n0=1161 n1=1006 n=2167 | class_wts=[0.933,1.077]\n"]},{"output_type":"stream","name":"stderr","text":["Some weights of RobertaModel were not initialized from the model checkpoint at roberta-base and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"]},{"output_type":"stream","name":"stdout","text":["✅ Initialized from: sentiment__FULL__roberta_extension_mixture_of_adapters__stage1_poolpretrain.pt\n","[sentiment | TRAIN_en-AU | roberta_extension_mixture_of_adapters] EP1 loss=0.1889 valF1@0.5=0.9114\n","[sentiment | TRAIN_en-AU | roberta_extension_mixture_of_adapters] EP2 loss=0.1108 valF1@0.5=0.9203\n","[sentiment | TRAIN_en-AU | roberta_extension_mixture_of_adapters] EP3 loss=0.0380 valF1@0.5=0.9221\n","[sentiment | TRAIN_en-AU | roberta_extension_mixture_of_adapters] EP4 loss=0.0406 valF1@0.5=0.9194\n","[sentiment | TRAIN_en-AU | roberta_extension_mixture_of_adapters] EP5 loss=0.0363 valF1@0.5=0.8782\n","Early stopping.\n","✅ Saved: /content/drive/MyDrive/DNLP/models/roberta_extension_mixture_of_adapters/checkpoints/sentiment__TRAIN_en-AU__roberta_extension_mixture_of_adapters__stage2_fromPOOL_routerFz.pt\n"]},{"output_type":"stream","name":"stderr","text":["Some weights of RobertaModel were not initialized from the model checkpoint at roberta-base and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"]},{"output_type":"stream","name":"stdout","text":["[sentiment | TRAIN_en-IN] train label counts: n0=1338 n1=1329 n=2667 | class_wts=[0.997,1.003]\n"]},{"output_type":"stream","name":"stderr","text":["Some weights of RobertaModel were not initialized from the model checkpoint at roberta-base and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"]},{"output_type":"stream","name":"stdout","text":["✅ Initialized from: sentiment__FULL__roberta_extension_mixture_of_adapters__stage1_poolpretrain.pt\n","[sentiment | TRAIN_en-IN | roberta_extension_mixture_of_adapters] EP1 loss=0.3025 valF1@0.5=0.8722\n","[sentiment | TRAIN_en-IN | roberta_extension_mixture_of_adapters] EP2 loss=0.2127 valF1@0.5=0.8708\n","[sentiment | TRAIN_en-IN | roberta_extension_mixture_of_adapters] EP3 loss=0.1509 valF1@0.5=0.8648\n","Early stopping.\n","✅ Saved: /content/drive/MyDrive/DNLP/models/roberta_extension_mixture_of_adapters/checkpoints/sentiment__TRAIN_en-IN__roberta_extension_mixture_of_adapters__stage2_fromPOOL_routerFz.pt\n"]},{"output_type":"stream","name":"stderr","text":["Some weights of RobertaModel were not initialized from the model checkpoint at roberta-base and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"]},{"output_type":"stream","name":"stdout","text":["[sentiment | TRAIN_en-UK] train label counts: n0=1080 n1=1179 n=2259 | class_wts=[1.046,0.958]\n"]},{"output_type":"stream","name":"stderr","text":["Some weights of RobertaModel were not initialized from the model checkpoint at roberta-base and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"]},{"output_type":"stream","name":"stdout","text":["✅ Initialized from: sentiment__FULL__roberta_extension_mixture_of_adapters__stage1_poolpretrain.pt\n","[sentiment | TRAIN_en-UK | roberta_extension_mixture_of_adapters] EP1 loss=0.1112 valF1@0.5=0.9537\n","[sentiment | TRAIN_en-UK | roberta_extension_mixture_of_adapters] EP2 loss=0.0545 valF1@0.5=0.9716\n","[sentiment | TRAIN_en-UK | roberta_extension_mixture_of_adapters] EP3 loss=0.0214 valF1@0.5=0.9787\n","[sentiment | TRAIN_en-UK | roberta_extension_mixture_of_adapters] EP4 loss=0.0151 valF1@0.5=0.9841\n","[sentiment | TRAIN_en-UK | roberta_extension_mixture_of_adapters] EP5 loss=0.0008 valF1@0.5=0.9752\n","[sentiment | TRAIN_en-UK | roberta_extension_mixture_of_adapters] EP6 loss=0.0074 valF1@0.5=0.9770\n","Early stopping.\n","✅ Saved: /content/drive/MyDrive/DNLP/models/roberta_extension_mixture_of_adapters/checkpoints/sentiment__TRAIN_en-UK__roberta_extension_mixture_of_adapters__stage2_fromPOOL_routerFz.pt\n"]},{"output_type":"stream","name":"stderr","text":["Some weights of RobertaModel were not initialized from the model checkpoint at roberta-base and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"]},{"output_type":"stream","name":"stdout","text":["\n","==============================\n","RUN TASK: sarcasm\n","Available train_settings: ['FULL', 'TRAIN_en-AU', 'TRAIN_en-IN', 'TRAIN_en-UK']\n","✅ [sarcasm] Using existing pooled setting: 'FULL' (exact_FULL)\n","[sarcasm | FULL] train label counts: n0=2631 n1=954 n=3585 | class_wts=[0.681,1.879]\n"]},{"output_type":"stream","name":"stderr","text":["Some weights of RobertaModel were not initialized from the model checkpoint at roberta-base and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"]},{"output_type":"stream","name":"stdout","text":["[sarcasm | FULL | roberta_extension_mixture_of_adapters] EP1 loss=0.6794 valF1@0.5=0.4709\n","[sarcasm | FULL | roberta_extension_mixture_of_adapters] EP2 loss=0.6310 valF1@0.5=0.6209\n","[sarcasm | FULL | roberta_extension_mixture_of_adapters] EP3 loss=0.5326 valF1@0.5=0.6802\n","[sarcasm | FULL | roberta_extension_mixture_of_adapters] EP4 loss=0.3684 valF1@0.5=0.6878\n","[sarcasm | FULL | roberta_extension_mixture_of_adapters] EP5 loss=0.2056 valF1@0.5=0.6595\n","[sarcasm | FULL | roberta_extension_mixture_of_adapters] EP6 loss=0.1380 valF1@0.5=0.6817\n","Early stopping.\n","✅ Saved: /content/drive/MyDrive/DNLP/models/roberta_extension_mixture_of_adapters/checkpoints/sarcasm__FULL__roberta_extension_mixture_of_adapters__stage1_poolpretrain.pt\n"]},{"output_type":"stream","name":"stderr","text":["Some weights of RobertaModel were not initialized from the model checkpoint at roberta-base and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"]},{"output_type":"stream","name":"stdout","text":["✅ [sarcasm] Stage-2: adapt 4 real settings from pooled init\n","[sarcasm | FULL] train label counts: n0=2631 n1=954 n=3585 | class_wts=[0.681,1.879]\n"]},{"output_type":"stream","name":"stderr","text":["Some weights of RobertaModel were not initialized from the model checkpoint at roberta-base and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"]},{"output_type":"stream","name":"stdout","text":["✅ Initialized from: sarcasm__FULL__roberta_extension_mixture_of_adapters__stage1_poolpretrain.pt\n","[sarcasm | FULL | roberta_extension_mixture_of_adapters] EP1 loss=0.1963 valF1@0.5=0.6768\n","[sarcasm | FULL | roberta_extension_mixture_of_adapters] EP2 loss=0.1152 valF1@0.5=0.6800\n","[sarcasm | FULL | roberta_extension_mixture_of_adapters] EP3 loss=0.0685 valF1@0.5=0.6896\n","[sarcasm | FULL | roberta_extension_mixture_of_adapters] EP4 loss=0.0697 valF1@0.5=0.6766\n","[sarcasm | FULL | roberta_extension_mixture_of_adapters] EP5 loss=0.0561 valF1@0.5=0.6904\n","[sarcasm | FULL | roberta_extension_mixture_of_adapters] EP6 loss=0.0277 valF1@0.5=0.6827\n","✅ Saved: /content/drive/MyDrive/DNLP/models/roberta_extension_mixture_of_adapters/checkpoints/sarcasm__FULL__roberta_extension_mixture_of_adapters__stage2_fromPOOL_routerFz.pt\n"]},{"output_type":"stream","name":"stderr","text":["Some weights of RobertaModel were not initialized from the model checkpoint at roberta-base and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"]},{"output_type":"stream","name":"stdout","text":["[sarcasm | TRAIN_en-AU] train label counts: n0=818 n1=593 n=1411 | class_wts=[0.862,1.190]\n"]},{"output_type":"stream","name":"stderr","text":["Some weights of RobertaModel were not initialized from the model checkpoint at roberta-base and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"]},{"output_type":"stream","name":"stdout","text":["✅ Initialized from: sarcasm__FULL__roberta_extension_mixture_of_adapters__stage1_poolpretrain.pt\n","[sarcasm | TRAIN_en-AU | roberta_extension_mixture_of_adapters] EP1 loss=0.2198 valF1@0.5=0.8309\n","[sarcasm | TRAIN_en-AU | roberta_extension_mixture_of_adapters] EP2 loss=0.1252 valF1@0.5=0.8589\n","[sarcasm | TRAIN_en-AU | roberta_extension_mixture_of_adapters] EP3 loss=0.0506 valF1@0.5=0.8120\n","[sarcasm | TRAIN_en-AU | roberta_extension_mixture_of_adapters] EP4 loss=0.0800 valF1@0.5=0.8381\n","Early stopping.\n","✅ Saved: /content/drive/MyDrive/DNLP/models/roberta_extension_mixture_of_adapters/checkpoints/sarcasm__TRAIN_en-AU__roberta_extension_mixture_of_adapters__stage2_fromPOOL_routerFz.pt\n"]},{"output_type":"stream","name":"stderr","text":["Some weights of RobertaModel were not initialized from the model checkpoint at roberta-base and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"]},{"output_type":"stream","name":"stdout","text":["[sarcasm | TRAIN_en-IN] train label counts: n0=1170 n1=179 n=1349 | class_wts=[0.576,3.768]\n"]},{"output_type":"stream","name":"stderr","text":["Some weights of RobertaModel were not initialized from the model checkpoint at roberta-base and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"]},{"output_type":"stream","name":"stdout","text":["✅ Initialized from: sarcasm__FULL__roberta_extension_mixture_of_adapters__stage1_poolpretrain.pt\n","[sarcasm | TRAIN_en-IN | roberta_extension_mixture_of_adapters] EP1 loss=0.3451 valF1@0.5=0.7417\n","[sarcasm | TRAIN_en-IN | roberta_extension_mixture_of_adapters] EP2 loss=0.2160 valF1@0.5=0.7408\n","[sarcasm | TRAIN_en-IN | roberta_extension_mixture_of_adapters] EP3 loss=0.1178 valF1@0.5=0.8321\n","[sarcasm | TRAIN_en-IN | roberta_extension_mixture_of_adapters] EP4 loss=0.0634 valF1@0.5=0.8232\n","[sarcasm | TRAIN_en-IN | roberta_extension_mixture_of_adapters] EP5 loss=0.0912 valF1@0.5=0.8498\n","[sarcasm | TRAIN_en-IN | roberta_extension_mixture_of_adapters] EP6 loss=0.0981 valF1@0.5=0.8112\n","✅ Saved: /content/drive/MyDrive/DNLP/models/roberta_extension_mixture_of_adapters/checkpoints/sarcasm__TRAIN_en-IN__roberta_extension_mixture_of_adapters__stage2_fromPOOL_routerFz.pt\n"]},{"output_type":"stream","name":"stderr","text":["Some weights of RobertaModel were not initialized from the model checkpoint at roberta-base and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"]},{"output_type":"stream","name":"stdout","text":["[sarcasm | TRAIN_en-UK] train label counts: n0=643 n1=182 n=825 | class_wts=[0.642,2.266]\n"]},{"output_type":"stream","name":"stderr","text":["Some weights of RobertaModel were not initialized from the model checkpoint at roberta-base and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"]},{"output_type":"stream","name":"stdout","text":["✅ Initialized from: sarcasm__FULL__roberta_extension_mixture_of_adapters__stage1_poolpretrain.pt\n","[sarcasm | TRAIN_en-UK | roberta_extension_mixture_of_adapters] EP1 loss=0.2976 valF1@0.5=0.8958\n","[sarcasm | TRAIN_en-UK | roberta_extension_mixture_of_adapters] EP2 loss=0.1708 valF1@0.5=0.8623\n","[sarcasm | TRAIN_en-UK | roberta_extension_mixture_of_adapters] EP3 loss=0.1031 valF1@0.5=0.9005\n","[sarcasm | TRAIN_en-UK | roberta_extension_mixture_of_adapters] EP4 loss=0.0638 valF1@0.5=0.8958\n","[sarcasm | TRAIN_en-UK | roberta_extension_mixture_of_adapters] EP5 loss=0.0343 valF1@0.5=0.8449\n","Early stopping.\n","✅ Saved: /content/drive/MyDrive/DNLP/models/roberta_extension_mixture_of_adapters/checkpoints/sarcasm__TRAIN_en-UK__roberta_extension_mixture_of_adapters__stage2_fromPOOL_routerFz.pt\n"]},{"output_type":"stream","name":"stderr","text":["Some weights of RobertaModel were not initialized from the model checkpoint at roberta-base and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"]},{"output_type":"stream","name":"stdout","text":["\n","✅ Saved: /content/drive/MyDrive/DNLP/models/roberta_extension_mixture_of_adapters/metrics/moe_metrics_all.csv\n","✅ Saved: /content/drive/MyDrive/DNLP/models/roberta_extension_mixture_of_adapters/predictions/moe_predictions_all.csv\n","✅ Saved: /content/drive/MyDrive/DNLP/models/roberta_extension_mixture_of_adapters/metrics/moe_pervar_metrics_all.csv\n"]},{"output_type":"display_data","data":{"text/plain":["         task train_setting                                variant  \\\n","0   sentiment          FULL  roberta_extension_mixture_of_adapters   \n","1   sentiment          FULL  roberta_extension_mixture_of_adapters   \n","2   sentiment          FULL  roberta_extension_mixture_of_adapters   \n","3   sentiment          FULL  roberta_extension_mixture_of_adapters   \n","4   sentiment          FULL  roberta_extension_mixture_of_adapters   \n","5   sentiment          FULL  roberta_extension_mixture_of_adapters   \n","6   sentiment        Google  roberta_extension_mixture_of_adapters   \n","7   sentiment        Google  roberta_extension_mixture_of_adapters   \n","8   sentiment        Google  roberta_extension_mixture_of_adapters   \n","9   sentiment        Google  roberta_extension_mixture_of_adapters   \n","10  sentiment        Google  roberta_extension_mixture_of_adapters   \n","11  sentiment        Google  roberta_extension_mixture_of_adapters   \n","12  sentiment        Reddit  roberta_extension_mixture_of_adapters   \n","13  sentiment        Reddit  roberta_extension_mixture_of_adapters   \n","14  sentiment        Reddit  roberta_extension_mixture_of_adapters   \n","15  sentiment        Reddit  roberta_extension_mixture_of_adapters   \n","16  sentiment        Reddit  roberta_extension_mixture_of_adapters   \n","17  sentiment        Reddit  roberta_extension_mixture_of_adapters   \n","18  sentiment   TRAIN_en-AU  roberta_extension_mixture_of_adapters   \n","19  sentiment   TRAIN_en-AU  roberta_extension_mixture_of_adapters   \n","\n","   test_setting split     n       acc  precision    recall  macro_f1  \\\n","0     TEST_FULL  test  1212  0.885314   0.887273  0.885612  0.885218   \n","1   TEST_Google  test   603  0.905473   0.894342  0.848235  0.867766   \n","2   TEST_Reddit  test   609  0.865353   0.815489  0.841674  0.826912   \n","3    TEST_en-AU  test   371  0.913747   0.913172  0.914813  0.913565   \n","4    TEST_en-IN  test   455  0.813187   0.819774  0.813657  0.812371   \n","5    TEST_en-UK  test   386  0.943005   0.943447  0.942397  0.942819   \n","6     TEST_FULL  test  1212  0.893564   0.893870  0.893686  0.893558   \n","7   TEST_Google  test   603  0.917081   0.895927  0.881895  0.888574   \n","8   TEST_Reddit  test   609  0.870279   0.823080  0.838291  0.830156   \n","9    TEST_en-AU  test   371  0.908356   0.908592  0.907208  0.907792   \n","10   TEST_en-IN  test   455  0.841758   0.843363  0.841983  0.841629   \n","11   TEST_en-UK  test   386  0.940415   0.941516  0.939437  0.940164   \n","12    TEST_FULL  test  1212  0.870462   0.883374  0.871223  0.869520   \n","13  TEST_Google  test   603  0.888889   0.930241  0.783203  0.826281   \n","14  TEST_Reddit  test   609  0.852217   0.801023  0.855136  0.819585   \n","15   TEST_en-AU  test   371  0.902965   0.907576  0.907266  0.902964   \n","16   TEST_en-IN  test   455  0.793407   0.820069  0.794354  0.789368   \n","17   TEST_en-UK  test   386  0.930052   0.936986  0.927357  0.929330   \n","18    TEST_FULL  test  1212  0.889439   0.891510  0.889745  0.889341   \n","19  TEST_Google  test   603  0.918740   0.924224  0.857124  0.883997   \n","\n","   threshold_type  threshold  \n","0        fixed0.5        0.5  \n","1        fixed0.5        0.5  \n","2        fixed0.5        0.5  \n","3        fixed0.5        0.5  \n","4        fixed0.5        0.5  \n","5        fixed0.5        0.5  \n","6        fixed0.5        0.5  \n","7        fixed0.5        0.5  \n","8        fixed0.5        0.5  \n","9        fixed0.5        0.5  \n","10       fixed0.5        0.5  \n","11       fixed0.5        0.5  \n","12       fixed0.5        0.5  \n","13       fixed0.5        0.5  \n","14       fixed0.5        0.5  \n","15       fixed0.5        0.5  \n","16       fixed0.5        0.5  \n","17       fixed0.5        0.5  \n","18       fixed0.5        0.5  \n","19       fixed0.5        0.5  "],"text/html":["\n","  <div id=\"df-772dbff6-334e-49fd-8821-91967f76d3a4\" class=\"colab-df-container\">\n","    <div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>task</th>\n","      <th>train_setting</th>\n","      <th>variant</th>\n","      <th>test_setting</th>\n","      <th>split</th>\n","      <th>n</th>\n","      <th>acc</th>\n","      <th>precision</th>\n","      <th>recall</th>\n","      <th>macro_f1</th>\n","      <th>threshold_type</th>\n","      <th>threshold</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>sentiment</td>\n","      <td>FULL</td>\n","      <td>roberta_extension_mixture_of_adapters</td>\n","      <td>TEST_FULL</td>\n","      <td>test</td>\n","      <td>1212</td>\n","      <td>0.885314</td>\n","      <td>0.887273</td>\n","      <td>0.885612</td>\n","      <td>0.885218</td>\n","      <td>fixed0.5</td>\n","      <td>0.5</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>sentiment</td>\n","      <td>FULL</td>\n","      <td>roberta_extension_mixture_of_adapters</td>\n","      <td>TEST_Google</td>\n","      <td>test</td>\n","      <td>603</td>\n","      <td>0.905473</td>\n","      <td>0.894342</td>\n","      <td>0.848235</td>\n","      <td>0.867766</td>\n","      <td>fixed0.5</td>\n","      <td>0.5</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>sentiment</td>\n","      <td>FULL</td>\n","      <td>roberta_extension_mixture_of_adapters</td>\n","      <td>TEST_Reddit</td>\n","      <td>test</td>\n","      <td>609</td>\n","      <td>0.865353</td>\n","      <td>0.815489</td>\n","      <td>0.841674</td>\n","      <td>0.826912</td>\n","      <td>fixed0.5</td>\n","      <td>0.5</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>sentiment</td>\n","      <td>FULL</td>\n","      <td>roberta_extension_mixture_of_adapters</td>\n","      <td>TEST_en-AU</td>\n","      <td>test</td>\n","      <td>371</td>\n","      <td>0.913747</td>\n","      <td>0.913172</td>\n","      <td>0.914813</td>\n","      <td>0.913565</td>\n","      <td>fixed0.5</td>\n","      <td>0.5</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>sentiment</td>\n","      <td>FULL</td>\n","      <td>roberta_extension_mixture_of_adapters</td>\n","      <td>TEST_en-IN</td>\n","      <td>test</td>\n","      <td>455</td>\n","      <td>0.813187</td>\n","      <td>0.819774</td>\n","      <td>0.813657</td>\n","      <td>0.812371</td>\n","      <td>fixed0.5</td>\n","      <td>0.5</td>\n","    </tr>\n","    <tr>\n","      <th>5</th>\n","      <td>sentiment</td>\n","      <td>FULL</td>\n","      <td>roberta_extension_mixture_of_adapters</td>\n","      <td>TEST_en-UK</td>\n","      <td>test</td>\n","      <td>386</td>\n","      <td>0.943005</td>\n","      <td>0.943447</td>\n","      <td>0.942397</td>\n","      <td>0.942819</td>\n","      <td>fixed0.5</td>\n","      <td>0.5</td>\n","    </tr>\n","    <tr>\n","      <th>6</th>\n","      <td>sentiment</td>\n","      <td>Google</td>\n","      <td>roberta_extension_mixture_of_adapters</td>\n","      <td>TEST_FULL</td>\n","      <td>test</td>\n","      <td>1212</td>\n","      <td>0.893564</td>\n","      <td>0.893870</td>\n","      <td>0.893686</td>\n","      <td>0.893558</td>\n","      <td>fixed0.5</td>\n","      <td>0.5</td>\n","    </tr>\n","    <tr>\n","      <th>7</th>\n","      <td>sentiment</td>\n","      <td>Google</td>\n","      <td>roberta_extension_mixture_of_adapters</td>\n","      <td>TEST_Google</td>\n","      <td>test</td>\n","      <td>603</td>\n","      <td>0.917081</td>\n","      <td>0.895927</td>\n","      <td>0.881895</td>\n","      <td>0.888574</td>\n","      <td>fixed0.5</td>\n","      <td>0.5</td>\n","    </tr>\n","    <tr>\n","      <th>8</th>\n","      <td>sentiment</td>\n","      <td>Google</td>\n","      <td>roberta_extension_mixture_of_adapters</td>\n","      <td>TEST_Reddit</td>\n","      <td>test</td>\n","      <td>609</td>\n","      <td>0.870279</td>\n","      <td>0.823080</td>\n","      <td>0.838291</td>\n","      <td>0.830156</td>\n","      <td>fixed0.5</td>\n","      <td>0.5</td>\n","    </tr>\n","    <tr>\n","      <th>9</th>\n","      <td>sentiment</td>\n","      <td>Google</td>\n","      <td>roberta_extension_mixture_of_adapters</td>\n","      <td>TEST_en-AU</td>\n","      <td>test</td>\n","      <td>371</td>\n","      <td>0.908356</td>\n","      <td>0.908592</td>\n","      <td>0.907208</td>\n","      <td>0.907792</td>\n","      <td>fixed0.5</td>\n","      <td>0.5</td>\n","    </tr>\n","    <tr>\n","      <th>10</th>\n","      <td>sentiment</td>\n","      <td>Google</td>\n","      <td>roberta_extension_mixture_of_adapters</td>\n","      <td>TEST_en-IN</td>\n","      <td>test</td>\n","      <td>455</td>\n","      <td>0.841758</td>\n","      <td>0.843363</td>\n","      <td>0.841983</td>\n","      <td>0.841629</td>\n","      <td>fixed0.5</td>\n","      <td>0.5</td>\n","    </tr>\n","    <tr>\n","      <th>11</th>\n","      <td>sentiment</td>\n","      <td>Google</td>\n","      <td>roberta_extension_mixture_of_adapters</td>\n","      <td>TEST_en-UK</td>\n","      <td>test</td>\n","      <td>386</td>\n","      <td>0.940415</td>\n","      <td>0.941516</td>\n","      <td>0.939437</td>\n","      <td>0.940164</td>\n","      <td>fixed0.5</td>\n","      <td>0.5</td>\n","    </tr>\n","    <tr>\n","      <th>12</th>\n","      <td>sentiment</td>\n","      <td>Reddit</td>\n","      <td>roberta_extension_mixture_of_adapters</td>\n","      <td>TEST_FULL</td>\n","      <td>test</td>\n","      <td>1212</td>\n","      <td>0.870462</td>\n","      <td>0.883374</td>\n","      <td>0.871223</td>\n","      <td>0.869520</td>\n","      <td>fixed0.5</td>\n","      <td>0.5</td>\n","    </tr>\n","    <tr>\n","      <th>13</th>\n","      <td>sentiment</td>\n","      <td>Reddit</td>\n","      <td>roberta_extension_mixture_of_adapters</td>\n","      <td>TEST_Google</td>\n","      <td>test</td>\n","      <td>603</td>\n","      <td>0.888889</td>\n","      <td>0.930241</td>\n","      <td>0.783203</td>\n","      <td>0.826281</td>\n","      <td>fixed0.5</td>\n","      <td>0.5</td>\n","    </tr>\n","    <tr>\n","      <th>14</th>\n","      <td>sentiment</td>\n","      <td>Reddit</td>\n","      <td>roberta_extension_mixture_of_adapters</td>\n","      <td>TEST_Reddit</td>\n","      <td>test</td>\n","      <td>609</td>\n","      <td>0.852217</td>\n","      <td>0.801023</td>\n","      <td>0.855136</td>\n","      <td>0.819585</td>\n","      <td>fixed0.5</td>\n","      <td>0.5</td>\n","    </tr>\n","    <tr>\n","      <th>15</th>\n","      <td>sentiment</td>\n","      <td>Reddit</td>\n","      <td>roberta_extension_mixture_of_adapters</td>\n","      <td>TEST_en-AU</td>\n","      <td>test</td>\n","      <td>371</td>\n","      <td>0.902965</td>\n","      <td>0.907576</td>\n","      <td>0.907266</td>\n","      <td>0.902964</td>\n","      <td>fixed0.5</td>\n","      <td>0.5</td>\n","    </tr>\n","    <tr>\n","      <th>16</th>\n","      <td>sentiment</td>\n","      <td>Reddit</td>\n","      <td>roberta_extension_mixture_of_adapters</td>\n","      <td>TEST_en-IN</td>\n","      <td>test</td>\n","      <td>455</td>\n","      <td>0.793407</td>\n","      <td>0.820069</td>\n","      <td>0.794354</td>\n","      <td>0.789368</td>\n","      <td>fixed0.5</td>\n","      <td>0.5</td>\n","    </tr>\n","    <tr>\n","      <th>17</th>\n","      <td>sentiment</td>\n","      <td>Reddit</td>\n","      <td>roberta_extension_mixture_of_adapters</td>\n","      <td>TEST_en-UK</td>\n","      <td>test</td>\n","      <td>386</td>\n","      <td>0.930052</td>\n","      <td>0.936986</td>\n","      <td>0.927357</td>\n","      <td>0.929330</td>\n","      <td>fixed0.5</td>\n","      <td>0.5</td>\n","    </tr>\n","    <tr>\n","      <th>18</th>\n","      <td>sentiment</td>\n","      <td>TRAIN_en-AU</td>\n","      <td>roberta_extension_mixture_of_adapters</td>\n","      <td>TEST_FULL</td>\n","      <td>test</td>\n","      <td>1212</td>\n","      <td>0.889439</td>\n","      <td>0.891510</td>\n","      <td>0.889745</td>\n","      <td>0.889341</td>\n","      <td>fixed0.5</td>\n","      <td>0.5</td>\n","    </tr>\n","    <tr>\n","      <th>19</th>\n","      <td>sentiment</td>\n","      <td>TRAIN_en-AU</td>\n","      <td>roberta_extension_mixture_of_adapters</td>\n","      <td>TEST_Google</td>\n","      <td>test</td>\n","      <td>603</td>\n","      <td>0.918740</td>\n","      <td>0.924224</td>\n","      <td>0.857124</td>\n","      <td>0.883997</td>\n","      <td>fixed0.5</td>\n","      <td>0.5</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>\n","    <div class=\"colab-df-buttons\">\n","\n","  <div class=\"colab-df-container\">\n","    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-772dbff6-334e-49fd-8821-91967f76d3a4')\"\n","            title=\"Convert this dataframe to an interactive table.\"\n","            style=\"display:none;\">\n","\n","  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n","    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n","  </svg>\n","    </button>\n","\n","  <style>\n","    .colab-df-container {\n","      display:flex;\n","      gap: 12px;\n","    }\n","\n","    .colab-df-convert {\n","      background-color: #E8F0FE;\n","      border: none;\n","      border-radius: 50%;\n","      cursor: pointer;\n","      display: none;\n","      fill: #1967D2;\n","      height: 32px;\n","      padding: 0 0 0 0;\n","      width: 32px;\n","    }\n","\n","    .colab-df-convert:hover {\n","      background-color: #E2EBFA;\n","      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n","      fill: #174EA6;\n","    }\n","\n","    .colab-df-buttons div {\n","      margin-bottom: 4px;\n","    }\n","\n","    [theme=dark] .colab-df-convert {\n","      background-color: #3B4455;\n","      fill: #D2E3FC;\n","    }\n","\n","    [theme=dark] .colab-df-convert:hover {\n","      background-color: #434B5C;\n","      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n","      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n","      fill: #FFFFFF;\n","    }\n","  </style>\n","\n","    <script>\n","      const buttonEl =\n","        document.querySelector('#df-772dbff6-334e-49fd-8821-91967f76d3a4 button.colab-df-convert');\n","      buttonEl.style.display =\n","        google.colab.kernel.accessAllowed ? 'block' : 'none';\n","\n","      async function convertToInteractive(key) {\n","        const element = document.querySelector('#df-772dbff6-334e-49fd-8821-91967f76d3a4');\n","        const dataTable =\n","          await google.colab.kernel.invokeFunction('convertToInteractive',\n","                                                    [key], {});\n","        if (!dataTable) return;\n","\n","        const docLinkHtml = 'Like what you see? Visit the ' +\n","          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n","          + ' to learn more about interactive tables.';\n","        element.innerHTML = '';\n","        dataTable['output_type'] = 'display_data';\n","        await google.colab.output.renderOutput(dataTable, element);\n","        const docLink = document.createElement('div');\n","        docLink.innerHTML = docLinkHtml;\n","        element.appendChild(docLink);\n","      }\n","    </script>\n","  </div>\n","\n","\n","    </div>\n","  </div>\n"],"application/vnd.google.colaboratory.intrinsic+json":{"type":"dataframe","summary":"{\n  \"name\": \"display(metrics_moe\",\n  \"rows\": 20,\n  \"fields\": [\n    {\n      \"column\": \"task\",\n      \"properties\": {\n        \"dtype\": \"category\",\n        \"num_unique_values\": 1,\n        \"samples\": [\n          \"sentiment\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"train_setting\",\n      \"properties\": {\n        \"dtype\": \"category\",\n        \"num_unique_values\": 4,\n        \"samples\": [\n          \"Google\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"variant\",\n      \"properties\": {\n        \"dtype\": \"category\",\n        \"num_unique_values\": 1,\n        \"samples\": [\n          \"roberta_extension_mixture_of_adapters\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"test_setting\",\n      \"properties\": {\n        \"dtype\": \"category\",\n        \"num_unique_values\": 6,\n        \"samples\": [\n          \"TEST_FULL\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"split\",\n      \"properties\": {\n        \"dtype\": \"category\",\n        \"num_unique_values\": 1,\n        \"samples\": [\n          \"test\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"n\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 310,\n        \"min\": 371,\n        \"max\": 1212,\n        \"num_unique_values\": 6,\n        \"samples\": [\n          1212\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"acc\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0.03977850364543912,\n        \"min\": 0.7934065934065934,\n        \"max\": 0.9430051813471503,\n        \"num_unique_values\": 20,\n        \"samples\": [\n          0.8853135313531353\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"precision\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0.04644389323896467,\n        \"min\": 0.8010234097315847,\n        \"max\": 0.9434466019417476,\n        \"num_unique_values\": 20,\n        \"samples\": [\n          0.8872731350304675\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"recall\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0.04547797145561174,\n        \"min\": 0.7832026143790849,\n        \"max\": 0.942396685320706,\n        \"num_unique_values\": 20,\n        \"samples\": [\n          0.8856120867838926\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"macro_f1\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0.04484150490693914,\n        \"min\": 0.7893684500827358,\n        \"max\": 0.9428194354664943,\n        \"num_unique_values\": 20,\n        \"samples\": [\n          0.8852178107662299\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"threshold_type\",\n      \"properties\": {\n        \"dtype\": \"category\",\n        \"num_unique_values\": 1,\n        \"samples\": [\n          \"fixed0.5\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"threshold\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0.0,\n        \"min\": 0.5,\n        \"max\": 0.5,\n        \"num_unique_values\": 1,\n        \"samples\": [\n          0.5\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    }\n  ]\n}"}},"metadata":{}}]},{"cell_type":"markdown","metadata":{"id":"d957e3f5"},"source":["## Step 13 — Delta vs CE + error analysis\n","*   **Purpose**: Compare MoE results against the loaded CE baseline and identify top error corrections.\n","*   **Inputs**: MoE outputs, CE outputs.\n","*   **Outputs**: Delta CSV (`moe_delta_vs_ce.csv`) and top error correction tables.\n","*   **Assumptions**: Requires matching CE metrics/predictions to be loaded."]},{"cell_type":"code","source":["# ==== Cell 13: Delta vs CE + error analysis ====\n","\n","# ---- Delta vs CE (global metrics) ----\n","delta_path = MET_DIR / \"moe_delta_vs_ce.csv\"\n","\n","if metrics_ce is None:\n","    print(\"⚠️ CE metrics not loaded -> skipping delta.\")\n","else:\n","    mce = metrics_ce.copy()\n","    mce = mce[mce[\"variant\"].astype(str).str.lower() == \"ce\"].copy()\n","\n","    moe_key = metrics_moe[[\"task\",\"train_setting\",\"test_setting\",\"split\",\"macro_f1\",\"acc\"]].copy()\n","    moe_key.rename(columns={\"macro_f1\":\"moe_macro_f1\",\"acc\":\"moe_acc\"}, inplace=True)\n","\n","    ce_key  = mce[[\"task\",\"train_setting\",\"test_setting\",\"split\",\"macro_f1\",\"acc\"]].copy()\n","    ce_key.rename(columns={\"macro_f1\":\"ce_macro_f1\",\"acc\":\"ce_acc\"}, inplace=True)\n","\n","    joined = moe_key.merge(ce_key, on=[\"task\",\"train_setting\",\"test_setting\",\"split\"], how=\"left\")\n","    joined[\"delta_macro_f1\"] = joined[\"moe_macro_f1\"] - joined[\"ce_macro_f1\"]\n","    joined[\"delta_acc\"]      = joined[\"moe_acc\"]      - joined[\"ce_acc\"]\n","\n","    joined.to_csv(delta_path, index=False)\n","    print(\"✅ Saved delta:\", delta_path)\n","    display(joined.sort_values([\"task\",\"delta_macro_f1\"], ascending=[True, False]).head(30))\n","\n","# ---- Error analysis: CE wrong, MoE correct (per task, per train_setting, per test_setting) ----\n","# Uses row_id joins between CE preds and MoE preds\n","if preds_ce is None:\n","    print(\"⚠️ CE preds not loaded -> skipping CE-vs-MoE error tables.\")\n","else:\n","    out_rows = []\n","    ce = preds_ce.copy()\n","    moe = preds_moe.copy()\n","\n","    # --- FIX START ---\n","    # Ensure 'prob' column name consistency for CE predictions\n","    if 'prob1' in ce.columns and 'prob' not in ce.columns:\n","        ce.rename(columns={'prob1': 'prob'}, inplace=True)\n","\n","    # Ensure 'row_id' column exists for CE predictions (create if missing)\n","    if 'row_id' not in ce.columns:\n","        ce['row_id'] = np.arange(len(ce))\n","    # --- FIX END ---\n","\n","    # keep only matching key columns\n","    key_cols = [\"task\",\"train_setting\",\"test_setting\",\"row_id\"]\n","    ce = ce[key_cols + [\"label\",\"pred\",\"prob\",\"text\"]].rename(columns={\"pred\":\"ce_pred\",\"prob\":\"ce_prob\"})\n","    moe = moe[key_cols + [\"pred\",\"prob\"]].rename(columns={\"pred\":\"moe_pred\",\"prob\":\"moe_prob\"})\n","\n","    merged = moe.merge(ce, on=key_cols, how=\"inner\")\n","\n","    # CE wrong, MoE correct\n","    good = merged[(merged[\"ce_pred\"] != merged[\"label\"]) & (merged[\"moe_pred\"] == merged[\"label\"])].copy()\n","    # MoE regressions\n","    bad  = merged[(merged[\"ce_pred\"] == merged[\"label\"]) & (merged[\"moe_pred\"] != merged[\"label\"])].copy()\n","\n","    # Save top tables (by confidence gap)\n","    good[\"gain\"] = (good[\"moe_prob\"] - good[\"ce_prob\"]).abs()\n","    bad[\"loss\"]  = (bad[\"moe_prob\"] - bad[\"ce_prob\"]).abs()\n","\n","    good_path = ANA_DIR / \"ce_wrong_moe_correct_top.csv\"\n","    bad_path  = ANA_DIR / \"ce_correct_moe_wrong_top.csv\"\n","\n","    good.sort_values(\"gain\", ascending=False).head(200).to_csv(good_path, index=False)\n","    bad.sort_values(\"loss\", ascending=False).head(200).to_csv(bad_path, index=False)\n","\n","    print(\"✅ Saved:\", good_path)\n","    print(\"✅ Saved:\", bad_path)"],"metadata":{"id":"MZIhk8JiVRam","colab":{"base_uri":"https://localhost:8080/","height":1000},"executionInfo":{"status":"ok","timestamp":1769815398767,"user_tz":-60,"elapsed":153,"user":{"displayName":"DNLP Project","userId":"02984253218627728392"}},"outputId":"1326107c-d5bb-46f6-a6bf-394198f4e89c"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["✅ Saved delta: /content/drive/MyDrive/DNLP/models/roberta_extension_mixture_of_adapters/metrics/moe_delta_vs_ce.csv\n"]},{"output_type":"display_data","data":{"text/plain":["         task train_setting test_setting split  moe_macro_f1   moe_acc  \\\n","50    sarcasm   TRAIN_en-AU   TEST_en-UK  test      0.648671  0.702128   \n","53    sarcasm   TRAIN_en-IN   TEST_en-AU  test      0.650958  0.676349   \n","58    sarcasm   TRAIN_en-UK   TEST_en-AU  test      0.710728  0.734440   \n","51    sarcasm   TRAIN_en-IN    TEST_FULL  test      0.693805  0.772876   \n","52    sarcasm   TRAIN_en-IN  TEST_Reddit  test      0.693805  0.772876   \n","46    sarcasm   TRAIN_en-AU    TEST_FULL  test      0.698643  0.758170   \n","47    sarcasm   TRAIN_en-AU  TEST_Reddit  test      0.698643  0.758170   \n","56    sarcasm   TRAIN_en-UK    TEST_FULL  test      0.709219  0.789216   \n","57    sarcasm   TRAIN_en-UK  TEST_Reddit  test      0.709219  0.789216   \n","49    sarcasm   TRAIN_en-AU   TEST_en-IN  test      0.543091  0.795652   \n","55    sarcasm   TRAIN_en-IN   TEST_en-UK  test      0.703631  0.794326   \n","44    sarcasm          FULL   TEST_en-IN  test      0.688155  0.856522   \n","40    sarcasm          FULL   TEST_en-UK  test      0.687361  0.751773   \n","54    sarcasm   TRAIN_en-IN   TEST_en-IN  test      0.701734  0.860870   \n","39    sarcasm          FULL   TEST_en-IN  test      0.635833  0.834783   \n","41    sarcasm          FULL    TEST_FULL  test      0.706747  0.754902   \n","42    sarcasm          FULL  TEST_Reddit  test      0.706747  0.754902   \n","59    sarcasm   TRAIN_en-UK   TEST_en-IN  test      0.664295  0.860870   \n","36    sarcasm          FULL    TEST_FULL  test      0.706173  0.771242   \n","37    sarcasm          FULL  TEST_Reddit  test      0.706173  0.771242   \n","45    sarcasm          FULL   TEST_en-UK  test      0.654763  0.709220   \n","38    sarcasm          FULL   TEST_en-AU  test      0.707296  0.721992   \n","48    sarcasm   TRAIN_en-AU   TEST_en-AU  test      0.745930  0.755187   \n","43    sarcasm          FULL   TEST_en-AU  test      0.681749  0.684647   \n","60    sarcasm   TRAIN_en-UK   TEST_en-UK  test      0.670117  0.765957   \n","13  sentiment        Reddit  TEST_Google  test      0.826281  0.888889   \n","15  sentiment        Reddit   TEST_en-AU  test      0.902964  0.902965   \n","8   sentiment        Google  TEST_Reddit  test      0.830156  0.870279   \n","17  sentiment        Reddit   TEST_en-UK  test      0.929330  0.930052   \n","12  sentiment        Reddit    TEST_FULL  test      0.869520  0.870462   \n","\n","    ce_macro_f1    ce_acc  delta_macro_f1  delta_acc  \n","50     0.368667  0.368794        0.280004   0.333333  \n","53     0.423267  0.580913        0.227691   0.095436  \n","58     0.494624  0.543568        0.216104   0.190871  \n","51     0.535637  0.727124        0.158168   0.045752  \n","52     0.535637  0.727124        0.158168   0.045752  \n","46     0.554371  0.566993        0.144272   0.191176  \n","47     0.554371  0.566993        0.144272   0.191176  \n","56     0.582790  0.676471        0.126428   0.112745  \n","57     0.582790  0.676471        0.126428   0.112745  \n","49     0.426662  0.508696        0.116430   0.286957  \n","55     0.592149  0.801418        0.111482  -0.007092  \n","44     0.593766  0.778261        0.094389   0.078261  \n","40     0.624653  0.673759        0.062709   0.078014  \n","54     0.655145  0.834783        0.046589   0.026087  \n","39     0.593766  0.778261        0.042067   0.056522  \n","41     0.669401  0.717320        0.037346   0.037582  \n","42     0.669401  0.717320        0.037346   0.037582  \n","59     0.627269  0.739130        0.037026   0.121739  \n","36     0.669401  0.717320        0.036772   0.053922  \n","37     0.669401  0.717320        0.036772   0.053922  \n","45     0.624653  0.673759        0.030110   0.035461  \n","38     0.680639  0.684647        0.026657   0.037344  \n","48     0.737123  0.738589        0.008808   0.016598  \n","43     0.680639  0.684647        0.001110   0.000000  \n","60     0.669900  0.801418        0.000217  -0.035461  \n","13     0.712374  0.830846        0.113907   0.058043  \n","15     0.857076  0.857143        0.045888   0.045822  \n","8      0.787971  0.839080        0.042185   0.031199  \n","17     0.894145  0.896373        0.035185   0.033679  \n","12     0.846069  0.847360        0.023451   0.023102  "],"text/html":["\n","  <div id=\"df-ba96ed17-3e1f-46ce-ac35-1be7984bfb14\" class=\"colab-df-container\">\n","    <div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>task</th>\n","      <th>train_setting</th>\n","      <th>test_setting</th>\n","      <th>split</th>\n","      <th>moe_macro_f1</th>\n","      <th>moe_acc</th>\n","      <th>ce_macro_f1</th>\n","      <th>ce_acc</th>\n","      <th>delta_macro_f1</th>\n","      <th>delta_acc</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>50</th>\n","      <td>sarcasm</td>\n","      <td>TRAIN_en-AU</td>\n","      <td>TEST_en-UK</td>\n","      <td>test</td>\n","      <td>0.648671</td>\n","      <td>0.702128</td>\n","      <td>0.368667</td>\n","      <td>0.368794</td>\n","      <td>0.280004</td>\n","      <td>0.333333</td>\n","    </tr>\n","    <tr>\n","      <th>53</th>\n","      <td>sarcasm</td>\n","      <td>TRAIN_en-IN</td>\n","      <td>TEST_en-AU</td>\n","      <td>test</td>\n","      <td>0.650958</td>\n","      <td>0.676349</td>\n","      <td>0.423267</td>\n","      <td>0.580913</td>\n","      <td>0.227691</td>\n","      <td>0.095436</td>\n","    </tr>\n","    <tr>\n","      <th>58</th>\n","      <td>sarcasm</td>\n","      <td>TRAIN_en-UK</td>\n","      <td>TEST_en-AU</td>\n","      <td>test</td>\n","      <td>0.710728</td>\n","      <td>0.734440</td>\n","      <td>0.494624</td>\n","      <td>0.543568</td>\n","      <td>0.216104</td>\n","      <td>0.190871</td>\n","    </tr>\n","    <tr>\n","      <th>51</th>\n","      <td>sarcasm</td>\n","      <td>TRAIN_en-IN</td>\n","      <td>TEST_FULL</td>\n","      <td>test</td>\n","      <td>0.693805</td>\n","      <td>0.772876</td>\n","      <td>0.535637</td>\n","      <td>0.727124</td>\n","      <td>0.158168</td>\n","      <td>0.045752</td>\n","    </tr>\n","    <tr>\n","      <th>52</th>\n","      <td>sarcasm</td>\n","      <td>TRAIN_en-IN</td>\n","      <td>TEST_Reddit</td>\n","      <td>test</td>\n","      <td>0.693805</td>\n","      <td>0.772876</td>\n","      <td>0.535637</td>\n","      <td>0.727124</td>\n","      <td>0.158168</td>\n","      <td>0.045752</td>\n","    </tr>\n","    <tr>\n","      <th>46</th>\n","      <td>sarcasm</td>\n","      <td>TRAIN_en-AU</td>\n","      <td>TEST_FULL</td>\n","      <td>test</td>\n","      <td>0.698643</td>\n","      <td>0.758170</td>\n","      <td>0.554371</td>\n","      <td>0.566993</td>\n","      <td>0.144272</td>\n","      <td>0.191176</td>\n","    </tr>\n","    <tr>\n","      <th>47</th>\n","      <td>sarcasm</td>\n","      <td>TRAIN_en-AU</td>\n","      <td>TEST_Reddit</td>\n","      <td>test</td>\n","      <td>0.698643</td>\n","      <td>0.758170</td>\n","      <td>0.554371</td>\n","      <td>0.566993</td>\n","      <td>0.144272</td>\n","      <td>0.191176</td>\n","    </tr>\n","    <tr>\n","      <th>56</th>\n","      <td>sarcasm</td>\n","      <td>TRAIN_en-UK</td>\n","      <td>TEST_FULL</td>\n","      <td>test</td>\n","      <td>0.709219</td>\n","      <td>0.789216</td>\n","      <td>0.582790</td>\n","      <td>0.676471</td>\n","      <td>0.126428</td>\n","      <td>0.112745</td>\n","    </tr>\n","    <tr>\n","      <th>57</th>\n","      <td>sarcasm</td>\n","      <td>TRAIN_en-UK</td>\n","      <td>TEST_Reddit</td>\n","      <td>test</td>\n","      <td>0.709219</td>\n","      <td>0.789216</td>\n","      <td>0.582790</td>\n","      <td>0.676471</td>\n","      <td>0.126428</td>\n","      <td>0.112745</td>\n","    </tr>\n","    <tr>\n","      <th>49</th>\n","      <td>sarcasm</td>\n","      <td>TRAIN_en-AU</td>\n","      <td>TEST_en-IN</td>\n","      <td>test</td>\n","      <td>0.543091</td>\n","      <td>0.795652</td>\n","      <td>0.426662</td>\n","      <td>0.508696</td>\n","      <td>0.116430</td>\n","      <td>0.286957</td>\n","    </tr>\n","    <tr>\n","      <th>55</th>\n","      <td>sarcasm</td>\n","      <td>TRAIN_en-IN</td>\n","      <td>TEST_en-UK</td>\n","      <td>test</td>\n","      <td>0.703631</td>\n","      <td>0.794326</td>\n","      <td>0.592149</td>\n","      <td>0.801418</td>\n","      <td>0.111482</td>\n","      <td>-0.007092</td>\n","    </tr>\n","    <tr>\n","      <th>44</th>\n","      <td>sarcasm</td>\n","      <td>FULL</td>\n","      <td>TEST_en-IN</td>\n","      <td>test</td>\n","      <td>0.688155</td>\n","      <td>0.856522</td>\n","      <td>0.593766</td>\n","      <td>0.778261</td>\n","      <td>0.094389</td>\n","      <td>0.078261</td>\n","    </tr>\n","    <tr>\n","      <th>40</th>\n","      <td>sarcasm</td>\n","      <td>FULL</td>\n","      <td>TEST_en-UK</td>\n","      <td>test</td>\n","      <td>0.687361</td>\n","      <td>0.751773</td>\n","      <td>0.624653</td>\n","      <td>0.673759</td>\n","      <td>0.062709</td>\n","      <td>0.078014</td>\n","    </tr>\n","    <tr>\n","      <th>54</th>\n","      <td>sarcasm</td>\n","      <td>TRAIN_en-IN</td>\n","      <td>TEST_en-IN</td>\n","      <td>test</td>\n","      <td>0.701734</td>\n","      <td>0.860870</td>\n","      <td>0.655145</td>\n","      <td>0.834783</td>\n","      <td>0.046589</td>\n","      <td>0.026087</td>\n","    </tr>\n","    <tr>\n","      <th>39</th>\n","      <td>sarcasm</td>\n","      <td>FULL</td>\n","      <td>TEST_en-IN</td>\n","      <td>test</td>\n","      <td>0.635833</td>\n","      <td>0.834783</td>\n","      <td>0.593766</td>\n","      <td>0.778261</td>\n","      <td>0.042067</td>\n","      <td>0.056522</td>\n","    </tr>\n","    <tr>\n","      <th>41</th>\n","      <td>sarcasm</td>\n","      <td>FULL</td>\n","      <td>TEST_FULL</td>\n","      <td>test</td>\n","      <td>0.706747</td>\n","      <td>0.754902</td>\n","      <td>0.669401</td>\n","      <td>0.717320</td>\n","      <td>0.037346</td>\n","      <td>0.037582</td>\n","    </tr>\n","    <tr>\n","      <th>42</th>\n","      <td>sarcasm</td>\n","      <td>FULL</td>\n","      <td>TEST_Reddit</td>\n","      <td>test</td>\n","      <td>0.706747</td>\n","      <td>0.754902</td>\n","      <td>0.669401</td>\n","      <td>0.717320</td>\n","      <td>0.037346</td>\n","      <td>0.037582</td>\n","    </tr>\n","    <tr>\n","      <th>59</th>\n","      <td>sarcasm</td>\n","      <td>TRAIN_en-UK</td>\n","      <td>TEST_en-IN</td>\n","      <td>test</td>\n","      <td>0.664295</td>\n","      <td>0.860870</td>\n","      <td>0.627269</td>\n","      <td>0.739130</td>\n","      <td>0.037026</td>\n","      <td>0.121739</td>\n","    </tr>\n","    <tr>\n","      <th>36</th>\n","      <td>sarcasm</td>\n","      <td>FULL</td>\n","      <td>TEST_FULL</td>\n","      <td>test</td>\n","      <td>0.706173</td>\n","      <td>0.771242</td>\n","      <td>0.669401</td>\n","      <td>0.717320</td>\n","      <td>0.036772</td>\n","      <td>0.053922</td>\n","    </tr>\n","    <tr>\n","      <th>37</th>\n","      <td>sarcasm</td>\n","      <td>FULL</td>\n","      <td>TEST_Reddit</td>\n","      <td>test</td>\n","      <td>0.706173</td>\n","      <td>0.771242</td>\n","      <td>0.669401</td>\n","      <td>0.717320</td>\n","      <td>0.036772</td>\n","      <td>0.053922</td>\n","    </tr>\n","    <tr>\n","      <th>45</th>\n","      <td>sarcasm</td>\n","      <td>FULL</td>\n","      <td>TEST_en-UK</td>\n","      <td>test</td>\n","      <td>0.654763</td>\n","      <td>0.709220</td>\n","      <td>0.624653</td>\n","      <td>0.673759</td>\n","      <td>0.030110</td>\n","      <td>0.035461</td>\n","    </tr>\n","    <tr>\n","      <th>38</th>\n","      <td>sarcasm</td>\n","      <td>FULL</td>\n","      <td>TEST_en-AU</td>\n","      <td>test</td>\n","      <td>0.707296</td>\n","      <td>0.721992</td>\n","      <td>0.680639</td>\n","      <td>0.684647</td>\n","      <td>0.026657</td>\n","      <td>0.037344</td>\n","    </tr>\n","    <tr>\n","      <th>48</th>\n","      <td>sarcasm</td>\n","      <td>TRAIN_en-AU</td>\n","      <td>TEST_en-AU</td>\n","      <td>test</td>\n","      <td>0.745930</td>\n","      <td>0.755187</td>\n","      <td>0.737123</td>\n","      <td>0.738589</td>\n","      <td>0.008808</td>\n","      <td>0.016598</td>\n","    </tr>\n","    <tr>\n","      <th>43</th>\n","      <td>sarcasm</td>\n","      <td>FULL</td>\n","      <td>TEST_en-AU</td>\n","      <td>test</td>\n","      <td>0.681749</td>\n","      <td>0.684647</td>\n","      <td>0.680639</td>\n","      <td>0.684647</td>\n","      <td>0.001110</td>\n","      <td>0.000000</td>\n","    </tr>\n","    <tr>\n","      <th>60</th>\n","      <td>sarcasm</td>\n","      <td>TRAIN_en-UK</td>\n","      <td>TEST_en-UK</td>\n","      <td>test</td>\n","      <td>0.670117</td>\n","      <td>0.765957</td>\n","      <td>0.669900</td>\n","      <td>0.801418</td>\n","      <td>0.000217</td>\n","      <td>-0.035461</td>\n","    </tr>\n","    <tr>\n","      <th>13</th>\n","      <td>sentiment</td>\n","      <td>Reddit</td>\n","      <td>TEST_Google</td>\n","      <td>test</td>\n","      <td>0.826281</td>\n","      <td>0.888889</td>\n","      <td>0.712374</td>\n","      <td>0.830846</td>\n","      <td>0.113907</td>\n","      <td>0.058043</td>\n","    </tr>\n","    <tr>\n","      <th>15</th>\n","      <td>sentiment</td>\n","      <td>Reddit</td>\n","      <td>TEST_en-AU</td>\n","      <td>test</td>\n","      <td>0.902964</td>\n","      <td>0.902965</td>\n","      <td>0.857076</td>\n","      <td>0.857143</td>\n","      <td>0.045888</td>\n","      <td>0.045822</td>\n","    </tr>\n","    <tr>\n","      <th>8</th>\n","      <td>sentiment</td>\n","      <td>Google</td>\n","      <td>TEST_Reddit</td>\n","      <td>test</td>\n","      <td>0.830156</td>\n","      <td>0.870279</td>\n","      <td>0.787971</td>\n","      <td>0.839080</td>\n","      <td>0.042185</td>\n","      <td>0.031199</td>\n","    </tr>\n","    <tr>\n","      <th>17</th>\n","      <td>sentiment</td>\n","      <td>Reddit</td>\n","      <td>TEST_en-UK</td>\n","      <td>test</td>\n","      <td>0.929330</td>\n","      <td>0.930052</td>\n","      <td>0.894145</td>\n","      <td>0.896373</td>\n","      <td>0.035185</td>\n","      <td>0.033679</td>\n","    </tr>\n","    <tr>\n","      <th>12</th>\n","      <td>sentiment</td>\n","      <td>Reddit</td>\n","      <td>TEST_FULL</td>\n","      <td>test</td>\n","      <td>0.869520</td>\n","      <td>0.870462</td>\n","      <td>0.846069</td>\n","      <td>0.847360</td>\n","      <td>0.023451</td>\n","      <td>0.023102</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>\n","    <div class=\"colab-df-buttons\">\n","\n","  <div class=\"colab-df-container\">\n","    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-ba96ed17-3e1f-46ce-ac35-1be7984bfb14')\"\n","            title=\"Convert this dataframe to an interactive table.\"\n","            style=\"display:none;\">\n","\n","  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n","    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n","  </svg>\n","    </button>\n","\n","  <style>\n","    .colab-df-container {\n","      display:flex;\n","      gap: 12px;\n","    }\n","\n","    .colab-df-convert {\n","      background-color: #E8F0FE;\n","      border: none;\n","      border-radius: 50%;\n","      cursor: pointer;\n","      display: none;\n","      fill: #1967D2;\n","      height: 32px;\n","      padding: 0 0 0 0;\n","      width: 32px;\n","    }\n","\n","    .colab-df-convert:hover {\n","      background-color: #E2EBFA;\n","      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n","      fill: #174EA6;\n","    }\n","\n","    .colab-df-buttons div {\n","      margin-bottom: 4px;\n","    }\n","\n","    [theme=dark] .colab-df-convert {\n","      background-color: #3B4455;\n","      fill: #D2E3FC;\n","    }\n","\n","    [theme=dark] .colab-df-convert:hover {\n","      background-color: #434B5C;\n","      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n","      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n","      fill: #FFFFFF;\n","    }\n","  </style>\n","\n","    <script>\n","      const buttonEl =\n","        document.querySelector('#df-ba96ed17-3e1f-46ce-ac35-1be7984bfb14 button.colab-df-convert');\n","      buttonEl.style.display =\n","        google.colab.kernel.accessAllowed ? 'block' : 'none';\n","\n","      async function convertToInteractive(key) {\n","        const element = document.querySelector('#df-ba96ed17-3e1f-46ce-ac35-1be7984bfb14');\n","        const dataTable =\n","          await google.colab.kernel.invokeFunction('convertToInteractive',\n","                                                    [key], {});\n","        if (!dataTable) return;\n","\n","        const docLinkHtml = 'Like what you see? Visit the ' +\n","          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n","          + ' to learn more about interactive tables.';\n","        element.innerHTML = '';\n","        dataTable['output_type'] = 'display_data';\n","        await google.colab.output.renderOutput(dataTable, element);\n","        const docLink = document.createElement('div');\n","        docLink.innerHTML = docLinkHtml;\n","        element.appendChild(docLink);\n","      }\n","    </script>\n","  </div>\n","\n","\n","    </div>\n","  </div>\n"],"application/vnd.google.colaboratory.intrinsic+json":{"type":"dataframe","summary":"{\n  \"name\": \"    print(\\\"\\u2705 Saved:\\\", bad_path)\",\n  \"rows\": 30,\n  \"fields\": [\n    {\n      \"column\": \"task\",\n      \"properties\": {\n        \"dtype\": \"category\",\n        \"num_unique_values\": 2,\n        \"samples\": [\n          \"sentiment\",\n          \"sarcasm\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"train_setting\",\n      \"properties\": {\n        \"dtype\": \"category\",\n        \"num_unique_values\": 6,\n        \"samples\": [\n          \"TRAIN_en-AU\",\n          \"TRAIN_en-IN\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"test_setting\",\n      \"properties\": {\n        \"dtype\": \"category\",\n        \"num_unique_values\": 6,\n        \"samples\": [\n          \"TEST_en-UK\",\n          \"TEST_en-AU\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"split\",\n      \"properties\": {\n        \"dtype\": \"category\",\n        \"num_unique_values\": 1,\n        \"samples\": [\n          \"test\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"moe_macro_f1\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0.08073202372406815,\n        \"min\": 0.5430914239824168,\n        \"max\": 0.9293303949822004,\n        \"num_unique_values\": 25,\n        \"samples\": [\n          0.6881548132626648\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"moe_acc\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0.06606020603473864,\n        \"min\": 0.6763485477178424,\n        \"max\": 0.9300518134715026,\n        \"num_unique_values\": 24,\n        \"samples\": [\n          0.8565217391304348\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"ce_macro_f1\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0.12223900948442659,\n        \"min\": 0.3686673039191024,\n        \"max\": 0.8941450706156588,\n        \"num_unique_values\": 21,\n        \"samples\": [\n          0.3686673039191024\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"ce_acc\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0.11728747808576301,\n        \"min\": 0.3687943262411347,\n        \"max\": 0.8963730569948186,\n        \"num_unique_values\": 20,\n        \"samples\": [\n          0.3687943262411347\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"delta_macro_f1\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0.07209537933331031,\n        \"min\": 0.00021731424255966303,\n        \"max\": 0.2800037924264125,\n        \"num_unique_values\": 25,\n        \"samples\": [\n          0.09438857949643109\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"delta_acc\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0.08376191020697261,\n        \"min\": -0.03546099290780147,\n        \"max\": 0.3333333333333334,\n        \"num_unique_values\": 25,\n        \"samples\": [\n          0.0782608695652175\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    }\n  ]\n}"}},"metadata":{}},{"output_type":"stream","name":"stdout","text":["✅ Saved: /content/drive/MyDrive/DNLP/models/roberta_extension_mixture_of_adapters/analysis/ce_wrong_moe_correct_top.csv\n","✅ Saved: /content/drive/MyDrive/DNLP/models/roberta_extension_mixture_of_adapters/analysis/ce_correct_moe_wrong_top.csv\n"]}]},{"cell_type":"markdown","metadata":{"id":"761cd8f5"},"source":["## Step 14 — Paper-style plots\n","*   **Purpose**: Generate visualizations (Heatmaps, Locale Bar Charts) to analyze cross-variety performance.\n","*   **Inputs**: Metrics CSVs (`moe_metrics_all.csv`, `moe_pervar_metrics_all.csv`).\n","*   **Outputs**: PNG files in `figures/`."]},{"cell_type":"code","source":["# ==== Cell 14: Paper-style plots (locale bars + cross-variety heatmaps) ====\n","import matplotlib.pyplot as plt\n","import seaborn as sns\n","\n","sns.set(style=\"whitegrid\")\n","\n","# Paper-like locale palette (sampled from paper figure)\n","LOCALE_COLORS = {\n","    \"en-AU\": \"#708EBF\",  # blue\n","    \"en-IN\": \"#FFB16E\",  # orange\n","    \"en-UK\": \"#E7797A\",  # red\n","    \"AU\": \"#708EBF\",\n","    \"IN\": \"#FFB16E\",\n","    \"UK\": \"#E7797A\",\n","}\n","\n","def normalize_locale_name(x):\n","    x = str(x)\n","    x = x.replace(\"TRAIN_\", \"\").replace(\"TEST_\", \"\")\n","    x = x.replace(\"_\", \"-\")\n","    if x in [\"en-au\",\"en-AU\"]: return \"en-AU\"\n","    if x in [\"en-in\",\"en-IN\"]: return \"en-IN\"\n","    if x in [\"en-uk\",\"en-UK\"]: return \"en-UK\"\n","    if x in [\"AU\"]: return \"en-AU\"\n","    if x in [\"IN\"]: return \"en-IN\"\n","    if x in [\"UK\"]: return \"en-UK\"\n","    return x\n","\n","def plot_locale_bars_from_pervar(task, train_setting, test_setting, pervar_df, out_png):\n","    d = pervar_df[(pervar_df[\"task\"]==task) &\n","                  (pervar_df[\"train_setting\"]==train_setting) &\n","                  (pervar_df[\"test_setting\"]==test_setting) &\n","                  (pervar_df[\"variant\"]==VARIANT)].copy()\n","    if len(d)==0:\n","        print(\"skip bars (no rows):\", task, train_setting, test_setting)\n","        return\n","    d[\"locale\"] = d[\"variety_name\"].map(normalize_locale_name)\n","\n","    # sort locales AU, IN, UK if present\n","    order = [x for x in [\"en-AU\",\"en-IN\",\"en-UK\"] if x in set(d[\"locale\"])]\n","    if not order:\n","        order = sorted(d[\"locale\"].unique().tolist())\n","\n","    d = d.sort_values(\"locale\")\n","    vals = [float(d[d[\"locale\"]==loc][\"macro_f1\"].iloc[0]) for loc in order]\n","    cols = [LOCALE_COLORS.get(loc, \"#999999\") for loc in order]\n","\n","    plt.figure(figsize=(6,4))\n","    bars = plt.bar(order, vals, color=cols)\n","    plt.ylim(0, 1.0)\n","    plt.ylabel(\"Macro-F1\")\n","    plt.title(f\"{task.upper()} | {train_setting} -> {test_setting} | {VARIANT}\")\n","    for b, v in zip(bars, vals):\n","        plt.text(b.get_x()+b.get_width()/2, v+0.01, f\"{v:.2f}\", ha=\"center\", fontsize=10)\n","    plt.tight_layout()\n","    plt.savefig(out_png, dpi=200)\n","    plt.close()\n","    print(\"✅ Saved:\", out_png)\n","\n","def plot_cross_variety_heatmap(task, metric_csv, out_prefix):\n","    df = pd.read_csv(metric_csv)\n","    df.columns = [c.strip().lower() for c in df.columns]\n","    df = df[(df[\"task\"]==task) & (df[\"split\"]==\"test\")].copy()\n","\n","    # Cross-variety: train_setting in TRAIN_en-*, test_setting in TEST_en-*\n","    tr = df[df[\"train_setting\"].astype(str).str.startswith(\"TRAIN_\")].copy()\n","    tr = tr[tr[\"test_setting\"].astype(str).str.startswith(\"TEST_en-\")].copy()\n","    if len(tr)==0:\n","        print(\"skip heatmap (no cross-variety rows):\", task)\n","        return\n","\n","    tr[\"train_var\"] = tr[\"train_setting\"].astype(str).str.replace(\"TRAIN_\", \"\", regex=False)\n","    tr[\"test_var\"]  = tr[\"test_setting\"].astype(str).str.replace(\"TEST_\", \"\", regex=False)\n","\n","    # build CE pivot if available\n","    piv_moe = tr[tr[\"variant\"].astype(str)==VARIANT].pivot_table(\n","        index=\"train_var\", columns=\"test_var\", values=\"macro_f1\", aggfunc=\"mean\"\n","    )\n","\n","    plt.figure(figsize=(5.5,4.5))\n","    sns.heatmap(piv_moe, annot=True, fmt=\".3f\", cbar=False, linewidths=0.8, linecolor=\"white\",\n","                cmap=\"Reds\" if task==\"sentiment\" else \"Blues\", vmin=0.0, vmax=1.0)\n","    plt.title(f\"{task.upper()} | MoE cross-variety (Macro-F1)\")\n","    plt.xlabel(\"Tested On\")\n","    plt.ylabel(\"Trained On\")\n","    plt.tight_layout()\n","    out1 = PLT_DIR / f\"{out_prefix}__{task}__moe_crossvar_heatmap.png\"\n","    plt.savefig(out1, dpi=220)\n","    plt.close()\n","    print(\"✅ Saved:\", out1)\n","\n","    # Delta vs CE heatmap (if delta exists)\n","    delta_file = MET_DIR / \"moe_delta_vs_ce.csv\"\n","    if delta_file.exists():\n","        dd = pd.read_csv(delta_file)\n","        dd.columns = [c.strip().lower() for c in dd.columns]\n","        dd = dd[(dd[\"task\"]==task) &\n","                (dd[\"train_setting\"].astype(str).str.startswith(\"TRAIN_\")) &\n","                (dd[\"test_setting\"].astype(str).str.startswith(\"TEST_en-\")) &\n","                (dd[\"split\"]==\"test\")].copy()\n","        if len(dd)>0:\n","            dd[\"train_var\"] = dd[\"train_setting\"].astype(str).str.replace(\"TRAIN_\", \"\", regex=False)\n","            dd[\"test_var\"]  = dd[\"test_setting\"].astype(str).str.replace(\"TEST_\", \"\", regex=False)\n","            piv_d = dd.pivot_table(index=\"train_var\", columns=\"test_var\", values=\"delta_macro_f1\", aggfunc=\"mean\")\n","\n","            plt.figure(figsize=(5.5,4.5))\n","            sns.heatmap(piv_d, annot=True, fmt=\".3f\", cbar=False, linewidths=0.8, linecolor=\"white\",\n","                        cmap=\"RdBu_r\", center=0.0)\n","            plt.title(f\"{task.upper()} | Δ(MoE-CE) cross-variety (Macro-F1)\")\n","            plt.xlabel(\"Tested On\")\n","            plt.ylabel(\"Trained On\")\n","            plt.tight_layout()\n","            out2 = PLT_DIR / f\"{out_prefix}__{task}__delta_crossvar_heatmap.png\"\n","            plt.savefig(out2, dpi=220)\n","            plt.close()\n","            print(\"✅ Saved:\", out2)\n","\n","# ---- Generate key plots ----\n","# 1) Cross-variety heatmaps (MoE + Δ)\n","plot_cross_variety_heatmap(\"sarcasm\", MET_DIR / \"moe_metrics_all.csv\", out_prefix=\"FIG_crossvar\")\n","plot_cross_variety_heatmap(\"sentiment\", MET_DIR / \"moe_metrics_all.csv\", out_prefix=\"FIG_crossvar\")\n","\n","# 2) Locale bars (sentiment): show Google and Reddit testsets if they exist\n","pervar_df = pd.read_csv(MET_DIR / \"moe_pervar_metrics_all.csv\")\n","pervar_df.columns = [c.strip().lower() for c in pervar_df.columns]\n","\n","for test_setting in [\"TEST_Google\", \"TEST_Reddit\", \"TEST_FULL\"]:\n","    out = PLT_DIR / f\"FIG_localeBars__sentiment__FULL_to_{test_setting}.png\"\n","    plot_locale_bars_from_pervar(\"sentiment\", \"FULL\", test_setting, pervar_df, out)\n","\n","# 3) Locale bars (sarcasm): typically Reddit + FULL\n","for test_setting in [\"TEST_Reddit\", \"TEST_FULL\"]:\n","    out = PLT_DIR / f\"FIG_localeBars__sarcasm__FULL_to_{test_setting}.png\"\n","    plot_locale_bars_from_pervar(\"sarcasm\", \"FULL\", test_setting, pervar_df, out)\n","\n","print(\"✅ Plotting finished. Check:\", PLT_DIR)"],"metadata":{"id":"y6klXJb_VV_g","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1769814101019,"user_tz":-60,"elapsed":2049,"user":{"displayName":"DNLP Project","userId":"02984253218627728392"}},"outputId":"b36b2f05-c845-48e4-ca7e-375398336cdd"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["✅ Saved: /content/drive/MyDrive/DNLP/models/roberta_extension_mixture_of_adapters/plots/FIG_crossvar__sarcasm__moe_crossvar_heatmap.png\n","✅ Saved: /content/drive/MyDrive/DNLP/models/roberta_extension_mixture_of_adapters/plots/FIG_crossvar__sarcasm__delta_crossvar_heatmap.png\n","✅ Saved: /content/drive/MyDrive/DNLP/models/roberta_extension_mixture_of_adapters/plots/FIG_crossvar__sentiment__moe_crossvar_heatmap.png\n","✅ Saved: /content/drive/MyDrive/DNLP/models/roberta_extension_mixture_of_adapters/plots/FIG_crossvar__sentiment__delta_crossvar_heatmap.png\n","✅ Saved: /content/drive/MyDrive/DNLP/models/roberta_extension_mixture_of_adapters/plots/FIG_localeBars__sentiment__FULL_to_TEST_Google.png\n","✅ Saved: /content/drive/MyDrive/DNLP/models/roberta_extension_mixture_of_adapters/plots/FIG_localeBars__sentiment__FULL_to_TEST_Reddit.png\n","✅ Saved: /content/drive/MyDrive/DNLP/models/roberta_extension_mixture_of_adapters/plots/FIG_localeBars__sentiment__FULL_to_TEST_FULL.png\n","✅ Saved: /content/drive/MyDrive/DNLP/models/roberta_extension_mixture_of_adapters/plots/FIG_localeBars__sarcasm__FULL_to_TEST_Reddit.png\n","✅ Saved: /content/drive/MyDrive/DNLP/models/roberta_extension_mixture_of_adapters/plots/FIG_localeBars__sarcasm__FULL_to_TEST_FULL.png\n","✅ Plotting finished. Check: /content/drive/MyDrive/DNLP/models/roberta_extension_mixture_of_adapters/plots\n"]}]},{"cell_type":"markdown","metadata":{"id":"5fd716f8"},"source":["## Step 15 — Leakage audit\n","*   **Purpose**: Verify that there is no row-overlap between Train/Val and Test sets.\n","*   **Inputs**: Task indices.\n","*   **Outputs**: Audit status prints (✅ or ❌)."]},{"cell_type":"code","source":["# ==== Cell 15: Leakage audit (row_id overlap train/val vs testsets) ====\n","import pandas as pd\n","from pathlib import Path\n","\n","def _read_row_ids(csv_path: Path):\n","    df = pd.read_csv(csv_path)\n","    df.columns = [c.strip().lower() for c in df.columns]\n","    if \"row_id\" not in df.columns:\n","        raise ValueError(f\"{csv_path} missing row_id column.\")\n","    return set(df[\"row_id\"].astype(int).tolist())\n","\n","def audit_task(task: str):\n","    settings_df, testsets_df = load_task_indices(task)\n","\n","    # 1) train vs val disjoint per setting\n","    tv_bad = 0\n","    for _, r in settings_df.iterrows():\n","        tr_ids = _read_row_ids(r[\"train_csv_abs\"])\n","        va_ids = _read_row_ids(r[\"val_csv_abs\"])\n","        inter = tr_ids & va_ids\n","        if inter:\n","            tv_bad += 1\n","            print(f\"❌ train∩val overlap | {task} | {r['train_setting']}: {len(inter)}\")\n","    if tv_bad == 0:\n","        print(f\"✅ {task}: All settings train∩val overlap = 0\")\n","\n","    # 2) train+val vs each testset disjoint\n","    leak = 0\n","    for _, r in settings_df.iterrows():\n","        trva = _read_row_ids(r[\"train_csv_abs\"]) | _read_row_ids(r[\"val_csv_abs\"])\n","        for _, t in testsets_df.iterrows():\n","            te_ids = _read_row_ids(t[\"test_csv_abs\"])\n","            inter = trva & te_ids\n","            if inter:\n","                leak += 1\n","                print(f\"🚨 LEAKAGE | {task} | {r['train_setting']} vs {t['test_setting']}: overlap={len(inter)}\")\n","\n","    if leak == 0:\n","        print(f\"✅ {task}: No train/val ↔ test overlap found.\")\n","    else:\n","        print(f\"🚨 {task}: Found leakage in {leak} (setting,test) pairs.\")\n","\n","audit_task(\"sarcasm\")\n","audit_task(\"sentiment\")"],"metadata":{"id":"hiQBQiEkVYxu","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1769814109296,"user_tz":-60,"elapsed":1514,"user":{"displayName":"DNLP Project","userId":"02984253218627728392"}},"outputId":"78282297-9abb-4878-a395-e832f8f9182a"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["✅ sarcasm: All settings train∩val overlap = 0\n","✅ sarcasm: No train/val ↔ test overlap found.\n","✅ sentiment: All settings train∩val overlap = 0\n","✅ sentiment: No train/val ↔ test overlap found.\n"]}]}]}