{
 "nbformat": 4,
 "nbformat_minor": 0,
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# Data Preprocessing"
   ],
   "metadata": {
    "id": "eTUX1t6yjpxZ"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "This pipeline transforms raw text data into a token format for Transformer-based encoder models (BERT/RoBERTa) and decoder model for detecting sentiment/sarcasm texts."
   ],
   "metadata": {
    "id": "TckXdMSt1Cvm"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Import Libraries"
   ],
   "metadata": {
    "id": "OD75R0FjFs85"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "import re\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from datasets import load_dataset\n",
    "import torch\n",
    "from transformers import AutoTokenizer\n",
    "from torch.utils.data import Dataset\n",
    "import random\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "\n",
    "\n",
    "from transformers import AutoTokenizer\n",
    "\n",
    "from google.colab import drive\n",
    "drive.mount('/content/drive')"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "tI8CSvJY0q-h",
    "executionInfo": {
     "status": "ok",
     "timestamp": 1766082938702,
     "user_tz": -60,
     "elapsed": 60444,
     "user": {
      "displayName": "Balzhan Daurenbekovna",
      "userId": "10210039747879563426"
     }
    },
    "outputId": "00ee75fe-a673-418e-bae3-d1986849dbdc"
   },
   "execution_count": null,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Mounted at /content/drive\n"
     ]
    }
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "##Data Loading"
   ],
   "metadata": {
    "id": "7Pg9jEtuzmLM"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "dataset = load_dataset(\"unswnlporg/BESSTIE\")\n",
    "train = dataset[\"train\"].to_pandas()\n",
    "val = dataset[\"validation\"].to_pandas()\n",
    "\n",
    "train.head()"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 490,
     "referenced_widgets": [
      "15212b2199144ce0af4630408511e6eb",
      "5e5a1fb525bb41d1afb449809e1c2b00",
      "1fb079f38aea4660a86dda4ea2cd9ca9",
      "dff4e969afc04f2ebe953c92a90e8216",
      "419e09c93e5f407fb1134f7430759907",
      "d83f0c67c1354c85a3228357e3fe6877",
      "7abe7cde8bf04c76971d208e7e026983",
      "a5cbd87d62fd49f8b3fd4016e47a8654",
      "4cefb15b1f6146fc9c0d91e5047bc3bb",
      "694d1f8824824c0d9b3e0c8e8fa9bfaa",
      "10cfefa4197e4bdbb67db64097bbd16b",
      "aa9a77a2398a4ba3bd99f0bedcf96c3c",
      "7134896448294501957f477229f6c35e",
      "2b8fcaf8a48e46f09d1693d3c1b07121",
      "be6053da9f7f42839326767f2da7562d",
      "6e4b8c64562146658351e3ce83239a18",
      "32a67e58d3534a77a592e0b53b39ed54",
      "c44d4f9176774fb6b0f424c8f2b28bd8",
      "fee0e0300d1a42179b722d3ddc0fdc99",
      "3eb3966a342a4d229213cb5dca44cc48",
      "80df947d430d442a8ef2d75a360087bb",
      "09eb89b21f4643bda355eb4559244ea7",
      "57df49d9ae714f5c83d67fc0d3ea2130",
      "d7846a992f804b919e3c83430f365175",
      "baa05ff7ad424349b385faf112f2f262",
      "78ff71cf62ff4855866f0b0aa1bb696e",
      "433a0c3b14c14d56b86afa62f385843c",
      "a07444ecf023455c94222b16c7a25c9c",
      "3da4c62a275041078e67d0699d79a65d",
      "005ed50d083a415eb810d527074dfb58",
      "6524b6f832374f84a6ce8e1add99eb8a",
      "0f655610b83c43dfa0f94c21ed99a3b6",
      "a278ecfda99149af8501d487a57eb3ed",
      "3580b248f2a7412ca198dbb277d1398d",
      "7657a125b4a04ca7a6ac1211de40d462",
      "0a1639afbdbe4e5182efda027e7f6542",
      "cc7b3b05dd764aab8f0b92bfcfdb583c",
      "f7f0c04c5e8a4fde8f2042637276796a",
      "01f9a7bc96c546ab823ac0067eb5d13f",
      "d072202b9b3a4aaaa48b0a37de676eff",
      "273a1d223e324e21b2c3bd25c985f1e1",
      "0111abf22b2e4c3da590b3d327fe528b",
      "9cb56b6286164bf7b9cb4b594678a1b1",
      "8217e8f75c224c90b046352d797c8135",
      "cfe18b3b854145a792b98fbab1f315f8",
      "dd671c24c43344fb980329b8cf3eae1c",
      "6930d7bf5e90461ab7489e10282dd09a",
      "5bc854a0429049e893956a2ec9a3d194",
      "df0df1428a764f5481e29e669e3eabdd",
      "a2791391f7784bba98ace37cf77f13d1",
      "a0df246a481b49e79cbdbede521e96e6",
      "932261d644fa47c2abc8cf74c191f2a4",
      "f6d5a7c7ca2d4370ab76ee2c524a7656",
      "fd55d5b6715240c78d8a7e21d0788383",
      "cd812a939d6a4c338d39b58cad1b2530"
     ]
    },
    "id": "-0Xd-TcOzn5F",
    "executionInfo": {
     "status": "ok",
     "timestamp": 1766083012710,
     "user_tz": -60,
     "elapsed": 5139,
     "user": {
      "displayName": "Balzhan Daurenbekovna",
      "userId": "10210039747879563426"
     }
    },
    "outputId": "75052adb-b778-40b8-9b17-e96b30704b73"
   },
   "execution_count": null,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "/usr/local/lib/python3.12/dist-packages/huggingface_hub/utils/_auth.py:94: UserWarning: \n",
      "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
      "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
      "You will be able to reuse this secret in all of your notebooks.\n",
      "Please note that authentication is recommended but still optional to access public models or datasets.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "README.md: 0.00B [00:00, ?B/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "15212b2199144ce0af4630408511e6eb"
      }
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "train.csv: 0.00B [00:00, ?B/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "aa9a77a2398a4ba3bd99f0bedcf96c3c"
      }
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "valid.csv: 0.00B [00:00, ?B/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "57df49d9ae714f5c83d67fc0d3ea2130"
      }
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "Generating train split:   0%|          | 0/17760 [00:00<?, ? examples/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "3580b248f2a7412ca198dbb277d1398d"
      }
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "Generating validation split:   0%|          | 0/2428 [00:00<?, ? examples/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "cfe18b3b854145a792b98fbab1f315f8"
      }
     },
     "metadata": {}
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "                                                text  label variety  source  \\\n",
       "0  This was one of the best dishes I've EVER had!...      1   en-AU  Google   \n",
       "1  This Mexican restaurant in Penrith is a great ...      1   en-AU  Google   \n",
       "2  This was not to bad, I ordered the big pork ri...      1   en-AU  Google   \n",
       "3  Clean cool and a nice smaller casino to check ...      1   en-AU  Google   \n",
       "4  Well set out. Great areas to enjoy. Good food ...      1   en-AU  Google   \n",
       "\n",
       "        task  \n",
       "0  Sentiment  \n",
       "1  Sentiment  \n",
       "2  Sentiment  \n",
       "3  Sentiment  \n",
       "4  Sentiment  "
      ],
      "text/html": [
       "\n",
       "  <div id=\"df-c93afb2b-3163-4710-ae11-5ce53d863b68\" class=\"colab-df-container\">\n",
       "    <div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>label</th>\n",
       "      <th>variety</th>\n",
       "      <th>source</th>\n",
       "      <th>task</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>This was one of the best dishes I've EVER had!...</td>\n",
       "      <td>1</td>\n",
       "      <td>en-AU</td>\n",
       "      <td>Google</td>\n",
       "      <td>Sentiment</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>This Mexican restaurant in Penrith is a great ...</td>\n",
       "      <td>1</td>\n",
       "      <td>en-AU</td>\n",
       "      <td>Google</td>\n",
       "      <td>Sentiment</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>This was not to bad, I ordered the big pork ri...</td>\n",
       "      <td>1</td>\n",
       "      <td>en-AU</td>\n",
       "      <td>Google</td>\n",
       "      <td>Sentiment</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Clean cool and a nice smaller casino to check ...</td>\n",
       "      <td>1</td>\n",
       "      <td>en-AU</td>\n",
       "      <td>Google</td>\n",
       "      <td>Sentiment</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Well set out. Great areas to enjoy. Good food ...</td>\n",
       "      <td>1</td>\n",
       "      <td>en-AU</td>\n",
       "      <td>Google</td>\n",
       "      <td>Sentiment</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>\n",
       "    <div class=\"colab-df-buttons\">\n",
       "\n",
       "  <div class=\"colab-df-container\">\n",
       "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-c93afb2b-3163-4710-ae11-5ce53d863b68')\"\n",
       "            title=\"Convert this dataframe to an interactive table.\"\n",
       "            style=\"display:none;\">\n",
       "\n",
       "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
       "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
       "  </svg>\n",
       "    </button>\n",
       "\n",
       "  <style>\n",
       "    .colab-df-container {\n",
       "      display:flex;\n",
       "      gap: 12px;\n",
       "    }\n",
       "\n",
       "    .colab-df-convert {\n",
       "      background-color: #E8F0FE;\n",
       "      border: none;\n",
       "      border-radius: 50%;\n",
       "      cursor: pointer;\n",
       "      display: none;\n",
       "      fill: #1967D2;\n",
       "      height: 32px;\n",
       "      padding: 0 0 0 0;\n",
       "      width: 32px;\n",
       "    }\n",
       "\n",
       "    .colab-df-convert:hover {\n",
       "      background-color: #E2EBFA;\n",
       "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
       "      fill: #174EA6;\n",
       "    }\n",
       "\n",
       "    .colab-df-buttons div {\n",
       "      margin-bottom: 4px;\n",
       "    }\n",
       "\n",
       "    [theme=dark] .colab-df-convert {\n",
       "      background-color: #3B4455;\n",
       "      fill: #D2E3FC;\n",
       "    }\n",
       "\n",
       "    [theme=dark] .colab-df-convert:hover {\n",
       "      background-color: #434B5C;\n",
       "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
       "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
       "      fill: #FFFFFF;\n",
       "    }\n",
       "  </style>\n",
       "\n",
       "    <script>\n",
       "      const buttonEl =\n",
       "        document.querySelector('#df-c93afb2b-3163-4710-ae11-5ce53d863b68 button.colab-df-convert');\n",
       "      buttonEl.style.display =\n",
       "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
       "\n",
       "      async function convertToInteractive(key) {\n",
       "        const element = document.querySelector('#df-c93afb2b-3163-4710-ae11-5ce53d863b68');\n",
       "        const dataTable =\n",
       "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
       "                                                    [key], {});\n",
       "        if (!dataTable) return;\n",
       "\n",
       "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
       "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
       "          + ' to learn more about interactive tables.';\n",
       "        element.innerHTML = '';\n",
       "        dataTable['output_type'] = 'display_data';\n",
       "        await google.colab.output.renderOutput(dataTable, element);\n",
       "        const docLink = document.createElement('div');\n",
       "        docLink.innerHTML = docLinkHtml;\n",
       "        element.appendChild(docLink);\n",
       "      }\n",
       "    </script>\n",
       "  </div>\n",
       "\n",
       "\n",
       "    <div id=\"df-6b50b2b3-9587-4d19-8187-1e2eda39320d\">\n",
       "      <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-6b50b2b3-9587-4d19-8187-1e2eda39320d')\"\n",
       "                title=\"Suggest charts\"\n",
       "                style=\"display:none;\">\n",
       "\n",
       "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
       "     width=\"24px\">\n",
       "    <g>\n",
       "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
       "    </g>\n",
       "</svg>\n",
       "      </button>\n",
       "\n",
       "<style>\n",
       "  .colab-df-quickchart {\n",
       "      --bg-color: #E8F0FE;\n",
       "      --fill-color: #1967D2;\n",
       "      --hover-bg-color: #E2EBFA;\n",
       "      --hover-fill-color: #174EA6;\n",
       "      --disabled-fill-color: #AAA;\n",
       "      --disabled-bg-color: #DDD;\n",
       "  }\n",
       "\n",
       "  [theme=dark] .colab-df-quickchart {\n",
       "      --bg-color: #3B4455;\n",
       "      --fill-color: #D2E3FC;\n",
       "      --hover-bg-color: #434B5C;\n",
       "      --hover-fill-color: #FFFFFF;\n",
       "      --disabled-bg-color: #3B4455;\n",
       "      --disabled-fill-color: #666;\n",
       "  }\n",
       "\n",
       "  .colab-df-quickchart {\n",
       "    background-color: var(--bg-color);\n",
       "    border: none;\n",
       "    border-radius: 50%;\n",
       "    cursor: pointer;\n",
       "    display: none;\n",
       "    fill: var(--fill-color);\n",
       "    height: 32px;\n",
       "    padding: 0;\n",
       "    width: 32px;\n",
       "  }\n",
       "\n",
       "  .colab-df-quickchart:hover {\n",
       "    background-color: var(--hover-bg-color);\n",
       "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
       "    fill: var(--button-hover-fill-color);\n",
       "  }\n",
       "\n",
       "  .colab-df-quickchart-complete:disabled,\n",
       "  .colab-df-quickchart-complete:disabled:hover {\n",
       "    background-color: var(--disabled-bg-color);\n",
       "    fill: var(--disabled-fill-color);\n",
       "    box-shadow: none;\n",
       "  }\n",
       "\n",
       "  .colab-df-spinner {\n",
       "    border: 2px solid var(--fill-color);\n",
       "    border-color: transparent;\n",
       "    border-bottom-color: var(--fill-color);\n",
       "    animation:\n",
       "      spin 1s steps(1) infinite;\n",
       "  }\n",
       "\n",
       "  @keyframes spin {\n",
       "    0% {\n",
       "      border-color: transparent;\n",
       "      border-bottom-color: var(--fill-color);\n",
       "      border-left-color: var(--fill-color);\n",
       "    }\n",
       "    20% {\n",
       "      border-color: transparent;\n",
       "      border-left-color: var(--fill-color);\n",
       "      border-top-color: var(--fill-color);\n",
       "    }\n",
       "    30% {\n",
       "      border-color: transparent;\n",
       "      border-left-color: var(--fill-color);\n",
       "      border-top-color: var(--fill-color);\n",
       "      border-right-color: var(--fill-color);\n",
       "    }\n",
       "    40% {\n",
       "      border-color: transparent;\n",
       "      border-right-color: var(--fill-color);\n",
       "      border-top-color: var(--fill-color);\n",
       "    }\n",
       "    60% {\n",
       "      border-color: transparent;\n",
       "      border-right-color: var(--fill-color);\n",
       "    }\n",
       "    80% {\n",
       "      border-color: transparent;\n",
       "      border-right-color: var(--fill-color);\n",
       "      border-bottom-color: var(--fill-color);\n",
       "    }\n",
       "    90% {\n",
       "      border-color: transparent;\n",
       "      border-bottom-color: var(--fill-color);\n",
       "    }\n",
       "  }\n",
       "</style>\n",
       "\n",
       "      <script>\n",
       "        async function quickchart(key) {\n",
       "          const quickchartButtonEl =\n",
       "            document.querySelector('#' + key + ' button');\n",
       "          quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
       "          quickchartButtonEl.classList.add('colab-df-spinner');\n",
       "          try {\n",
       "            const charts = await google.colab.kernel.invokeFunction(\n",
       "                'suggestCharts', [key], {});\n",
       "          } catch (error) {\n",
       "            console.error('Error during call to suggestCharts:', error);\n",
       "          }\n",
       "          quickchartButtonEl.classList.remove('colab-df-spinner');\n",
       "          quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
       "        }\n",
       "        (() => {\n",
       "          let quickchartButtonEl =\n",
       "            document.querySelector('#df-6b50b2b3-9587-4d19-8187-1e2eda39320d button');\n",
       "          quickchartButtonEl.style.display =\n",
       "            google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
       "        })();\n",
       "      </script>\n",
       "    </div>\n",
       "\n",
       "    </div>\n",
       "  </div>\n"
      ],
      "application/vnd.google.colaboratory.intrinsic+json": {
       "type": "dataframe",
       "variable_name": "train",
       "summary": "{\n  \"name\": \"train\",\n  \"rows\": 17760,\n  \"fields\": [\n    {\n      \"column\": \"text\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 11515,\n        \"samples\": [\n          \"Center city old hotel resent new stylish good foodie taste\",\n          \"How does the government know they are empty?\\nDo they have a heap of public servants cost tens of millions of dollars who walk up every street and knock on every door to make sure someone is home?\",\n          \"We drove especially to visit a Cold Rock ice creamy and were not disappointed. The lady serving was very efficient and also friendly. The business has a great selection and a very large selection of add on. The chocolate add on were all quality and this added to the whole experience. Due to stupid restrictions we could not taste test We also could not sit inside but we're free to sit in the mall mixing with far more people.\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"label\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0,\n        \"min\": 0,\n        \"max\": 1,\n        \"num_unique_values\": 2,\n        \"samples\": [\n          0,\n          1\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"variety\",\n      \"properties\": {\n        \"dtype\": \"category\",\n        \"num_unique_values\": 3,\n        \"samples\": [\n          \"en-AU\",\n          \"en-IN\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"source\",\n      \"properties\": {\n        \"dtype\": \"category\",\n        \"num_unique_values\": 2,\n        \"samples\": [\n          \"Reddit\",\n          \"Google\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"task\",\n      \"properties\": {\n        \"dtype\": \"category\",\n        \"num_unique_values\": 2,\n        \"samples\": [\n          \"Sarcasm\",\n          \"Sentiment\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    }\n  ]\n}"
      }
     },
     "metadata": {},
     "execution_count": 2
    }
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "```\n",
    "Dataset shapes (train, validation):\n",
    "(17760, 6) (2428, 5)\n",
    "```\n",
    "\n",
    "Columns in dataset:\n",
    "Index(['text', 'label', 'variety', 'source', 'task', 'text_len'], dtype='object')"
   ],
   "metadata": {
    "id": "Jg9tYP0dT3Rr"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Sentiment vs Sarcasm Task splitting"
   ],
   "metadata": {
    "id": "Kn-ZSGWWlXZF"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "print(\"\\nLabel distribution by tasks:\")\n",
    "display(pd.crosstab(train[\"label\"], train[\"task\"]))"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 178
    },
    "id": "R53SNTFcUI55",
    "executionInfo": {
     "status": "ok",
     "timestamp": 1766008616323,
     "user_tz": -60,
     "elapsed": 90,
     "user": {
      "displayName": "Balzhan Daurenbekovna",
      "userId": "10210039747879563426"
     }
    },
    "outputId": "5abb0198-b1b5-4f7a-a4da-f98f1bf58c16"
   },
   "execution_count": null,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "\n",
      "Label distribution by tasks:\n"
     ]
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "task   Sarcasm  Sentiment\n",
       "label                    \n",
       "0         7619       4473\n",
       "1         1275       4393"
      ],
      "text/html": [
       "\n",
       "  <div id=\"df-5202fd17-60ae-447b-93dd-e53c4cf6e371\" class=\"colab-df-container\">\n",
       "    <div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th>task</th>\n",
       "      <th>Sarcasm</th>\n",
       "      <th>Sentiment</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>label</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>7619</td>\n",
       "      <td>4473</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1275</td>\n",
       "      <td>4393</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>\n",
       "    <div class=\"colab-df-buttons\">\n",
       "\n",
       "  <div class=\"colab-df-container\">\n",
       "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-5202fd17-60ae-447b-93dd-e53c4cf6e371')\"\n",
       "            title=\"Convert this dataframe to an interactive table.\"\n",
       "            style=\"display:none;\">\n",
       "\n",
       "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
       "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
       "  </svg>\n",
       "    </button>\n",
       "\n",
       "  <style>\n",
       "    .colab-df-container {\n",
       "      display:flex;\n",
       "      gap: 12px;\n",
       "    }\n",
       "\n",
       "    .colab-df-convert {\n",
       "      background-color: #E8F0FE;\n",
       "      border: none;\n",
       "      border-radius: 50%;\n",
       "      cursor: pointer;\n",
       "      display: none;\n",
       "      fill: #1967D2;\n",
       "      height: 32px;\n",
       "      padding: 0 0 0 0;\n",
       "      width: 32px;\n",
       "    }\n",
       "\n",
       "    .colab-df-convert:hover {\n",
       "      background-color: #E2EBFA;\n",
       "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
       "      fill: #174EA6;\n",
       "    }\n",
       "\n",
       "    .colab-df-buttons div {\n",
       "      margin-bottom: 4px;\n",
       "    }\n",
       "\n",
       "    [theme=dark] .colab-df-convert {\n",
       "      background-color: #3B4455;\n",
       "      fill: #D2E3FC;\n",
       "    }\n",
       "\n",
       "    [theme=dark] .colab-df-convert:hover {\n",
       "      background-color: #434B5C;\n",
       "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
       "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
       "      fill: #FFFFFF;\n",
       "    }\n",
       "  </style>\n",
       "\n",
       "    <script>\n",
       "      const buttonEl =\n",
       "        document.querySelector('#df-5202fd17-60ae-447b-93dd-e53c4cf6e371 button.colab-df-convert');\n",
       "      buttonEl.style.display =\n",
       "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
       "\n",
       "      async function convertToInteractive(key) {\n",
       "        const element = document.querySelector('#df-5202fd17-60ae-447b-93dd-e53c4cf6e371');\n",
       "        const dataTable =\n",
       "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
       "                                                    [key], {});\n",
       "        if (!dataTable) return;\n",
       "\n",
       "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
       "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
       "          + ' to learn more about interactive tables.';\n",
       "        element.innerHTML = '';\n",
       "        dataTable['output_type'] = 'display_data';\n",
       "        await google.colab.output.renderOutput(dataTable, element);\n",
       "        const docLink = document.createElement('div');\n",
       "        docLink.innerHTML = docLinkHtml;\n",
       "        element.appendChild(docLink);\n",
       "      }\n",
       "    </script>\n",
       "  </div>\n",
       "\n",
       "\n",
       "    <div id=\"df-56e75c34-ba5f-47c2-9282-ac1c617a7af1\">\n",
       "      <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-56e75c34-ba5f-47c2-9282-ac1c617a7af1')\"\n",
       "                title=\"Suggest charts\"\n",
       "                style=\"display:none;\">\n",
       "\n",
       "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
       "     width=\"24px\">\n",
       "    <g>\n",
       "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
       "    </g>\n",
       "</svg>\n",
       "      </button>\n",
       "\n",
       "<style>\n",
       "  .colab-df-quickchart {\n",
       "      --bg-color: #E8F0FE;\n",
       "      --fill-color: #1967D2;\n",
       "      --hover-bg-color: #E2EBFA;\n",
       "      --hover-fill-color: #174EA6;\n",
       "      --disabled-fill-color: #AAA;\n",
       "      --disabled-bg-color: #DDD;\n",
       "  }\n",
       "\n",
       "  [theme=dark] .colab-df-quickchart {\n",
       "      --bg-color: #3B4455;\n",
       "      --fill-color: #D2E3FC;\n",
       "      --hover-bg-color: #434B5C;\n",
       "      --hover-fill-color: #FFFFFF;\n",
       "      --disabled-bg-color: #3B4455;\n",
       "      --disabled-fill-color: #666;\n",
       "  }\n",
       "\n",
       "  .colab-df-quickchart {\n",
       "    background-color: var(--bg-color);\n",
       "    border: none;\n",
       "    border-radius: 50%;\n",
       "    cursor: pointer;\n",
       "    display: none;\n",
       "    fill: var(--fill-color);\n",
       "    height: 32px;\n",
       "    padding: 0;\n",
       "    width: 32px;\n",
       "  }\n",
       "\n",
       "  .colab-df-quickchart:hover {\n",
       "    background-color: var(--hover-bg-color);\n",
       "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
       "    fill: var(--button-hover-fill-color);\n",
       "  }\n",
       "\n",
       "  .colab-df-quickchart-complete:disabled,\n",
       "  .colab-df-quickchart-complete:disabled:hover {\n",
       "    background-color: var(--disabled-bg-color);\n",
       "    fill: var(--disabled-fill-color);\n",
       "    box-shadow: none;\n",
       "  }\n",
       "\n",
       "  .colab-df-spinner {\n",
       "    border: 2px solid var(--fill-color);\n",
       "    border-color: transparent;\n",
       "    border-bottom-color: var(--fill-color);\n",
       "    animation:\n",
       "      spin 1s steps(1) infinite;\n",
       "  }\n",
       "\n",
       "  @keyframes spin {\n",
       "    0% {\n",
       "      border-color: transparent;\n",
       "      border-bottom-color: var(--fill-color);\n",
       "      border-left-color: var(--fill-color);\n",
       "    }\n",
       "    20% {\n",
       "      border-color: transparent;\n",
       "      border-left-color: var(--fill-color);\n",
       "      border-top-color: var(--fill-color);\n",
       "    }\n",
       "    30% {\n",
       "      border-color: transparent;\n",
       "      border-left-color: var(--fill-color);\n",
       "      border-top-color: var(--fill-color);\n",
       "      border-right-color: var(--fill-color);\n",
       "    }\n",
       "    40% {\n",
       "      border-color: transparent;\n",
       "      border-right-color: var(--fill-color);\n",
       "      border-top-color: var(--fill-color);\n",
       "    }\n",
       "    60% {\n",
       "      border-color: transparent;\n",
       "      border-right-color: var(--fill-color);\n",
       "    }\n",
       "    80% {\n",
       "      border-color: transparent;\n",
       "      border-right-color: var(--fill-color);\n",
       "      border-bottom-color: var(--fill-color);\n",
       "    }\n",
       "    90% {\n",
       "      border-color: transparent;\n",
       "      border-bottom-color: var(--fill-color);\n",
       "    }\n",
       "  }\n",
       "</style>\n",
       "\n",
       "      <script>\n",
       "        async function quickchart(key) {\n",
       "          const quickchartButtonEl =\n",
       "            document.querySelector('#' + key + ' button');\n",
       "          quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
       "          quickchartButtonEl.classList.add('colab-df-spinner');\n",
       "          try {\n",
       "            const charts = await google.colab.kernel.invokeFunction(\n",
       "                'suggestCharts', [key], {});\n",
       "          } catch (error) {\n",
       "            console.error('Error during call to suggestCharts:', error);\n",
       "          }\n",
       "          quickchartButtonEl.classList.remove('colab-df-spinner');\n",
       "          quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
       "        }\n",
       "        (() => {\n",
       "          let quickchartButtonEl =\n",
       "            document.querySelector('#df-56e75c34-ba5f-47c2-9282-ac1c617a7af1 button');\n",
       "          quickchartButtonEl.style.display =\n",
       "            google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
       "        })();\n",
       "      </script>\n",
       "    </div>\n",
       "\n",
       "    </div>\n",
       "  </div>\n"
      ],
      "application/vnd.google.colaboratory.intrinsic+json": {
       "type": "dataframe",
       "summary": "{\n  \"name\": \"display(pd\",\n  \"rows\": 2,\n  \"fields\": [\n    {\n      \"column\": \"label\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0,\n        \"min\": 0,\n        \"max\": 1,\n        \"num_unique_values\": 2,\n        \"samples\": [\n          1,\n          0\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"Sarcasm\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 4485,\n        \"min\": 1275,\n        \"max\": 7619,\n        \"num_unique_values\": 2,\n        \"samples\": [\n          1275,\n          7619\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"Sentiment\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 56,\n        \"min\": 4393,\n        \"max\": 4473,\n        \"num_unique_values\": 2,\n        \"samples\": [\n          4393,\n          4473\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    }\n  ]\n}"
      }
     },
     "metadata": {}
    }
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Variety"
   ],
   "metadata": {
    "id": "sMchHKFLwONs"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "\n",
    "all_varieties = sorted(train['variety'].unique())\n",
    "\n",
    "# {'en-AU': 0, 'en-GB': 1, 'en-US': 2, ...}\n",
    "variety_map = {v: i for i, v in enumerate(all_varieties)}\n",
    "\n",
    "print(\"Variety Mapping (Save):\")\n",
    "print(variety_map)\n"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "CoJksjK_woHE",
    "executionInfo": {
     "status": "ok",
     "timestamp": 1766083173324,
     "user_tz": -60,
     "elapsed": 39,
     "user": {
      "displayName": "Balzhan Daurenbekovna",
      "userId": "10210039747879563426"
     }
    },
    "outputId": "a9d6fe68-03c1-4992-fb23-f2b0b84b663a"
   },
   "execution_count": null,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Variety Mapping (Save):\n",
      "{'en-AU': 0, 'en-IN': 1, 'en-UK': 2}\n"
     ]
    }
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "# For Sentiment\n",
    "sent_train = train[train['task'] == 'Sentiment'].copy()\n",
    "sent_val = val[val['task'] == 'Sentiment'].copy()\n",
    "\n",
    "# For Sarcasm\n",
    "sarc_train = train[train['task'] == 'Sarcasm'].copy()\n",
    "sarc_val = val[val['task'] == 'Sarcasm'].copy()"
   ],
   "metadata": {
    "id": "DIyMEON2lV2N"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Cleaning"
   ],
   "metadata": {
    "id": "dHNWMhWGtV7F"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "* HTML & URL Removal: Web-scraped data (Google/Reddit) often contains artifacts\n",
    "\n",
    "*   Specific Reddit user mentions removal\n",
    "\n",
    "*  Hashtag Handling: Removed the # symbol but kept the word ( #politics $\\to$ politics). The content is valuable, but the symbol is noise\n",
    "\n",
    "*   Skipped the standard text.lower() step. In Sarcasm and Sentiment, capitalization is a massive feature. Bert model can read uppercases.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ],
   "metadata": {
    "id": "9VqXLp2V1Q0v"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "def clean_text(text):\n",
    "    if pd.isna(text) or text == \"\":\n",
    "        return \"\"\n",
    "\n",
    "    text = str(text)\n",
    "\n",
    "    # A. Remove User Mentions (Specific to Reddit)\n",
    "    text = re.sub(r\"(u/|@)\\w+\", \"[USER]\", text)\n",
    "\n",
    "    # B. Remove HTML entities (Corrected regex)\n",
    "    text = re.sub(r\"&\\w+;\", \" \", text)\n",
    "\n",
    "    # C. Remove URLs\n",
    "    text = re.sub(r\"https?://\\S+|www\\.\\S+\", \"[URL]\", text)\n",
    "\n",
    "    # D. Handle Hashtags (Remove #, keep word)\n",
    "    text = re.sub(r\"#(\\w+)\", r\"\\1\", text)\n",
    "\n",
    "    # E. Normalize whitespace\n",
    "    text = re.sub(r\"\\s+\", \" \", text).strip()\n",
    "\n",
    "    return text\n"
   ],
   "metadata": {
    "id": "bbVYVjqmz_RG"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "def apply_cleaning(df, name=\"Dataset\"):\n",
    "\n",
    "    df = df.copy()\n",
    "\n",
    "    before = len(df)\n",
    "    df['text_clean'] = df['text'].apply(clean_text)\n",
    "    df_clean = df[df['text_clean'].notna()]\n",
    "    after = len(df_clean)\n",
    "    removed = before - after\n",
    "\n",
    "    print(f\"[{name}]\")\n",
    "    print(f\"  Before: {before:,} rows\")\n",
    "    print(f\"  After:  {after:,} rows\")\n",
    "    print(f\"  Removed: {removed} empty rows\")\n",
    "\n",
    "    return df_clean\n",
    "\n",
    "\n",
    "print(\"\\n--- SENTIMENT ---\")\n",
    "sent_train = apply_cleaning(sent_train, \"Sentiment Train\")\n",
    "sent_val = apply_cleaning(sent_val, \"Sentiment Val\")\n",
    "\n",
    "print(\"\\n--- SARCASM ---\")\n",
    "sarc_train = apply_cleaning(sarc_train, \"Sarcasm Train\")\n",
    "sarc_val = apply_cleaning(sarc_val, \"Sarcasm Val\")\n"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "qrT1-H_mz_Oo",
    "executionInfo": {
     "status": "ok",
     "timestamp": 1766083183615,
     "user_tz": -60,
     "elapsed": 645,
     "user": {
      "displayName": "Balzhan Daurenbekovna",
      "userId": "10210039747879563426"
     }
    },
    "outputId": "3c0e1b4c-90c4-47c8-efa0-73acc75a2f32"
   },
   "execution_count": null,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "\n",
      "--- SENTIMENT ---\n",
      "[Sentiment Train]\n",
      "  Before: 8,866 rows\n",
      "  After:  8,866 rows\n",
      "  Removed: 0 empty rows\n",
      "[Sentiment Val]\n",
      "  Before: 1,212 rows\n",
      "  After:  1,212 rows\n",
      "  Removed: 0 empty rows\n",
      "\n",
      "--- SARCASM ---\n",
      "[Sarcasm Train]\n",
      "  Before: 8,894 rows\n",
      "  After:  8,894 rows\n",
      "  Removed: 0 empty rows\n",
      "[Sarcasm Val]\n",
      "  Before: 1,216 rows\n",
      "  After:  1,216 rows\n",
      "  Removed: 0 empty rows\n"
     ]
    }
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Train / Val / Test Splitting"
   ],
   "metadata": {
    "id": "hX9Yy4MUFsUF"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "The paper's test set is ~2,500 rows (Table 5). The Hugging Face validation set might actually be the Test set. We use train to create a new validation set and use the provided validation purely as test to match the paper's volume."
   ],
   "metadata": {
    "id": "ka6dOW-LS41F"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "# Train set splits into train/val\n",
    "\n",
    "def internal_split(df):\n",
    "    train_split, val_split = train_test_split(\n",
    "        df,\n",
    "        test_size=0.10,  # 10% for validation\n",
    "        random_state=42,\n",
    "        stratify=df['label']\n",
    "    )\n",
    "    return train_split, val_split\n",
    "\n",
    "# apply\n",
    "train_sentiment, val_sentiment = internal_split(sent_train)\n",
    "train_sarcasm, val_sarcasm = internal_split(sarc_train)\n",
    "\n",
    "test_sentiment = sent_val\n",
    "test_sarcasm   = sarc_val"
   ],
   "metadata": {
    "id": "HARmIkbbmAnG"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "\n",
    "print(\"--- SENTIMENT ---\")\n",
    "print(f\"Train: {len(train_sentiment)} \")\n",
    "print(f\"Val:   {len(val_sentiment)}   \")\n",
    "print(f\"Test:  {len(test_sentiment)}  \")\n",
    "\n",
    "print(\"\\n--- SARCASM ---\")\n",
    "print(f\"Train: {len(train_sarcasm)}\")\n",
    "print(f\"Val:   {len(val_sarcasm)}\")\n",
    "print(f\"Test:  {len(test_sarcasm)}\")"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "abvWpT2WmbjI",
    "executionInfo": {
     "status": "ok",
     "timestamp": 1766083209166,
     "user_tz": -60,
     "elapsed": 15,
     "user": {
      "displayName": "Balzhan Daurenbekovna",
      "userId": "10210039747879563426"
     }
    },
    "outputId": "7b97141b-894d-49b8-df53-36c25ec31bb2"
   },
   "execution_count": null,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "--- SENTIMENT ---\n",
      "Train: 7979 \n",
      "Val:   887   \n",
      "Test:  1212  \n",
      "\n",
      "--- SARCASM ---\n",
      "Train: 8004\n",
      "Val:   890\n",
      "Test:  1216\n"
     ]
    }
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Tokenization / Padding"
   ],
   "metadata": {
    "id": "BhxPAcA1pz09"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Convert human text into numerical Input IDs."
   ],
   "metadata": {
    "id": "3SeEN6saAls6"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "\n",
    "Padding: Sequences shorter than 512\n",
    "tokens were padded with [PAD] (zeros) to ensure uniform matrix dimensions for batch processing.\n",
    "\n",
    "\n"
   ],
   "metadata": {
    "id": "aultTnsT4bHl"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Bert Encoder"
   ],
   "metadata": {
    "id": "A4dmRUo6O50h"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "ENCODER_MODEL_NAME = \"bert-base-cased\"\n",
    "OUTPUT_DIR = \"/content/drive/MyDrive/DNLP/data/processed_data_final\"\n",
    "\n",
    "if not os.path.exists(OUTPUT_DIR):\n",
    "    os.makedirs(OUTPUT_DIR)\n",
    "\n",
    "# ENCODER (BERT, RoBERTa)\n",
    "\n",
    "def prepare_encoder_data(train_df, val_df, test_df, model_name,variety_map):\n",
    "    print(f\"Tokenizing for Encoder model: {model_name}...\")\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "\n",
    "    datasets = {'train': train_df, 'val': val_df, 'test': test_df}\n",
    "    processed_data = {}\n",
    "\n",
    "    for split_name, df in datasets.items():\n",
    "        # \u0422okenization\n",
    "\n",
    "        encodings = tokenizer(\n",
    "            df['text_clean'].tolist(),\n",
    "            truncation=True,\n",
    "            padding='max_length',\n",
    "            max_length=256,\n",
    "            return_tensors='pt'\n",
    "        )\n",
    "\n",
    "        # 2.Variety\n",
    "        # ['en-US', 'en-AU'] -> [2, 0]\n",
    "\n",
    "        variety_ids = df['variety'].map(variety_map).fillna(-1).astype(int).values\n",
    "\n",
    "        processed_data[split_name] = {\n",
    "            'input_ids': encodings['input_ids'],\n",
    "            'attention_mask': encodings['attention_mask'],\n",
    "            'labels': torch.tensor(df['label'].values, dtype=torch.long),\n",
    "            'variety': torch.tensor(variety_ids, dtype=torch.long)\n",
    "        }\n",
    "\n",
    "    return processed_data\n",
    "\n",
    "\n",
    "# Apply separetly for tasks\n",
    "print(\"Processing Encoders\")\n",
    "bert_sentiment_data = prepare_encoder_data(train_sentiment, val_sentiment, test_sentiment, ENCODER_MODEL_NAME,variety_map)\n",
    "\n",
    "bert_sarcasm_data   = prepare_encoder_data(train_sarcasm, val_sarcasm, test_sarcasm, ENCODER_MODEL_NAME,variety_map)"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "PhQnkRVBO4av",
    "executionInfo": {
     "status": "ok",
     "timestamp": 1766083624291,
     "user_tz": -60,
     "elapsed": 14266,
     "user": {
      "displayName": "Balzhan Daurenbekovna",
      "userId": "10210039747879563426"
     }
    },
    "outputId": "862bb47c-2c64-42c4-f5b3-f1035273a6a6"
   },
   "execution_count": null,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Processing Encoders\n",
      "Tokenizing for Encoder model: bert-base-cased...\n",
      "Tokenizing for Encoder model: bert-base-cased...\n"
     ]
    }
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "def inspect_encoder_data(data_dict, task_name, model_name):\n",
    "\n",
    "    print(f\"ENCODER DATA: {task_name} using {model_name}\")\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "\n",
    "    for split in ['train', 'val', 'test']:\n",
    "        split_data = data_dict[split]\n",
    "\n",
    "        # 1. get tensors\n",
    "        input_ids = split_data['input_ids']\n",
    "        attention_mask = split_data['attention_mask']\n",
    "        labels = split_data['labels']\n",
    "\n",
    "        print(f\"\\n Split: {split.upper()} ---\")\n",
    "\n",
    "        # 2. check shapes\n",
    "        print(f\"Input IDs shape:      {input_ids.shape}\")\n",
    "        print(f\"Attention Mask shape: {attention_mask.shape}\")\n",
    "        print(f\"Labels shape:         {labels.shape}\")\n",
    "\n",
    "        if len(input_ids) != len(labels):\n",
    "            print(\"Mismatch between inputs and labels count!\")\n",
    "        else:\n",
    "            print(\"Counts match.\")\n",
    "\n",
    "        # 3. Random sample decoder\n",
    "\n",
    "        idx = random.randint(0, len(input_ids) - 1)\n",
    "\n",
    "        print(f\"\\n[Sample Data at index {idx}]\")\n",
    "\n",
    "\n",
    "        # As a result example print first 10 and last 5 ids\n",
    "        ids = input_ids[idx]\n",
    "        print(f\"Tensor IDs (shortened): {ids[:10].tolist()} ... {ids[-5:].tolist()}\")\n",
    "\n",
    "        # \u0411) Decode to text\n",
    "        decoded_text = tokenizer.decode(ids)\n",
    "        print(f\"Decoded Text: \\\"{decoded_text}\\\"\")\n",
    "\n",
    "        print(f\"Label: {labels[idx].item()}\")\n",
    "\n",
    "\n",
    "\n",
    "# Sentiment (BERT)\n",
    "inspect_encoder_data(bert_sentiment_data, \"SENTIMENT\", \"bert-base-cased\")\n",
    "\n",
    "# Sarcasm (BERT)\n",
    "inspect_encoder_data(bert_sarcasm_data, \"SARCASM\", \"bert-base-cased\")"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Ynq0r_iDcJuu",
    "executionInfo": {
     "status": "ok",
     "timestamp": 1766083399739,
     "user_tz": -60,
     "elapsed": 2641,
     "user": {
      "displayName": "Balzhan Daurenbekovna",
      "userId": "10210039747879563426"
     }
    },
    "outputId": "543ec38f-ef6c-4e62-f830-2259783e2b9b"
   },
   "execution_count": null,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "ENCODER DATA: SENTIMENT using bert-base-cased\n",
      "\n",
      " Split: TRAIN ---\n",
      "Input IDs shape:      torch.Size([7979, 256])\n",
      "Attention Mask shape: torch.Size([7979, 256])\n",
      "Labels shape:         torch.Size([7979])\n",
      "Counts match.\n",
      "\n",
      "[Sample Data at index 5769]\n",
      "Tensor IDs (shortened): [101, 13832, 18734, 1174, 170, 1374, 8898, 1107, 1303, 1114] ... [0, 0, 0, 0, 0]\n",
      "Decoded Text: \"[CLS] Enjoyed a few drinks in here with friends. Quite lively on a Saturday afternoon with sport on the screens creating a vibrant atmosphere. Ales were of a good quality [SEP] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD]\"\n",
      "Label: 1\n",
      "\n",
      " Split: VAL ---\n",
      "Input IDs shape:      torch.Size([887, 256])\n",
      "Attention Mask shape: torch.Size([887, 256])\n",
      "Labels shape:         torch.Size([887])\n",
      "Counts match.\n",
      "\n",
      "[Sample Data at index 191]\n",
      "Tensor IDs (shortened): [101, 2096, 1736, 117, 1125, 1195, 1678, 5776, 112, 188] ... [0, 0, 0, 0, 0]\n",
      "Decoded Text: \"[CLS] Of course, had we taken Finland ' s approach, it may have turned out like it did in Georgia USA where their new reactor cost US $ 35 billion to build ( AU $ 52 billion ), was 7 years late, and is going to cost consumers an extra US $ 170 + per year on their electricity bills ( AU $ 250 + ). [SEP] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD]\"\n",
      "Label: 0\n",
      "\n",
      " Split: TEST ---\n",
      "Input IDs shape:      torch.Size([1212, 256])\n",
      "Attention Mask shape: torch.Size([1212, 256])\n",
      "Labels shape:         torch.Size([1212])\n",
      "Counts match.\n",
      "\n",
      "[Sample Data at index 1120]\n",
      "Tensor IDs (shortened): [101, 107, 3956, 3713, 1177, 1195, 1274, 112, 189, 1660] ... [0, 0, 0, 0, 0]\n",
      "Decoded Text: \"[CLS] \" Okay guys so we don ' t give a shit about the poor people enough to actually sort the country out so any ideas? \" \" WE BLIND THEM WITH FREE FOOTBALL \" This country is so ridiculous. [SEP] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD]\"\n",
      "Label: 0\n",
      "ENCODER DATA: SARCASM using bert-base-cased\n",
      "\n",
      " Split: TRAIN ---\n",
      "Input IDs shape:      torch.Size([8004, 256])\n",
      "Attention Mask shape: torch.Size([8004, 256])\n",
      "Labels shape:         torch.Size([8004])\n",
      "Counts match.\n",
      "\n",
      "[Sample Data at index 3405]\n",
      "Tensor IDs (shortened): [101, 12786, 10806, 3954, 1128, 112, 1325, 1243, 5768, 16628] ... [0, 0, 0, 0, 0]\n",
      "Decoded Text: \"[CLS] Dumbass you ' ll get loose motions if you eat that many mangoes. [SEP] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD]\"\n",
      "Label: 0\n",
      "\n",
      " Split: VAL ---\n",
      "Input IDs shape:      torch.Size([890, 256])\n",
      "Attention Mask shape: torch.Size([890, 256])\n",
      "Labels shape:         torch.Size([890])\n",
      "Counts match.\n",
      "\n",
      "[Sample Data at index 357]\n",
      "Tensor IDs (shortened): [101, 2066, 4107, 11078, 1813, 119, 151, 146, 1238, 112] ... [0, 0, 0, 0, 0]\n",
      "Decoded Text: \"[CLS] Just asking yaar. N I didn ' t know how else to point and ask. Do you have an answer? [SEP] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD]\"\n",
      "Label: 0\n",
      "\n",
      " Split: TEST ---\n",
      "Input IDs shape:      torch.Size([1216, 256])\n",
      "Attention Mask shape: torch.Size([1216, 256])\n",
      "Labels shape:         torch.Size([1216])\n",
      "Counts match.\n",
      "\n",
      "[Sample Data at index 986]\n",
      "Tensor IDs (shortened): [101, 1127, 170, 1374, 1614, 146, 1225, 183, 112, 189] ... [1115, 1131, 1674, 1123, 102]\n",
      "Decoded Text: \"[CLS] were a few things I did n ' t understand, and I think it ' s very unnecessary, first of all, the security guards who met you at the door take you through the passport check - in, as if they ' re taking you through the bag check - in, the top search check - in, the ticket check - out, and the water you have on you, they Decoy you to security in detail, so this security is too much of an exaggeration, because we ' re going to enter a roller coaster on the bottom Another issue that I ' m uncomfortable with is that sometimes on the roller rink we stepped aside to take a breath and relax, I was n ' t even allowed to take a photo with my friend, we were standing in a place where we would n ' t disturb anyone, the skating officials inside told us to get off the rink to relax, was it worth getting off the rink for a minute, it ' s really ridiculous The staff inside are very respectful and very in love with their work people standing at the door motivated us very much with the style of the cloakroom gentleman and his smiling face. Another employee inside, the Portuguese bartender lady, is also smiling with patience and I really like that she does her [SEP]\"\n",
      "Label: 0\n"
     ]
    }
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Variety cheking"
   ],
   "metadata": {
    "id": "-9n4v_ur33ay"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "#Checking variety\n",
    "\n",
    "print(\"\\nChecking Variety tensor...\")\n",
    "sample_var_id = bert_sentiment_data['train']['variety'][0].item()\n",
    "\n",
    "# (reverse lookup)\n",
    "\n",
    "id2variety = {i: v for v, i in variety_map.items()}\n",
    "print(f\"ID: {sample_var_id} -> Variety: {id2variety[sample_var_id]}\")"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "7lYvb1RSxkcr",
    "executionInfo": {
     "status": "ok",
     "timestamp": 1766083434004,
     "user_tz": -60,
     "elapsed": 22,
     "user": {
      "displayName": "Balzhan Daurenbekovna",
      "userId": "10210039747879563426"
     }
    },
    "outputId": "1c0f33b2-62eb-4c7e-bd0a-845e3b4c6ec5"
   },
   "execution_count": null,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "\n",
      "Checking Variety tensor...\n",
      "ID: 1 -> Variety: en-IN\n"
     ]
    }
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "import random\n",
    "\n",
    "def inspect_variety_metadata(data_dict, variety_map, task_name):\n",
    "\n",
    "    print(f\" VARIETY FIELD: {task_name}\")\n",
    "\n",
    "    id2variety = {i: name for name, i in variety_map.items()}\n",
    "\n",
    "    for split in ['train', 'val', 'test']:\n",
    "        split_data = data_dict[split]\n",
    "\n",
    "        # 1. Key exist or not\n",
    "        if 'variety' not in split_data:\n",
    "            print(f\"ERROR:'variety' key MISSING in {split}!\")\n",
    "            return\n",
    "\n",
    "        var_tensor = split_data['variety']\n",
    "        input_tensor = split_data['input_ids']\n",
    "\n",
    "        print(f\"\\n--- Split: {split.upper()} ---\")\n",
    "\n",
    "        # 2. Shape cheking\n",
    "\n",
    "        print(f\"Variety Shape: {var_tensor.shape}\")\n",
    "\n",
    "        if len(var_tensor) != len(input_tensor):\n",
    "            print(f\"ERROR: Shape mismatch! Text: {len(input_tensor)}, Variety: {len(var_tensor)}\")\n",
    "        else:\n",
    "            print(f\"Length matches input_ids.\")\n",
    "\n",
    "        # 3. Sample\n",
    "        idx = random.randint(0, len(var_tensor) - 1)\n",
    "        val_id = var_tensor[idx].item()\n",
    "\n",
    "        # If ID = -1, then warning\n",
    "        if val_id == -1:\n",
    "             print(f\"Found -1 (Unknown variety) at index {idx}\")\n",
    "        else:\n",
    "            country_name = id2variety.get(val_id, \"UNKNOWN\")\n",
    "            print(f\"Sample Index {idx}: ID {val_id} -> means '{country_name}'\")\n",
    "\n",
    "\n",
    "\n",
    "inspect_variety_metadata(bert_sentiment_data, variety_map, \"SENTIMENT\")\n",
    "inspect_variety_metadata(bert_sarcasm_data, variety_map, \"SARCASM\")"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "SUmRA3FpzSHH",
    "executionInfo": {
     "status": "ok",
     "timestamp": 1766084031290,
     "user_tz": -60,
     "elapsed": 20,
     "user": {
      "displayName": "Balzhan Daurenbekovna",
      "userId": "10210039747879563426"
     }
    },
    "outputId": "1f14f25e-4960-49ac-c78e-3932386389a6"
   },
   "execution_count": null,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      " VARIETY FIELD: SENTIMENT\n",
      "\n",
      "--- Split: TRAIN ---\n",
      "Variety Shape: torch.Size([7979])\n",
      "Length matches input_ids.\n",
      "Sample Index 7683: ID 0 -> means 'en-AU'\n",
      "\n",
      "--- Split: VAL ---\n",
      "Variety Shape: torch.Size([887])\n",
      "Length matches input_ids.\n",
      "Sample Index 309: ID 1 -> means 'en-IN'\n",
      "\n",
      "--- Split: TEST ---\n",
      "Variety Shape: torch.Size([1212])\n",
      "Length matches input_ids.\n",
      "Sample Index 806: ID 1 -> means 'en-IN'\n",
      " VARIETY FIELD: SARCASM\n",
      "\n",
      "--- Split: TRAIN ---\n",
      "Variety Shape: torch.Size([8004])\n",
      "Length matches input_ids.\n",
      "Sample Index 5748: ID 1 -> means 'en-IN'\n",
      "\n",
      "--- Split: VAL ---\n",
      "Variety Shape: torch.Size([890])\n",
      "Length matches input_ids.\n",
      "Sample Index 359: ID 2 -> means 'en-UK'\n",
      "\n",
      "--- Split: TEST ---\n",
      "Variety Shape: torch.Size([1216])\n",
      "Length matches input_ids.\n",
      "Sample Index 1207: ID 2 -> means 'en-UK'\n"
     ]
    }
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Robert Encoder"
   ],
   "metadata": {
    "id": "qVzUv36jm4J0"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "ROBERTA_MODEL = \"roberta-base\"\n",
    "\n",
    "print(f\"\\n Starting processing for {ROBERTA_MODEL}...\")\n",
    "\n",
    "roberta_sentiment_data = prepare_encoder_data(\n",
    "    train_sentiment, val_sentiment, test_sentiment, ROBERTA_MODEL,variety_map\n",
    ")\n",
    "\n",
    "roberta_sarcasm_data = prepare_encoder_data(\n",
    "    train_sarcasm, val_sarcasm, test_sarcasm, ROBERTA_MODEL,variety_map\n",
    ")\n"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 246,
     "referenced_widgets": [
      "418c928fd6c14fcfb455f46c8e26b4e6",
      "f12f0f9676a44a95b7776c9225108a5b",
      "bf6fb1a500bf4487b32bb7f89f8f4e97",
      "e20cc228eca3441194b730b138ee348e",
      "58fbdbe83faa40fb8e7032efee9fc54f",
      "5c23386734e9444fa5b1af9d9eb05e80",
      "f2081c172c6a4770af6aef3dd43dcf99",
      "4c7fff0aa8434748b571263c86533ca8",
      "cb2e3daacc0f499ea702c9f3067c9105",
      "c9131a0d6b3144c99896c182c1eb3d4e",
      "d1caec5234934818a55a89f20f7a5b41",
      "2e9d358ac46d4b0a907418aa799a76c6",
      "ef0510480fe649778cc7369c15c7e09a",
      "dd262e3d52a64422a8e91c00fae14cdd",
      "785dfd57a9cf4cf4be232f1ba93c203b",
      "f47d7a2fd06f4b3da10acf51b9e10a8b",
      "9d234946bcb045899ffe6c866d8527e9",
      "8c2f5ef34dc54d98a8fe1dc636b68489",
      "03985e354c9b47c48d60350f62332607",
      "264e534dc2f0450ebf51d178a8eb103b",
      "b15444a2517c43d8be5a2fb2bade474b",
      "8d420f88bfe546cda1a38f1e93771253",
      "c03c4f3ca45547e6806432e08d5fab8e",
      "82f14018e8124f738c4a1edc35c57ec5",
      "3c0f6da38c7945ffaaa1c4ab28122e16",
      "996486c18e134f25b1de92edd73fdc27",
      "859a73c07e204e3290df9c0cd1b29a89",
      "a7eb0fc13c5e497390a5c6fed7e7bb01",
      "db9b07958dc64978b361ffab3e155824",
      "092db7bc0066485b9f40cb7fa221e917",
      "f7dd1e007cd049adb0c58d0403522d0e",
      "25ffc4e75e474f379691fa12d1c8ad46",
      "83fadfa4df314746aaa31d3eaf40e568",
      "a492b4619a7d4d899821b85c0b4d3793",
      "0763479c43104bb3b6f58619d9e4650f",
      "0941bffd5ab74e0aa3db14e34c9c42b8",
      "7a0f2e6bec7e46d5a9aed225c487ab20",
      "424c8bc231664d259439f9aca3e1a155",
      "e5b1103a08524c8ca129bfa9bf679715",
      "37d6b238bbd44453ab98ac805c8c5408",
      "52876c67d5e948e890c33303ef684adb",
      "e36d68911a3a45c9a9374d1a01a26ffd",
      "c60c4d0eaab1414db3b6871e74195b20",
      "27d310b9b91b4b40bbc336a93dea1d32",
      "23a4802555ea4871984073bf221d7cc4",
      "04e0710978604e6bb9ca3f574ae23355",
      "53de3cd6c65f41fc832797fd46638a4e",
      "66a85437ebad4e85a7a168b2783d9590",
      "c2558b0924604230b8bb70b0a09c2509",
      "347f169348f8498b917817ae01f6b44e",
      "465374e06b2d41788114ac5114c5bdee",
      "73c4d59387c846bfaf62fc03634492c7",
      "f701fddf9ed44b7b90018f46004ffb10",
      "13ede7eaaca74b008dfaf1aa78eaa1e4",
      "45a4b67438044fe88a40837cd3bdb755"
     ]
    },
    "id": "lMBnLMFA0Bvw",
    "executionInfo": {
     "status": "ok",
     "timestamp": 1766083540280,
     "user_tz": -60,
     "elapsed": 16620,
     "user": {
      "displayName": "Balzhan Daurenbekovna",
      "userId": "10210039747879563426"
     }
    },
    "outputId": "02ac615d-2370-48a7-a6a3-6e75f830fce9"
   },
   "execution_count": null,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "\n",
      " Starting processing for roberta-base...\n",
      "Tokenizing for Encoder model: roberta-base...\n"
     ]
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/25.0 [00:00<?, ?B/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "418c928fd6c14fcfb455f46c8e26b4e6"
      }
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "config.json:   0%|          | 0.00/481 [00:00<?, ?B/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "2e9d358ac46d4b0a907418aa799a76c6"
      }
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "vocab.json:   0%|          | 0.00/899k [00:00<?, ?B/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "c03c4f3ca45547e6806432e08d5fab8e"
      }
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "merges.txt:   0%|          | 0.00/456k [00:00<?, ?B/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "a492b4619a7d4d899821b85c0b4d3793"
      }
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/1.36M [00:00<?, ?B/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "23a4802555ea4871984073bf221d7cc4"
      }
     },
     "metadata": {}
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Tokenizing for Encoder model: roberta-base...\n"
     ]
    }
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Decoder"
   ],
   "metadata": {
    "id": "scFt0jjRO7aQ"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "#Formatting text to Prompts\n",
    "\n",
    "def format_decoder_prompt(text, task_name):\n",
    "\n",
    "    if task_name == \"Sentiment\":\n",
    "        # Prompt  output \"1\" or \"0\"\n",
    "        return f\"Generate the sentiment of the given text. 1 for positive, 0 for negative. Do not give an explanation.Text: {text} Answer:\"\n",
    "\n",
    "    elif task_name == \"Sarcasm\":\n",
    "        return f\"Predict if the given text is sarcastic. 1 if sarcastic, 0 if not. Do not give an explanation. Text: {text} Answer:\"\n",
    "\n",
    "    return text\n",
    "\n",
    "\n",
    "def prepare_decoder_data(train_df, val_df, test_df, task_name):\n",
    "    print(f\"Formatting prompts for Decoder (Task: {task_name})...\")\n",
    "\n",
    "    datasets = {'train': train_df, 'val': val_df, 'test': test_df}\n",
    "    processed_data = {}\n",
    "\n",
    "    for split_name, df in datasets.items():\n",
    "        # 1. Create Input (prompt)\n",
    "        prompts = df['text_clean'].apply(lambda x: format_decoder_prompt(x, task_name)).tolist()\n",
    "\n",
    "        # 2. Create Output (Target Text)\n",
    "        # convert numbers 0 and 1 to strings\n",
    "        targets = df['label'].astype(str).tolist()\n",
    "\n",
    "        # List of Strings\n",
    "        processed_data[split_name] = {\n",
    "            'prompts': prompts,\n",
    "            'targets': targets,\n",
    "            'original_labels': df['label'].values\n",
    "        }\n",
    "\n",
    "    return processed_data\n",
    "\n",
    "\n",
    "# apply\n",
    "print(\"\\n Processing Decoders \")\n",
    "gen_sentiment_data = prepare_decoder_data(train_sentiment, val_sentiment, test_sentiment, \"Sentiment\")\n",
    "\n",
    "gen_sarcasm_data  = prepare_decoder_data(train_sarcasm, val_sarcasm, test_sarcasm, \"Sarcasm\")"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Gl2N06_VO4YP",
    "executionInfo": {
     "status": "ok",
     "timestamp": 1766083572834,
     "user_tz": -60,
     "elapsed": 38,
     "user": {
      "displayName": "Balzhan Daurenbekovna",
      "userId": "10210039747879563426"
     }
    },
    "outputId": "3a94fa8d-edad-423e-baa8-67ac0d3fdd95"
   },
   "execution_count": null,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "\n",
      " Processing Decoders \n",
      "Formatting prompts for Decoder (Task: Sentiment)...\n",
      "Formatting prompts for Decoder (Task: Sarcasm)...\n"
     ]
    }
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "#  Print example results\n",
    "\n",
    "def inspect_decoder_data(data_dict, task_name):\n",
    "    print(f\"\\n\")\n",
    "    print(f\" Inspecting: {task_name}\")\n",
    "\n",
    "    # Cheking for all sets (train, val, test)\n",
    "    for split in ['train', 'val', 'test']:\n",
    "        split_data = data_dict[split]\n",
    "\n",
    "\n",
    "        n_prompts = len(split_data['prompts'])\n",
    "        n_targets = len(split_data['targets'])\n",
    "\n",
    "        print(f\"\\n Split: {split.upper()} \")\n",
    "        print(f\"Rows count: {n_prompts}\")\n",
    "\n",
    "        # Check length (should be True)\n",
    "        if n_prompts != n_targets:\n",
    "            print(f\"Mismatch! Prompts: {n_prompts}, Targets: {n_targets}\")\n",
    "        else:\n",
    "            print(f\"Lengths match.\")\n",
    "\n",
    "        # 2. Random example\n",
    "        if n_prompts > 0:\n",
    "            idx = random.randint(0, n_prompts - 1)\n",
    "            print(f\"\\n[Sample Data at index {idx}]\")\n",
    "            print(f\"PROMPT:  {split_data['prompts'][idx]}\")\n",
    "            print(f\"TARGET:  '{split_data['targets'][idx]}' (Type: {type(split_data['targets'][idx])})\")\n",
    "            print(f\"Raw Label: {split_data['original_labels'][idx]}\")\n",
    "\n",
    "\n",
    "inspect_decoder_data(gen_sentiment_data, \"SENTIMENT TASK\")\n",
    "inspect_decoder_data(gen_sarcasm_data, \"SARCASM TASK\")"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "5vOLOnqZO4T0",
    "executionInfo": {
     "status": "ok",
     "timestamp": 1766083577140,
     "user_tz": -60,
     "elapsed": 14,
     "user": {
      "displayName": "Balzhan Daurenbekovna",
      "userId": "10210039747879563426"
     }
    },
    "outputId": "f5af901c-7825-4333-da57-64357ccc1e33"
   },
   "execution_count": null,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "\n",
      "\n",
      " Inspecting: SENTIMENT TASK\n",
      "\n",
      " Split: TRAIN \n",
      "Rows count: 7979\n",
      "Lengths match.\n",
      "\n",
      "[Sample Data at index 5206]\n",
      "PROMPT:  Generate the sentiment of the given text. 1 for positive, 0 for negative. Do not give an explanation.Text: That's not true though. You have to take a moment to analyse the scenes they have. The lighting and the framing is obviously really good. The real problem is nowadays movies are very repetitive in terms of cinematography and music design. People don't want to experiment with the techniques unlike Hollywood productions - the real reason behind people hating bollywood movies Answer:\n",
      "TARGET:  '0' (Type: <class 'str'>)\n",
      "Raw Label: 0\n",
      "\n",
      " Split: VAL \n",
      "Rows count: 887\n",
      "Lengths match.\n",
      "\n",
      "[Sample Data at index 243]\n",
      "PROMPT:  Generate the sentiment of the given text. 1 for positive, 0 for negative. Do not give an explanation.Text: Best place for fast foods under one roof, \" Amazing quality with A1 Taste. \" Answer:\n",
      "TARGET:  '1' (Type: <class 'str'>)\n",
      "Raw Label: 1\n",
      "\n",
      " Split: TEST \n",
      "Rows count: 1212\n",
      "Lengths match.\n",
      "\n",
      "[Sample Data at index 208]\n",
      "PROMPT:  Generate the sentiment of the given text. 1 for positive, 0 for negative. Do not give an explanation.Text: Angus Taylor's National Press Club performance was shockingly bad today. It's like he's not even in the Shadow Cabinet meetings despite being Shadow Treasurer. He couldn't provide any details of any policies, and his premeditated lines completely contradicted Dutton's Budget Reply speech last week. Especially on Immigration and Nuclear where the Liberal's numbers are all over the shop. Answer:\n",
      "TARGET:  '0' (Type: <class 'str'>)\n",
      "Raw Label: 0\n",
      "\n",
      "\n",
      " Inspecting: SARCASM TASK\n",
      "\n",
      " Split: TRAIN \n",
      "Rows count: 8004\n",
      "Lengths match.\n",
      "\n",
      "[Sample Data at index 3384]\n",
      "PROMPT:  Predict if the given text is sarcastic. 1 if sarcastic, 0 if not. Do not give an explanation. Text: The new renovations have certainly lightened the mood of the dining rooms. We enjoyed our entrees more than our mains; likely because the prices seemed to be' right' for the entrees. Whereas the mains were a bit more pricey than we felt was appropriate( compared to other similar options in and around The Mount). Very good customer service, and the location is easily accessed-plenty of parking between The Mac and the public library / Robert Helpmann theatre. Answer:\n",
      "TARGET:  '0' (Type: <class 'str'>)\n",
      "Raw Label: 0\n",
      "\n",
      " Split: VAL \n",
      "Rows count: 890\n",
      "Lengths match.\n",
      "\n",
      "[Sample Data at index 36]\n",
      "PROMPT:  Predict if the given text is sarcastic. 1 if sarcastic, 0 if not. Do not give an explanation. Text: Got the Chicken n chips was the chicken was abit too thick for my liking but the sauces. The guy that was making the chips very friendly and the guy at the till that was putting the chicken and sauces on the food was not very friendly at all abit like a robot like he did n't want to be there! The shop itself needs updating and it does not have a card machine!!! Also no Seating area in the shop so it has to be takeout. Answer:\n",
      "TARGET:  '0' (Type: <class 'str'>)\n",
      "Raw Label: 0\n",
      "\n",
      " Split: TEST \n",
      "Rows count: 1216\n",
      "Lengths match.\n",
      "\n",
      "[Sample Data at index 1038]\n",
      "PROMPT:  Predict if the given text is sarcastic. 1 if sarcastic, 0 if not. Do not give an explanation. Text: I think this is a good value cinema in an excellent location, plenty of restaurants along the promenade for a before / after meal. I saw Equaliser movie, Denziel Washington at his best as the \" Avenging Angel \". I had a saver seat() it was OK but would like some recline. I think it would be worth paying 2 more for the VIP seats. Answer:\n",
      "TARGET:  '0' (Type: <class 'str'>)\n",
      "Raw Label: 0\n"
     ]
    }
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Save results"
   ],
   "metadata": {
    "id": "Ks5bHeYyWHor"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "print(f\"\\nSaving separate files to '{OUTPUT_DIR}'...\")\n",
    "\n",
    "# 1. Save Encoder Bert Files (Tensors)\n",
    "torch.save(bert_sentiment_data, os.path.join(OUTPUT_DIR, 'sentiment_bert_encoder_data.pt'))\n",
    "torch.save(bert_sarcasm_data,   os.path.join(OUTPUT_DIR, 'sarcasm_bert_encoder_data.pt'))\n",
    "\n",
    "# Saving Robert\n",
    "torch.save(roberta_sentiment_data, os.path.join(OUTPUT_DIR, 'sentiment_roberta_encoder.pt'))\n",
    "torch.save(roberta_sarcasm_data,   os.path.join(OUTPUT_DIR, 'sarcasm_roberta_encoder.pt'))\n",
    "\n",
    "print(\"RoBERTa inputs saved.\")\n",
    "\n",
    "# 2. Save Decoder Files (Lists of Strings)\n",
    "torch.save(gen_sentiment_data, os.path.join(OUTPUT_DIR, 'sentiment_decoder_data.pt'))\n",
    "torch.save(gen_sarcasm_data,   os.path.join(OUTPUT_DIR, 'sarcasm_decoder_data.pt'))\n",
    "\n",
    "print(\"All done, 4 clean files ready.\")"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "L-AsFZ-DO4WO",
    "executionInfo": {
     "status": "ok",
     "timestamp": 1766083638737,
     "user_tz": -60,
     "elapsed": 1125,
     "user": {
      "displayName": "Balzhan Daurenbekovna",
      "userId": "10210039747879563426"
     }
    },
    "outputId": "90bd785f-8b3f-4430-da21-7bcb8fee2e96"
   },
   "execution_count": null,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "\n",
      "Saving separate files to '/content/drive/MyDrive/DNLP/data/processed_data_final'...\n",
      "RoBERTa inputs saved.\n",
      "All done, 4 clean files ready.\n"
     ]
    }
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Compute weights for Labels"
   ],
   "metadata": {
    "id": "11hgAlwXrKB8"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "**Crosstab of Label x Task**\n",
    "\n",
    "\n",
    "`The dataset is imbalanced (~86% Non-Sarcastic vs ~14% Sarcastic). A model could cheat by predicting more \"0\" every time than \"1\".`\n",
    "\n",
    "\n",
    "\n",
    "**The weights are saved to be passed to the Loss Function (CrossEntropyLoss) further.**"
   ],
   "metadata": {
    "id": "JRF46PrV59Ax"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "\n",
    "\n",
    "OUTPUT_DIR = \"/content/drive/MyDrive/DNLP/data/processed_data_final\"\n",
    "if not os.path.exists(OUTPUT_DIR):\n",
    "    os.makedirs(OUTPUT_DIR)\n",
    "\n",
    "def get_and_save_weights(labels_array, task_name):\n",
    "    print(f\"\\n--- Calculating Weights for: {task_name} ---\")\n",
    "\n",
    "    # 1. \u0421alculate weights\n",
    "    class_weights = compute_class_weight(\n",
    "        class_weight='balanced',\n",
    "        classes=np.unique(labels_array),\n",
    "        y=labels_array\n",
    "    )\n",
    "\n",
    "    # 2. Convert to Tensor (float32 for Loss function)\n",
    "    weights_tensor = torch.tensor(class_weights, dtype=torch.float)\n",
    "\n",
    "    # 3. Print how many 0 and 1 each task has\n",
    "    print(f\"Counts: {np.bincount(labels_array)}\")\n",
    "    print(f\"Weights: Class 0: {weights_tensor[0]:.4f}, Class 1: {weights_tensor[1]:.4f}\")\n",
    "\n",
    "    # 4. Saving\n",
    "    save_path = os.path.join(OUTPUT_DIR, f'{task_name.lower()}_weights.pt')\n",
    "    torch.save(weights_tensor, save_path)\n",
    "    print(f\"Saved to: {save_path}\")\n",
    "\n",
    "    return weights_tensor\n",
    "\n",
    "\n",
    "# Take labels from train_sentiment \u0438 train_sarcasm\n",
    "\n",
    "# 1. Sentiment Weights\n",
    "# Expected 1.0, since there already 50/50\n",
    "sent_weights = get_and_save_weights(train_sentiment['label'].values, \"Sentiment\")\n",
    "\n",
    "# 2. Sarcasm Weights\n",
    "# Expected more values for sarcasm: Class - 1\n",
    "\n",
    "sarc_weights = get_and_save_weights(train_sarcasm['label'].values, \"Sarcasm\")\n",
    "\n"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "PCvD0OyffPW8",
    "executionInfo": {
     "status": "ok",
     "timestamp": 1766083676219,
     "user_tz": -60,
     "elapsed": 51,
     "user": {
      "displayName": "Balzhan Daurenbekovna",
      "userId": "10210039747879563426"
     }
    },
    "outputId": "dbbca779-2fa1-43a6-f36c-332e9a05d573"
   },
   "execution_count": null,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "\n",
      "--- Calculating Weights for: Sentiment ---\n",
      "Counts: [4025 3954]\n",
      "Weights: Class 0: 0.9912, Class 1: 1.0090\n",
      "Saved to: /content/drive/MyDrive/DNLP/data/processed_data_final/sentiment_weights.pt\n",
      "\n",
      "--- Calculating Weights for: Sarcasm ---\n",
      "Counts: [6857 1147]\n",
      "Weights: Class 0: 0.5836, Class 1: 3.4891\n",
      "Saved to: /content/drive/MyDrive/DNLP/data/processed_data_final/sarcasm_weights.pt\n"
     ]
    }
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Final outcome:"
   ],
   "metadata": {
    "id": "fhILTCfVmjrQ"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "**1. Data Structuring & Splitting**\n",
    "\n",
    "Separated Tasks: Split the raw dataset into two distinct streams: Sentiment and Sarcasm.\n",
    "\n",
    "Correct Splits:\n",
    "\n",
    "Train: ~8,000 rows per task (Derived from 90% of original Train).\n",
    "\n",
    "Validation: ~900 rows per task (Derived from 10% of original Train).\n",
    "\n",
    "Test: ~1,200 rows per task (Used the full HuggingFace validation set to match the paper\u2019s ~2.5k test size).\n",
    "\n",
    "\n",
    "**2. Data Cleaning**\n",
    "\n",
    "Applied for all dataframes.\n",
    "\n",
    "HTML & URL Removal: Web-scraped data (Google/Reddit) often contains artifacts\n",
    "\n",
    "Specific Reddit user mentions removal and etc...\n",
    "\n",
    "\n",
    "**3.Preparation for models**\n",
    "\n",
    "For Encoders (BERT/RoBERTa):\n",
    "\n",
    "Tokenized inputs (input_ids, attention_mask) padded to 512 tokens.\n",
    "\n",
    "Verified [CLS] and [SEP] tokens are correctly placed.\n",
    "\n",
    "For Decoder:\n",
    "\n",
    "Formatted inputs as Instruction Prompts (e.g., \"Predict if the given text is sarcastic...\").\n",
    "\n",
    "Formatted targets as Text Strings (\"0\" or \"1\") instead of integers.\n",
    "\n",
    "\n",
    "\n",
    "**4. Class Imbalance Handling**\n",
    "\n",
    "Calculated Class Weights separately for each task.\n",
    "\n",
    "Sentiment: Balanced weights (~1.0).\n",
    "\n",
    "Sarcasm: Heavily weighted to penalize missing the minority class (14% positive).\n",
    "\n",
    "Saved weights as tensors for use in CrossEntropyLoss.\n",
    "\n",
    "**Outcome:** All data files (.pt tensors for Encoders, Decoder, and weight tensors) are saved."
   ],
   "metadata": {
    "id": "Hu7YykDjmjVZ"
   }
  },
  {
   "cell_type": "code",
   "source": [],
   "metadata": {
    "id": "bhTJeABLoggw"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "b2126890"
   },
   "source": [
    "# Task\n",
    "Load the 'bert-base-cased' tokenizer. Then, define a function to tokenize text without truncation and calculate the length of the resulting token IDs. Apply this function to the 'text_clean' column of `train_sentiment` and `train_sarcasm` dataframes to get their token lengths. After that, filter the sentiment and sarcasm training dataframes to identify records where the token length is greater than 512. Finally, print the number of records longer than 512 tokens for both sentiment and sarcasm datasets and display a few sample records from these filtered dataframes, including their original text and calculated token length."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "c4672e81"
   },
   "source": [
    "## Load Tokenizer\n",
    "\n",
    "### Subtask:\n",
    "Load the 'bert-base-cased' tokenizer.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8686cc5d"
   },
   "source": [
    "**Reasoning**:\n",
    "To load the 'bert-base-cased' tokenizer, I will use the `AutoTokenizer.from_pretrained()` method as specified in the instructions.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "266780f7",
    "executionInfo": {
     "status": "ok",
     "timestamp": 1766083689987,
     "user_tz": -60,
     "elapsed": 1296,
     "user": {
      "displayName": "Balzhan Daurenbekovna",
      "userId": "10210039747879563426"
     }
    },
    "outputId": "aee83037-9013-45ff-891c-825e233f5637"
   },
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained('bert-base-cased')\n",
    "print(\"Tokenizer 'bert-base-cased' loaded successfully.\")"
   ],
   "execution_count": null,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Tokenizer 'bert-base-cased' loaded successfully.\n"
     ]
    }
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ef78a343"
   },
   "source": [
    "**Reasoning**:\n",
    "The previous code block failed because `AutoTokenizer` was not defined. I need to re-import `AutoTokenizer` from the `transformers` library to resolve this `NameError`.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "50d92f26",
    "executionInfo": {
     "status": "ok",
     "timestamp": 1766083699815,
     "user_tz": -60,
     "elapsed": 1466,
     "user": {
      "displayName": "Balzhan Daurenbekovna",
      "userId": "10210039747879563426"
     }
    },
    "outputId": "99a0a0aa-90de-4abc-8883-8828f974cba4"
   },
   "source": [
    "from transformers import AutoTokenizer\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained('bert-base-cased')\n",
    "print(\"Tokenizer 'bert-base-cased' loaded successfully.\")"
   ],
   "execution_count": null,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Tokenizer 'bert-base-cased' loaded successfully.\n"
     ]
    }
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3cd1aa55"
   },
   "source": [
    "**Reasoning**:\n",
    "I need to define a function that tokenizes text without truncation and calculates the length of the resulting token IDs. Then, I will apply this function to the 'text_clean' column of both `train_sentiment` and `train_sarcasm` dataframes to get their token lengths. The `tokenizer` variable is already loaded from the previous step.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "b33ba5ba",
    "executionInfo": {
     "status": "ok",
     "timestamp": 1766083710481,
     "user_tz": -60,
     "elapsed": 5206,
     "user": {
      "displayName": "Balzhan Daurenbekovna",
      "userId": "10210039747879563426"
     }
    },
    "outputId": "3483314e-7bfd-4f5c-9f4a-094b44b263f3"
   },
   "source": [
    "def get_token_length(text):\n",
    "    # Tokenize the text without truncation to get the full token list\n",
    "    tokens = tokenizer(text, truncation=False, return_tensors=None)\n",
    "    # The length of input_ids corresponds to the number of tokens\n",
    "    return len(tokens['input_ids'])\n",
    "\n",
    "# Apply the function to sentiment training data\n",
    "train_sentiment['token_length'] = train_sentiment['text_clean'].apply(get_token_length)\n",
    "print(\"Token lengths calculated for train_sentiment.\")\n",
    "\n",
    "# Apply the function to sarcasm training data\n",
    "train_sarcasm['token_length'] = train_sarcasm['text_clean'].apply(get_token_length)\n",
    "print(\"Token lengths calculated for train_sarcasm.\")"
   ],
   "execution_count": null,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "Token indices sequence length is longer than the specified maximum sequence length for this model (524 > 512). Running this sequence through the model will result in indexing errors\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Token lengths calculated for train_sentiment.\n",
      "Token lengths calculated for train_sarcasm.\n"
     ]
    }
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9eff7edb"
   },
   "source": [
    "**Reasoning**:\n",
    "The previous code block failed because the `train_sentiment` and `train_sarcasm` dataframes were not defined in the current execution context. I need to re-initialize these dataframes by including the necessary data loading, cleaning, and splitting steps from previous successful notebook cells before proceeding with the token length calculation.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "97ab8851",
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "status": "ok",
     "timestamp": 1766083731584,
     "user_tz": -60,
     "elapsed": 8409,
     "user": {
      "displayName": "Balzhan Daurenbekovna",
      "userId": "10210039747879563426"
     }
    },
    "outputId": "56724919-e331-4937-e16f-f154f5affba4"
   },
   "source": [
    "import re\n",
    "import pandas as pd\n",
    "from datasets import load_dataset\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Data Loading (from cell -0Xd-TcOzn5F)\n",
    "dataset = load_dataset(\"unswnlporg/BESSTIE\")\n",
    "train = dataset[\"train\"].to_pandas()\n",
    "val = dataset[\"validation\"].to_pandas()\n",
    "\n",
    "# Sentiment vs Sarcasm Task splitting (from cell DIyMEON2lV2N)\n",
    "sent_train = train[train['task'] == 'Sentiment'].copy()\n",
    "sent_val = val[val['task'] == 'Sentiment'].copy()\n",
    "sarc_train = train[train['task'] == 'Sarcasm'].copy()\n",
    "sarc_val = val[val['task'] == 'Sarcasm'].copy()\n",
    "\n",
    "# Cleaning function (from cell bbVYVjqmz_RG)\n",
    "def clean_text(text):\n",
    "    if pd.isna(text) or text == \"\":\n",
    "        return \"\"\n",
    "    text = str(text)\n",
    "    text = re.sub(r\"(u/|@)\\w+\", \"[USER]\", text)\n",
    "    text = re.sub(r\"&\\w+;\", \" \", text)\n",
    "    text = re.sub(r\"https?://\\S+|www\\.\\S+\", \"[URL]\", text)\n",
    "    text = re.sub(r\"#(\\w+)\", r\"\\1\", text)\n",
    "    text = re.sub(r\"\\s+\", \" \", text).strip()\n",
    "    return text\n",
    "\n",
    "# Apply cleaning function (from cell qrT1-H_mz_Oo)\n",
    "def apply_cleaning(df, name=\"Dataset\"):\n",
    "    df = df.copy()\n",
    "    df['text_clean'] = df['text'].apply(clean_text)\n",
    "    df_clean = df[df['text_clean'].notna()]\n",
    "    return df_clean\n",
    "\n",
    "sent_train = apply_cleaning(sent_train, \"Sentiment Train\")\n",
    "sarc_train = apply_cleaning(sarc_train, \"Sarcasm Train\")\n",
    "\n",
    "# Train / Val / Test Splitting function (from cell HARmIkbbmAnG)\n",
    "def internal_split(df):\n",
    "    train_split, val_split = train_test_split(\n",
    "        df,\n",
    "        test_size=0.10,\n",
    "        random_state=42,\n",
    "        stratify=df['label']\n",
    "    )\n",
    "    return train_split, val_split\n",
    "\n",
    "# Apply internal split\n",
    "train_sentiment, val_sentiment = internal_split(sent_train)\n",
    "train_sarcasm, val_sarcasm = internal_split(sarc_train)\n",
    "\n",
    "# Original code for token length calculation\n",
    "def get_token_length(text):\n",
    "    tokens = tokenizer(text, truncation=False, return_tensors=None)\n",
    "    return len(tokens['input_ids'])\n",
    "\n",
    "train_sentiment['token_length'] = train_sentiment['text_clean'].apply(get_token_length)\n",
    "print(\"Token lengths calculated for train_sentiment.\")\n",
    "\n",
    "train_sarcasm['token_length'] = train_sarcasm['text_clean'].apply(get_token_length)\n",
    "print(\"Token lengths calculated for train_sarcasm.\")"
   ],
   "execution_count": null,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Token lengths calculated for train_sentiment.\n",
      "Token lengths calculated for train_sarcasm.\n"
     ]
    }
   ]
  }
 ]
}