{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"machine_shape":"hm","gpuType":"T4"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU"},"cells":[{"cell_type":"markdown","source":["# Phase 1 - Data Preprocessing\n","\n","This notebook builds the **processed datasets** used by all downstream model notebooks (RoBERTa/BERT/Mistral).\n","\n","Key outputs (written to `data/processed_data/` and related folders):\n","- A **single canonical schema** (`text`, `label`, `task`, `variety_name`, `source_name`)\n","- Minimal **text normalization** (sarcasm-safe)\n","- Standard **test sets** (FULL / by source / by variety)\n","- Training **settings** (by source and by variety)\n","- Deterministic **train/val splits** with appropriate stratification\n","- Manifests + audit files so results are reproducible and easy to debug\n"],"metadata":{"id":"mOMmg15L2O97"}},{"cell_type":"markdown","source":["## Cell 1 — Imports\n","\n","Load standard libraries used across preprocessing, file I/O, and reproducibility.\n","We keep dependencies minimal and push reusable logic into `src/` so all notebooks share the same behavior."],"metadata":{"id":"UQI0GaGp2dw0"}},{"cell_type":"code","source":["import os, sys, json, random, re\n","from pathlib import Path\n","from typing import Dict, List, Tuple\n","\n","import numpy as np\n","import pandas as pd"],"metadata":{"id":"TJKgOSawETQA"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## Cell 2 — Mount Google Drive\n","\n","Mount Drive so all outputs (processed data, split files, reports) persist across Colab sessions."],"metadata":{"id":"fp7QATIA2hXz"}},{"cell_type":"code","source":["from google.colab import drive\n","drive.mount(\"/content/drive\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"HhmDOdHuH5lF","executionInfo":{"status":"ok","timestamp":1769344066791,"user_tz":-60,"elapsed":947,"user":{"displayName":"DNLP Project","userId":"02984253218627728392"}},"outputId":"d926ad5d-da22-423e-9f56-62aafb8e0db5"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"]}]},{"cell_type":"markdown","source":["## Cell 3 — Project paths\n","\n","Define the project root and key directories:\n","- `data/raw`: input files (expected: `besstie_train.*`, `besstie_validation.*`)\n","- `data/processed_data`: processed outputs used by training notebooks\n","- `data/splits`: JSON split metadata (row ids, strategy, seed)\n","- `data/reports`: preprocessing audits and sanity checks\n","- `src/`: reusable preprocessing modules written below"],"metadata":{"id":"tX4xt27M2kpG"}},{"cell_type":"code","source":["BASE = Path(\"/content/drive/MyDrive/DNLP\")\n","\n","DATA_DIR   = BASE / \"data\"\n","RAW_DIR    = DATA_DIR / \"raw\"\n","PROC_DIR   = DATA_DIR / \"processed\"\n","SPLITS_DIR = DATA_DIR / \"splits\"\n","REPORT_DIR = DATA_DIR / \"reports\"\n","\n","SRC_DIR    = BASE / \"src\"\n","NB_DIR     = BASE / \"notebooks\"\n","DOCS_DIR   = BASE / \"docs\"\n","MODELS_DIR = BASE / \"models\"\n","\n","# Ensure base directories exist\n","for d in [DATA_DIR, RAW_DIR, PROC_DIR, SPLITS_DIR, REPORT_DIR, SRC_DIR, NB_DIR, DOCS_DIR, MODELS_DIR]:\n","    d.mkdir(parents=True, exist_ok=True)\n","\n","print(\"BASE:\", BASE)\n","print(\"RAW:\", RAW_DIR)\n","print(\"PROC:\", PROC_DIR)\n","print(\"SPLITS:\", SPLITS_DIR)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"dhLYD8QGH65B","executionInfo":{"status":"ok","timestamp":1770297959728,"user_tz":-60,"elapsed":3095,"user":{"displayName":"Amir Masoud Almasi","userId":"04359925793684512423"}},"outputId":"1e4c70b9-06da-431e-dd67-ecb8f2478e76"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["BASE: /content/drive/MyDrive/DNLP\n","RAW: /content/drive/MyDrive/DNLP/data/raw\n","PROC: /content/drive/MyDrive/DNLP/data/processed\n","SPLITS: /content/drive/MyDrive/DNLP/data/splits\n"]}]},{"cell_type":"markdown","source":["## Cell 4 — Global configuration\n","\n","Centralize settings that affect all runs:\n","- Random seed for deterministic splits\n","- Validation ratio\n","- Sarcasm source filtering (e.g., keep sarcasm only from Reddit)\n","- `MAX_LEN_FOR_MODELS=512` (note: actual tokenization happens later in model notebooks)"],"metadata":{"id":"DLAw_v952pNk"}},{"cell_type":"code","source":["CFG = {\n","    \"SEED\": 42,\n","    \"VAL_RATIO\": 0.2,             # internal val from train pool\n","    \"SARC_SOURCE_ONLY\": \"Reddit\", # keep sarcasm only from Reddit\n","    \"MAX_LEN_FOR_MODELS\": 512,\n","}\n","\n","def set_seed(seed: int):\n","    random.seed(seed)\n","    np.random.seed(seed)\n","    os.environ[\"PYTHONHASHSEED\"] = str(seed)\n","\n","set_seed(CFG[\"SEED\"])"],"metadata":{"id":"qDMASsJCH-B1"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## Cell 5 — Initialize `src/` package\n","\n","Create a minimal Python package under `src/` so later notebooks can `import src.*`\n","instead of copying helper code around."],"metadata":{"id":"VWXVOWOG2r2n"}},{"cell_type":"code","source":["PKG_DIR = SRC_DIR\n","PKG_DIR.mkdir(parents=True, exist_ok=True)\n","\n","(PKG_DIR / \"__init__.py\").write_text(\n","    \"__version__ = '1.1.0'\\n\"\n",")\n","\n","print(\"Created package:\", PKG_DIR)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"-kzFNhqtIAQl","executionInfo":{"status":"ok","timestamp":1770297967436,"user_tz":-60,"elapsed":822,"user":{"displayName":"Amir Masoud Almasi","userId":"04359925793684512423"}},"outputId":"185893a6-4b50-4b13-f4dc-c9447e2458ca"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Created package: /content/drive/MyDrive/DNLP/src\n"]}]},{"cell_type":"markdown","source":["## Cell 6 — Write `src/io_utils.py`\n","\n","Create small, reusable utilities:\n","- `load_any`: load CSV or Parquet\n","- `safe_name`: sanitize names for folder/file creation (keeps `en-AU` style hyphens)"],"metadata":{"id":"ABq2WG_W2vj2"}},{"cell_type":"code","source":["io_code = r'''\n","from pathlib import Path\n","import pandas as pd\n","import shutil\n","\n","def load_any(path: Path) -> pd.DataFrame:\n","    suf = path.suffix.lower()\n","    if suf == \".csv\":\n","        return pd.read_csv(path)\n","    if suf in [\".parquet\", \".pq\"]:\n","        return pd.read_parquet(path)\n","    raise ValueError(f\"Unsupported file type: {path}\")\n","\n","def safe_name(s: str) -> str:\n","    # Aggressive filename safety: lowercase, no spaces, standard hyphens\n","    s = str(s).strip().lower()\n","    s = s.replace(\" \", \"\")\n","    s = s.replace(\"/\", \"-\")\n","    # minimal fallback for weird chars\n","    s = \"\".join([c if c.isalnum() or c in \"-_.\" else \"\" for c in s])\n","    return s\n","\n","def clean_dir(path: Path):\n","    \"\"\"Safely delete and recreate a directory.\"\"\"\n","    if path.exists():\n","        shutil.rmtree(path)\n","    path.mkdir(parents=True, exist_ok=True)\n","'''\n","(PKG_DIR / \"io_utils.py\").write_text(io_code)\n","print(\"Wrote:\", PKG_DIR / \"io_utils.py\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"wB7iqymkIDMz","executionInfo":{"status":"ok","timestamp":1769342893176,"user_tz":-60,"elapsed":10,"user":{"displayName":"DNLP Project","userId":"02984253218627728392"}},"outputId":"17b835fd-a7e2-4204-81b1-7b4a6656ba07"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Wrote: /content/drive/MyDrive/DNLP/src/io_utils.py\n"]}]},{"cell_type":"markdown","source":["## Cell 7 — Write `src/text_norm.py`\n","\n","Define **minimal, sarcasm-safe** text normalization:\n","- Replace URLs with `<url>`, usernames with `<user>`, and numbers with `<num>`\n","- Decode common HTML entities\n","- Only whitespace cleanup (do NOT remove punctuation/emojis/casing/elongations)\n","\n","This preserves the cues that matter for figurative language/sarcasm."],"metadata":{"id":"l1piugFZ2zvS"}},{"cell_type":"code","source":["norm_code = r'''\n","import re\n","\n","URL_RE  = re.compile(r\"(https?://\\S+|www\\.\\S+)\", re.IGNORECASE)\n","USER_RE = re.compile(r\"(?<!\\w)@\\w+\")\n","NUM_RE  = re.compile(r\"(?<!\\w)\\d+([.,]\\d+)?(?!\\w)\")\n","\n","HTML_ENT = {\n","    \"&amp;\": \"&\", \"&lt;\": \"<\", \"&gt;\": \">\", \"&quot;\": '\"', \"&#39;\": \"'\", \"&nbsp;\": \" \"\n","}\n","\n","def normalize_text(s: str) -> str:\n","    if s is None:\n","        return \"\"\n","    s = str(s)\n","\n","    for k, v in HTML_ENT.items():\n","        s = s.replace(k, v)\n","\n","    s = URL_RE.sub(\"<url>\", s)\n","    s = USER_RE.sub(\"<user>\", s)\n","    s = NUM_RE.sub(\"<num>\", s)\n","\n","    s = re.sub(r\"\\s+\", \" \", s).strip()\n","    return s\n","'''\n","(PKG_DIR / \"text_norm.py\").write_text(norm_code)\n","print(\"Wrote:\", PKG_DIR / \"text_norm.py\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"hZ0FuVbyIFEC","executionInfo":{"status":"ok","timestamp":1769342908310,"user_tz":-60,"elapsed":27,"user":{"displayName":"DNLP Project","userId":"02984253218627728392"}},"outputId":"ea77ab18-65a2-4ae6-ba49-a51457cf8cad"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Wrote: /content/drive/MyDrive/DNLP/src/text_norm.py\n"]}]},{"cell_type":"markdown","source":["## Cell 8 — Write `src/schema.py`\n","\n","Enforce a single canonical schema regardless of raw column naming:\n","- Map columns (`variety`→`variety_name`, `source`→`source_name`, etc.)\n","- Cast types\n","- Validate binary labels {0,1}\n","\n","If anything is missing or inconsistent, we fail early with a clear error."],"metadata":{"id":"NoNno_7n286b"}},{"cell_type":"code","source":["schema_code = r'''\n","import pandas as pd\n","\n","CANON = {\n","    \"text\": [\"text\"],\n","    \"label\": [\"label\"],\n","    \"task\": [\"task\"],\n","    \"variety_name\": [\"variety\", \"variety_name\"],\n","    \"source_name\": [\"source\", \"source_name\"],\n","}\n","\n","# Fix A: Canonical Source Mapping\n","SOURCE_MAP = {\n","    \"reddit\": \"Reddit\",\n","    \"google\": \"Google\",\n","    \"twitter\": \"Twitter\",\n","    \"youtube\": \"YouTube\"\n","}\n","\n","def fix_dashes(s: str) -> str:\n","    \"\"\"Normalize weird unicode dashes (en-dash, em-dash) to standard hyphen.\"\"\"\n","    if not isinstance(s, str): return str(s)\n","    return s.replace(u'\\u2013', '-').replace(u'\\u2014', '-').replace(u'\\u00AD', '-')\n","\n","def canonicalize(df: pd.DataFrame) -> pd.DataFrame:\n","    df = df.copy()\n","    lower_map = {c.lower(): c for c in df.columns}\n","\n","    found = {}\n","    for std, aliases in CANON.items():\n","        for a in aliases:\n","            if a in lower_map:\n","                found[std] = lower_map[a]\n","                break\n","\n","    missing = [k for k in CANON.keys() if k not in found]\n","    if missing:\n","        raise ValueError(f\"Missing columns {missing}. Found mapping={found}.\")\n","\n","    df = df.rename(columns={found[k]: k for k in found})\n","\n","    # --- CANONICAL VALUE NORMALIZATION ---\n","    df[\"text\"] = df[\"text\"].astype(str)\n","    df[\"label\"] = df[\"label\"].astype(int)\n","\n","    # Force task to lowercase (sentiment/sarcasm)\n","    df[\"task\"] = df[\"task\"].astype(str).str.strip().str.lower()\n","\n","    # Fix variety names (unicode dashes)\n","    df[\"variety_name\"] = df[\"variety_name\"].apply(fix_dashes).str.strip()\n","\n","    # Fix source names (Map 'reddit' -> 'Reddit')\n","    df[\"source_name\"] = df[\"source_name\"].astype(str).str.strip()\n","    df[\"source_name\"] = df[\"source_name\"].apply(lambda x: SOURCE_MAP.get(x.lower(), x))\n","\n","    uniq = sorted(df[\"label\"].unique().tolist())\n","    if set(uniq) - {0, 1}:\n","        raise ValueError(f\"Label values are {uniq} (expected binary 0/1).\")\n","\n","    return df\n","'''\n","(PKG_DIR / \"schema.py\").write_text(schema_code)\n","print(\"Wrote:\", PKG_DIR / \"schema.py\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"3jkU-2meIITP","executionInfo":{"status":"ok","timestamp":1769343912787,"user_tz":-60,"elapsed":70,"user":{"displayName":"DNLP Project","userId":"02984253218627728392"}},"outputId":"3010e472-11a3-4649-9284-38096eca1473"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Wrote: /content/drive/MyDrive/DNLP/src/schema.py\n"]}]},{"cell_type":"markdown","source":["## Cell 9 — Write `src/splits.py`\n","\n","Implement deterministic train/val splitting:\n","- `stratified_split_indices`: stratify by **label** (default)\n","- `stratified_split_indices_multi`: stratify by composite keys (e.g., **label + variety**)\n","\n","We use label+variety stratification when the training pool contains multiple varieties, to avoid skew.\n","Also includes a helper to save split metadata as JSON."],"metadata":{"id":"AMbnHHIl3BZF"}},{"cell_type":"code","source":["splits_code = r'''\n","import json\n","import numpy as np\n","from pathlib import Path\n","from typing import Tuple, List, Dict\n","\n","def stratified_split_indices(labels: np.ndarray, val_ratio: float, seed: int) -> Tuple[np.ndarray, np.ndarray]:\n","    rng = np.random.default_rng(seed)\n","    idx = np.arange(len(labels))\n","    y = labels.astype(int)\n","\n","    classes = np.unique(y)\n","    if len(classes) < 2:\n","        rng.shuffle(idx)\n","        n_val = int(np.ceil(len(idx) * val_ratio))\n","        return idx[n_val:], idx[:n_val]\n","\n","    train_idx, val_idx = [], []\n","    for c in classes:\n","        c_idx = idx[y == c]\n","        rng.shuffle(c_idx)\n","        n_c = len(c_idx)\n","        n_val = int(np.round(n_c * val_ratio))\n","        if n_c >= 2:\n","            n_val = max(1, min(n_val, n_c - 1))\n","        else:\n","            n_val = 0\n","        val_idx.append(c_idx[:n_val])\n","        train_idx.append(c_idx[n_val:])\n","\n","    train_idx = np.concatenate(train_idx) if len(train_idx) else np.array([], dtype=int)\n","    val_idx   = np.concatenate(val_idx) if len(val_idx) else np.array([], dtype=int)\n","    rng.shuffle(train_idx)\n","    rng.shuffle(val_idx)\n","    return train_idx, val_idx\n","\n","def stratified_split_indices_multi(strata: np.ndarray, val_ratio: float, seed: int) -> Tuple[np.ndarray, np.ndarray]:\n","    rng = np.random.default_rng(seed)\n","    idx = np.arange(len(strata))\n","    uniq = np.unique(strata)\n","    train_idx, val_idx = [], []\n","\n","    for s in uniq:\n","        s_idx = idx[strata == s]\n","        rng.shuffle(s_idx)\n","        n_s = len(s_idx)\n","        n_val = int(np.round(n_s * val_ratio))\n","        if n_s >= 2:\n","            n_val = max(1, min(n_val, n_s - 1))\n","        else:\n","            n_val = 0\n","        val_idx.append(s_idx[:n_val])\n","        train_idx.append(s_idx[n_val:])\n","\n","    train_idx = np.concatenate(train_idx) if len(train_idx) else np.array([], dtype=int)\n","    val_idx   = np.concatenate(val_idx) if len(val_idx) else np.array([], dtype=int)\n","    rng.shuffle(train_idx)\n","    rng.shuffle(val_idx)\n","    return train_idx, val_idx\n","\n","def save_json(path: Path, obj: Dict):\n","    path.parent.mkdir(parents=True, exist_ok=True)\n","    with open(path, \"w\") as f:\n","        json.dump(obj, f, indent=2)\n","'''\n","(PKG_DIR / \"splits.py\").write_text(splits_code)\n","print(\"Wrote:\", PKG_DIR / \"splits.py\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"YOahmW2YIK4a","executionInfo":{"status":"ok","timestamp":1769342934644,"user_tz":-60,"elapsed":5,"user":{"displayName":"DNLP Project","userId":"02984253218627728392"}},"outputId":"e71baa32-734a-4fb2-9832-92212d52777c"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Wrote: /content/drive/MyDrive/DNLP/src/splits.py\n"]}]},{"cell_type":"markdown","source":["## Cell 10 — Main Pipeline\n","\n","This is the core preprocessing logic:\n","1. Load raw train/validation files and canonicalize schema  \n","2. Compute `text_norm` with minimal normalization  \n","3. Apply the agreed constraint: **sarcasm only from one source** (e.g., Reddit)  \n","4. Save debuggable snapshots (`_all/train_all.csv`, `_all/validation_all.csv`)  \n","5. Build standardized **test sets** per task:\n","   - `TEST_FULL`\n","   - `TEST_<source>`\n","   - `TEST_<variety>`\n","6. Build training **settings** per task:\n","   - Sentiment: train on each **source** and also each **variety** (for cross-variety analysis)\n","   - Sarcasm: `FULL` + per-variety training settings\n","7. For each (task, setting), create **train/val split** with correct stratification:\n","   - If multiple varieties in the training pool → stratify by **label+variety**\n","   - Otherwise → stratify by **label**\n","8. Save:\n","   - `train.csv`, `val.csv`\n","   - `manifest.json` (pointers + summaries)\n","   - split JSON with row IDs\n","   - `index.csv` (master table of all settings)\n","   - `preprocess_audit.csv` (what strategy was used and resulting distributions)"],"metadata":{"id":"2tmrDZsH3G3w"}},{"cell_type":"code","source":["build_code = r'''\n","import json, shutil\n","from pathlib import Path\n","from typing import Dict, List, Tuple\n","\n","import numpy as np\n","import pandas as pd\n","\n","from .io_utils import load_any, safe_name\n","from .schema import canonicalize\n","from .text_norm import normalize_text\n","from .splits import stratified_split_indices, stratified_split_indices_multi, save_json\n","\n","KEEP_COLS = [\"row_id\",\"task\",\"label\",\"variety_name\",\"source_name\",\"text\",\"text_norm\"]\n","\n","def find_raw_files(raw_dir: Path) -> Tuple[Path, Path]:\n","    candidates = list(raw_dir.glob(\"besstie_train.*\"))\n","    candidates2 = list(raw_dir.glob(\"besstie_validation.*\"))\n","    if len(candidates) != 1 or len(candidates2) != 1:\n","        raise FileNotFoundError(f\"Missing train/val in {raw_dir}\")\n","    return candidates[0], candidates2[0]\n","\n","def summarize(df: pd.DataFrame) -> Dict:\n","    return {\n","        \"n\": int(len(df)),\n","        \"label_counts\": df[\"label\"].value_counts().to_dict() if len(df) else {},\n","        \"variety_counts\": df[\"variety_name\"].value_counts().to_dict() if len(df) else {},\n","        \"source_counts\": df[\"source_name\"].value_counts().to_dict() if len(df) else {},\n","    }\n","\n","def filter_trainpool(df_task_train: pd.DataFrame, task: str, setting: str) -> pd.DataFrame:\n","    # Logic: Sentiment (Source or Variety); Sarcasm (FULL or Variety)\n","    if task == \"sentiment\":\n","        if setting in df_task_train[\"source_name\"].unique():\n","            return df_task_train[df_task_train[\"source_name\"] == setting].copy()\n","        if setting.startswith(\"TRAIN_\"):\n","            v = setting.replace(\"TRAIN_\", \"\")\n","            return df_task_train[df_task_train[\"variety_name\"] == v].copy()\n","        return df_task_train.copy()\n","\n","    if task == \"sarcasm\":\n","        if setting == \"FULL\":\n","            return df_task_train.copy()\n","        if setting.startswith(\"TRAIN_\"):\n","            v = setting.replace(\"TRAIN_\", \"\")\n","            return df_task_train[df_task_train[\"variety_name\"] == v].copy()\n","\n","    return df_task_train.copy()\n","\n","def build_testsets(df_task_valid: pd.DataFrame, out_dir_task: Path) -> pd.DataFrame:\n","    # Writes to proc_dir/<task>/testsets/\n","    test_dir = out_dir_task / \"testsets\"\n","    test_dir.mkdir(parents=True, exist_ok=True)\n","\n","    rows = []\n","\n","    # 1. FULL\n","    p_full = test_dir / \"TEST_FULL.csv\"\n","    df_task_valid[KEEP_COLS].to_csv(p_full, index=False)\n","    rows.append({\"test_setting\":\"TEST_FULL\", \"csv\":str(p_full), **summarize(df_task_valid)})\n","\n","    # 2. By Source\n","    for src in sorted(df_task_valid[\"source_name\"].unique()):\n","        df_s = df_task_valid[df_task_valid[\"source_name\"] == src].copy()\n","        p = test_dir / f\"TEST_{safe_name(src)}.csv\"\n","        df_s[KEEP_COLS].to_csv(p, index=False)\n","        rows.append({\"test_setting\":f\"TEST_{src}\", \"csv\":str(p), **summarize(df_s)})\n","\n","    # 3. By Variety\n","    for v in sorted(df_task_valid[\"variety_name\"].unique()):\n","        df_v = df_task_valid[df_task_valid[\"variety_name\"] == v].copy()\n","        p = test_dir / f\"TEST_{safe_name(v)}.csv\"\n","        df_v[KEEP_COLS].to_csv(p, index=False)\n","        rows.append({\"test_setting\":f\"TEST_{v}\", \"csv\":str(p), **summarize(df_v)})\n","\n","    idx = pd.DataFrame(rows).sort_values(\"test_setting\").reset_index(drop=True)\n","    idx.to_csv(test_dir / \"index_testsets.csv\", index=False)\n","    return idx\n","\n","def run_preprocess(raw_dir, proc_dir, splits_dir, report_dir, seed, val_ratio, sarc_source_only, max_len_for_models):\n","    train_file, valid_file = find_raw_files(raw_dir)\n","\n","    # Load & Canonicalize\n","    df_train = canonicalize(load_any(train_file))\n","    df_valid = canonicalize(load_any(valid_file))\n","\n","    # Normalize Text\n","    df_train[\"text_norm\"] = df_train[\"text\"].apply(normalize_text)\n","    df_valid[\"text_norm\"] = df_valid[\"text\"].apply(normalize_text)\n","\n","    # IDs\n","    df_train = df_train.reset_index(drop=True)\n","    df_valid = df_valid.reset_index(drop=True)\n","    df_train[\"row_id\"] = np.arange(len(df_train), dtype=np.int64)\n","    df_valid[\"row_id\"] = np.arange(len(df_valid), dtype=np.int64) + int(10**9)\n","\n","    # Filter Sarcasm Source (Handle Case Insensitivity via Canonical Names)\n","    # Note: canonicalize() now forces \"Reddit\" to be Title Case, so simple comparison is safe.\n","    mask_tr_sarc = df_train[\"task\"] == \"sarcasm\"\n","    mask_va_sarc = df_valid[\"task\"] == \"sarcasm\"\n","\n","    if mask_tr_sarc.any():\n","        # sarc_source_only from config is usually \"Reddit\"\n","        df_train = pd.concat([df_train[~mask_tr_sarc], df_train[mask_tr_sarc & (df_train[\"source_name\"] == sarc_source_only)]], ignore_index=True)\n","    if mask_va_sarc.any():\n","        df_valid = pd.concat([df_valid[~mask_va_sarc], df_valid[mask_va_sarc & (df_valid[\"source_name\"] == sarc_source_only)]], ignore_index=True)\n","\n","    # Reset IDs after filtering\n","    df_train = df_train.reset_index(drop=True)\n","    df_valid = df_valid.reset_index(drop=True)\n","    df_train[\"row_id\"] = np.arange(len(df_train), dtype=np.int64)\n","    df_valid[\"row_id\"] = np.arange(len(df_valid), dtype=np.int64) + int(10**9)\n","\n","    # --- SAVE FULL SNAPSHOTS ---\n","    (proc_dir / \"_all\").mkdir(parents=True, exist_ok=True)\n","    df_train[KEEP_COLS].to_csv(proc_dir / \"_all\" / \"train_all.csv\", index=False)\n","    df_valid[KEEP_COLS].to_csv(proc_dir / \"_all\" / \"validation_all.csv\", index=False)\n","\n","    tasks = sorted(df_train[\"task\"].unique().tolist())\n","    all_index_rows = []\n","    audit_rows = []\n","\n","    for task in tasks:\n","        # Create Task Directory Structure\n","        task_dir = proc_dir / safe_name(task)\n","        trainsets_dir = task_dir / \"trainsets\"\n","        splits_out_dir = task_dir / \"splits\"\n","\n","        for d in [task_dir, trainsets_dir, splits_out_dir]:\n","            d.mkdir(parents=True, exist_ok=True)\n","\n","        # 1. Build Testsets\n","        df_task_valid = df_valid[df_valid[\"task\"] == task].copy()\n","        testsets_index_path = task_dir / \"testsets\" / \"index_testsets.csv\"\n","        if len(df_task_valid) > 0:\n","            build_testsets(df_task_valid, task_dir)\n","\n","        # 2. Build Training Settings\n","        df_task_train = df_train[df_train[\"task\"] == task].copy()\n","        if len(df_task_train) == 0: continue\n","\n","        # Define Settings\n","        if task == \"sentiment\":\n","            settings = sorted(df_task_train[\"source_name\"].unique().tolist()) + \\\n","                       [f\"TRAIN_{v}\" for v in sorted(df_task_train[\"variety_name\"].unique())]\n","        elif task == \"sarcasm\":\n","            settings = [\"FULL\"] + [f\"TRAIN_{v}\" for v in sorted(df_task_train[\"variety_name\"].unique())]\n","        else:\n","            settings = [\"FULL\"]\n","\n","        task_settings_rows = []\n","\n","        for setting in settings:\n","            tr_pool = filter_trainpool(df_task_train, task, setting)\n","            if len(tr_pool) < 10: continue\n","\n","            # --- Fix B: IMPROVED STRATIFICATION ---\n","            n_var = tr_pool[\"variety_name\"].nunique()\n","            n_src = tr_pool[\"source_name\"].nunique()\n","\n","            # If variety differs, split by label+variety\n","            if n_var > 1:\n","                strategy = \"label_variety\"\n","                strata = (tr_pool[\"label\"].astype(str) + \"__\" + tr_pool[\"variety_name\"]).values\n","                tr_idx, va_idx = stratified_split_indices_multi(strata, val_ratio, seed)\n","            # If variety is fixed (e.g. TRAIN_Au) but source differs (Google/Reddit), split by label+source\n","            elif n_src > 1:\n","                strategy = \"label_source\"\n","                strata = (tr_pool[\"label\"].astype(str) + \"__\" + tr_pool[\"source_name\"]).values\n","                tr_idx, va_idx = stratified_split_indices_multi(strata, val_ratio, seed)\n","            # Otherwise simple stratified\n","            else:\n","                strategy = \"label\"\n","                strata = tr_pool[\"label\"].values\n","                tr_idx, va_idx = stratified_split_indices(strata, val_ratio, seed)\n","\n","            df_tr = tr_pool.iloc[tr_idx][KEEP_COLS].reset_index(drop=True)\n","            df_va = tr_pool.iloc[va_idx][KEEP_COLS].reset_index(drop=True)\n","\n","            out_dir = trainsets_dir / safe_name(setting)\n","            out_dir.mkdir(parents=True, exist_ok=True)\n","\n","            p_tr = out_dir / \"train.csv\"\n","            p_va = out_dir / \"val.csv\"\n","            df_tr.to_csv(p_tr, index=False)\n","            df_va.to_csv(p_va, index=False)\n","\n","            # Splits JSON\n","            split_obj = {\n","                \"task\": task, \"setting\": setting, \"seed\": seed, \"val_ratio\": val_ratio,\n","                \"split_strategy\": strategy, \"max_len_for_models\": int(max_len_for_models),\n","                \"train_row_ids\": df_tr[\"row_id\"].tolist(), \"val_row_ids\": df_va[\"row_id\"].tolist(),\n","            }\n","            p_split_global = splits_dir / f\"{safe_name(task)}__{safe_name(setting)}.json\"\n","            save_json(p_split_global, split_obj)\n","\n","            p_split_local = splits_out_dir / f\"{safe_name(setting)}.json\"\n","            save_json(p_split_local, split_obj)\n","\n","            # Manifest\n","            manifest = {\n","                \"task\": task, \"setting\": setting,\n","                \"files\": {\"train_csv\": str(p_tr), \"val_csv\": str(p_va)},\n","                \"splits_json\": str(p_split_global),\n","                \"testsets_index_csv\": str(testsets_index_path),\n","                \"summary\": {\"train\": summarize(df_tr), \"val\": summarize(df_va)}\n","            }\n","            save_json(out_dir / \"manifest.json\", manifest)\n","\n","            # Record for indices\n","            row = {\n","                \"task\": task, \"setting\": setting,\n","                \"train_csv\": str(p_tr), \"val_csv\": str(p_va),\n","                \"splits_json\": str(p_split_global),\n","                \"testsets_index_csv\": str(testsets_index_path),\n","                \"n_train\": len(df_tr), \"n_val\": len(df_va)\n","            }\n","            all_index_rows.append(row)\n","            task_settings_rows.append(row)\n","\n","            # Audit\n","            audit_rows.append({\n","                \"task\": task, \"setting\": setting, \"split_strategy\": strategy,\n","                \"train_label_dist\": dict(df_tr[\"label\"].value_counts(normalize=True)),\n","                \"val_label_dist\": dict(df_va[\"label\"].value_counts(normalize=True))\n","            })\n","\n","        # --- SAVE PER-TASK INDICES ---\n","        if task_settings_rows:\n","            df_ts = pd.DataFrame(task_settings_rows)\n","            # 1. processed/<task>/index_settings.csv\n","            df_ts.to_csv(task_dir / \"index_settings.csv\", index=False)\n","            # 2. processed/<task>/trainsets/index_trainsets.csv\n","            df_ts.to_csv(trainsets_dir / \"index_trainsets.csv\", index=False)\n","\n","            # 3. processed/<task>/train/index_train.csv (Fake this one if loader looks for it)\n","            (task_dir / \"train\").mkdir(exist_ok=True)\n","            df_ts.to_csv(task_dir / \"train\" / \"index_train.csv\", index=False)\n","\n","            # 4. Fix D: processed/<task>/index_train.csv (The missing one!)\n","            df_ts.to_csv(task_dir / \"index_train.csv\", index=False)\n","\n","    # Global Index\n","    index_df = pd.DataFrame(all_index_rows)\n","    if not index_df.empty:\n","        index_df = index_df.sort_values([\"task\", \"setting\"]).reset_index(drop=True)\n","        index_df.to_csv(proc_dir / \"index.csv\", index=False)\n","\n","    audit_df = pd.DataFrame(audit_rows)\n","    audit_df.to_csv(report_dir / \"preprocess_audit.csv\", index=False)\n","\n","    return index_df\n","'''\n","(PKG_DIR / \"preprocess.py\").write_text(build_code)\n","print(\"Wrote:\", PKG_DIR / \"preprocess.py\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"jIQUDf1xINBH","executionInfo":{"status":"ok","timestamp":1770297978484,"user_tz":-60,"elapsed":805,"user":{"displayName":"Amir Masoud Almasi","userId":"04359925793684512423"}},"outputId":"a30b84ff-66f3-488d-97f5-8d1a5c54ecdb"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Wrote: /content/drive/MyDrive/DNLP/src/preprocess.py\n"]}]},{"cell_type":"markdown","source":["## Cell 11 — Import the pipeline entrypoint\n","\n","Add project root to `sys.path` and import `run_preprocess` from `src.preprocess`.\n","This keeps downstream notebooks clean and consistent."],"metadata":{"id":"HdyJ8VBc3Rem"}},{"cell_type":"code","source":["import sys\n","if str(BASE) not in sys.path:\n","    sys.path.insert(0, str(BASE))\n","\n","if str(SRC_DIR) in sys.path:\n","    sys.path.remove(str(SRC_DIR))\n","\n","from src.preprocess import run_preprocess\n","from src.io_utils import clean_dir"],"metadata":{"id":"jonaYkCNIPVQ"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## Cell 12 — Run preprocessing\n","\n","Execute `run_preprocess(...)` with the configured paths and parameters.\n","This writes all processed datasets + indices to disk and returns the master `index_df`."],"metadata":{"id":"yUjotj-33U9c"}},{"cell_type":"code","source":["# 1. Clean old outputs to prevent mixed state\n","print(\"Cleaning old processed data...\")\n","clean_dir(PROC_DIR)\n","clean_dir(SPLITS_DIR)\n","clean_dir(REPORT_DIR)\n","\n","# 2. Run\n","index_df = run_preprocess(\n","    raw_dir=RAW_DIR,\n","    proc_dir=PROC_DIR,\n","    splits_dir=SPLITS_DIR,\n","    report_dir=REPORT_DIR,\n","    seed=CFG[\"SEED\"],\n","    val_ratio=CFG[\"VAL_RATIO\"],\n","    sarc_source_only=CFG[\"SARC_SOURCE_ONLY\"],\n","    max_len_for_models=CFG[\"MAX_LEN_FOR_MODELS\"]\n",")\n","\n","print(\"Preprocess done. Rows in index:\", len(index_df))\n","index_df.head(50)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":490},"id":"Si0_pGdeIZ9b","executionInfo":{"status":"ok","timestamp":1769344094042,"user_tz":-60,"elapsed":2555,"user":{"displayName":"DNLP Project","userId":"02984253218627728392"}},"outputId":"b804e132-7c89-45ea-abf4-906a4f9397df"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Cleaning old processed data...\n","Preprocess done. Rows in index: 9\n"]},{"output_type":"execute_result","data":{"text/plain":["        task      setting                                          train_csv  \\\n","0    sarcasm         FULL  /content/drive/MyDrive/DNLP/data/processed/sar...   \n","1    sarcasm  TRAIN_en-AU  /content/drive/MyDrive/DNLP/data/processed/sar...   \n","2    sarcasm  TRAIN_en-IN  /content/drive/MyDrive/DNLP/data/processed/sar...   \n","3    sarcasm  TRAIN_en-UK  /content/drive/MyDrive/DNLP/data/processed/sar...   \n","4  sentiment       Google  /content/drive/MyDrive/DNLP/data/processed/sen...   \n","5  sentiment       Reddit  /content/drive/MyDrive/DNLP/data/processed/sen...   \n","6  sentiment  TRAIN_en-AU  /content/drive/MyDrive/DNLP/data/processed/sen...   \n","7  sentiment  TRAIN_en-IN  /content/drive/MyDrive/DNLP/data/processed/sen...   \n","8  sentiment  TRAIN_en-UK  /content/drive/MyDrive/DNLP/data/processed/sen...   \n","\n","                                             val_csv  \\\n","0  /content/drive/MyDrive/DNLP/data/processed/sar...   \n","1  /content/drive/MyDrive/DNLP/data/processed/sar...   \n","2  /content/drive/MyDrive/DNLP/data/processed/sar...   \n","3  /content/drive/MyDrive/DNLP/data/processed/sar...   \n","4  /content/drive/MyDrive/DNLP/data/processed/sen...   \n","5  /content/drive/MyDrive/DNLP/data/processed/sen...   \n","6  /content/drive/MyDrive/DNLP/data/processed/sen...   \n","7  /content/drive/MyDrive/DNLP/data/processed/sen...   \n","8  /content/drive/MyDrive/DNLP/data/processed/sen...   \n","\n","                                         splits_json  \\\n","0  /content/drive/MyDrive/DNLP/data/splits/sarcas...   \n","1  /content/drive/MyDrive/DNLP/data/splits/sarcas...   \n","2  /content/drive/MyDrive/DNLP/data/splits/sarcas...   \n","3  /content/drive/MyDrive/DNLP/data/splits/sarcas...   \n","4  /content/drive/MyDrive/DNLP/data/splits/sentim...   \n","5  /content/drive/MyDrive/DNLP/data/splits/sentim...   \n","6  /content/drive/MyDrive/DNLP/data/splits/sentim...   \n","7  /content/drive/MyDrive/DNLP/data/splits/sentim...   \n","8  /content/drive/MyDrive/DNLP/data/splits/sentim...   \n","\n","                                  testsets_index_csv  n_train  n_val  \n","0  /content/drive/MyDrive/DNLP/data/processed/sar...     3585    895  \n","1  /content/drive/MyDrive/DNLP/data/processed/sar...     1411    352  \n","2  /content/drive/MyDrive/DNLP/data/processed/sar...     1349    337  \n","3  /content/drive/MyDrive/DNLP/data/processed/sar...      825    206  \n","4  /content/drive/MyDrive/DNLP/data/processed/sen...     3529    882  \n","5  /content/drive/MyDrive/DNLP/data/processed/sen...     3564    891  \n","6  /content/drive/MyDrive/DNLP/data/processed/sen...     2167    542  \n","7  /content/drive/MyDrive/DNLP/data/processed/sen...     2667    666  \n","8  /content/drive/MyDrive/DNLP/data/processed/sen...     2259    565  "],"text/html":["\n","  <div id=\"df-02487383-bc53-4228-b8c5-8c4d685384cb\" class=\"colab-df-container\">\n","    <div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>task</th>\n","      <th>setting</th>\n","      <th>train_csv</th>\n","      <th>val_csv</th>\n","      <th>splits_json</th>\n","      <th>testsets_index_csv</th>\n","      <th>n_train</th>\n","      <th>n_val</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>sarcasm</td>\n","      <td>FULL</td>\n","      <td>/content/drive/MyDrive/DNLP/data/processed/sar...</td>\n","      <td>/content/drive/MyDrive/DNLP/data/processed/sar...</td>\n","      <td>/content/drive/MyDrive/DNLP/data/splits/sarcas...</td>\n","      <td>/content/drive/MyDrive/DNLP/data/processed/sar...</td>\n","      <td>3585</td>\n","      <td>895</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>sarcasm</td>\n","      <td>TRAIN_en-AU</td>\n","      <td>/content/drive/MyDrive/DNLP/data/processed/sar...</td>\n","      <td>/content/drive/MyDrive/DNLP/data/processed/sar...</td>\n","      <td>/content/drive/MyDrive/DNLP/data/splits/sarcas...</td>\n","      <td>/content/drive/MyDrive/DNLP/data/processed/sar...</td>\n","      <td>1411</td>\n","      <td>352</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>sarcasm</td>\n","      <td>TRAIN_en-IN</td>\n","      <td>/content/drive/MyDrive/DNLP/data/processed/sar...</td>\n","      <td>/content/drive/MyDrive/DNLP/data/processed/sar...</td>\n","      <td>/content/drive/MyDrive/DNLP/data/splits/sarcas...</td>\n","      <td>/content/drive/MyDrive/DNLP/data/processed/sar...</td>\n","      <td>1349</td>\n","      <td>337</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>sarcasm</td>\n","      <td>TRAIN_en-UK</td>\n","      <td>/content/drive/MyDrive/DNLP/data/processed/sar...</td>\n","      <td>/content/drive/MyDrive/DNLP/data/processed/sar...</td>\n","      <td>/content/drive/MyDrive/DNLP/data/splits/sarcas...</td>\n","      <td>/content/drive/MyDrive/DNLP/data/processed/sar...</td>\n","      <td>825</td>\n","      <td>206</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>sentiment</td>\n","      <td>Google</td>\n","      <td>/content/drive/MyDrive/DNLP/data/processed/sen...</td>\n","      <td>/content/drive/MyDrive/DNLP/data/processed/sen...</td>\n","      <td>/content/drive/MyDrive/DNLP/data/splits/sentim...</td>\n","      <td>/content/drive/MyDrive/DNLP/data/processed/sen...</td>\n","      <td>3529</td>\n","      <td>882</td>\n","    </tr>\n","    <tr>\n","      <th>5</th>\n","      <td>sentiment</td>\n","      <td>Reddit</td>\n","      <td>/content/drive/MyDrive/DNLP/data/processed/sen...</td>\n","      <td>/content/drive/MyDrive/DNLP/data/processed/sen...</td>\n","      <td>/content/drive/MyDrive/DNLP/data/splits/sentim...</td>\n","      <td>/content/drive/MyDrive/DNLP/data/processed/sen...</td>\n","      <td>3564</td>\n","      <td>891</td>\n","    </tr>\n","    <tr>\n","      <th>6</th>\n","      <td>sentiment</td>\n","      <td>TRAIN_en-AU</td>\n","      <td>/content/drive/MyDrive/DNLP/data/processed/sen...</td>\n","      <td>/content/drive/MyDrive/DNLP/data/processed/sen...</td>\n","      <td>/content/drive/MyDrive/DNLP/data/splits/sentim...</td>\n","      <td>/content/drive/MyDrive/DNLP/data/processed/sen...</td>\n","      <td>2167</td>\n","      <td>542</td>\n","    </tr>\n","    <tr>\n","      <th>7</th>\n","      <td>sentiment</td>\n","      <td>TRAIN_en-IN</td>\n","      <td>/content/drive/MyDrive/DNLP/data/processed/sen...</td>\n","      <td>/content/drive/MyDrive/DNLP/data/processed/sen...</td>\n","      <td>/content/drive/MyDrive/DNLP/data/splits/sentim...</td>\n","      <td>/content/drive/MyDrive/DNLP/data/processed/sen...</td>\n","      <td>2667</td>\n","      <td>666</td>\n","    </tr>\n","    <tr>\n","      <th>8</th>\n","      <td>sentiment</td>\n","      <td>TRAIN_en-UK</td>\n","      <td>/content/drive/MyDrive/DNLP/data/processed/sen...</td>\n","      <td>/content/drive/MyDrive/DNLP/data/processed/sen...</td>\n","      <td>/content/drive/MyDrive/DNLP/data/splits/sentim...</td>\n","      <td>/content/drive/MyDrive/DNLP/data/processed/sen...</td>\n","      <td>2259</td>\n","      <td>565</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>\n","    <div class=\"colab-df-buttons\">\n","\n","  <div class=\"colab-df-container\">\n","    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-02487383-bc53-4228-b8c5-8c4d685384cb')\"\n","            title=\"Convert this dataframe to an interactive table.\"\n","            style=\"display:none;\">\n","\n","  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n","    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n","  </svg>\n","    </button>\n","\n","  <style>\n","    .colab-df-container {\n","      display:flex;\n","      gap: 12px;\n","    }\n","\n","    .colab-df-convert {\n","      background-color: #E8F0FE;\n","      border: none;\n","      border-radius: 50%;\n","      cursor: pointer;\n","      display: none;\n","      fill: #1967D2;\n","      height: 32px;\n","      padding: 0 0 0 0;\n","      width: 32px;\n","    }\n","\n","    .colab-df-convert:hover {\n","      background-color: #E2EBFA;\n","      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n","      fill: #174EA6;\n","    }\n","\n","    .colab-df-buttons div {\n","      margin-bottom: 4px;\n","    }\n","\n","    [theme=dark] .colab-df-convert {\n","      background-color: #3B4455;\n","      fill: #D2E3FC;\n","    }\n","\n","    [theme=dark] .colab-df-convert:hover {\n","      background-color: #434B5C;\n","      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n","      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n","      fill: #FFFFFF;\n","    }\n","  </style>\n","\n","    <script>\n","      const buttonEl =\n","        document.querySelector('#df-02487383-bc53-4228-b8c5-8c4d685384cb button.colab-df-convert');\n","      buttonEl.style.display =\n","        google.colab.kernel.accessAllowed ? 'block' : 'none';\n","\n","      async function convertToInteractive(key) {\n","        const element = document.querySelector('#df-02487383-bc53-4228-b8c5-8c4d685384cb');\n","        const dataTable =\n","          await google.colab.kernel.invokeFunction('convertToInteractive',\n","                                                    [key], {});\n","        if (!dataTable) return;\n","\n","        const docLinkHtml = 'Like what you see? Visit the ' +\n","          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n","          + ' to learn more about interactive tables.';\n","        element.innerHTML = '';\n","        dataTable['output_type'] = 'display_data';\n","        await google.colab.output.renderOutput(dataTable, element);\n","        const docLink = document.createElement('div');\n","        docLink.innerHTML = docLinkHtml;\n","        element.appendChild(docLink);\n","      }\n","    </script>\n","  </div>\n","\n","\n","    </div>\n","  </div>\n"],"application/vnd.google.colaboratory.intrinsic+json":{"type":"dataframe","variable_name":"index_df","summary":"{\n  \"name\": \"index_df\",\n  \"rows\": 9,\n  \"fields\": [\n    {\n      \"column\": \"task\",\n      \"properties\": {\n        \"dtype\": \"category\",\n        \"num_unique_values\": 2,\n        \"samples\": [\n          \"sentiment\",\n          \"sarcasm\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"setting\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 6,\n        \"samples\": [\n          \"FULL\",\n          \"TRAIN_en-AU\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"train_csv\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 9,\n        \"samples\": [\n          \"/content/drive/MyDrive/DNLP/data/processed/sentiment/trainsets/train_en-in/train.csv\",\n          \"/content/drive/MyDrive/DNLP/data/processed/sarcasm/trainsets/train_en-au/train.csv\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"val_csv\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 9,\n        \"samples\": [\n          \"/content/drive/MyDrive/DNLP/data/processed/sentiment/trainsets/train_en-in/val.csv\",\n          \"/content/drive/MyDrive/DNLP/data/processed/sarcasm/trainsets/train_en-au/val.csv\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"splits_json\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 9,\n        \"samples\": [\n          \"/content/drive/MyDrive/DNLP/data/splits/sentiment__train_en-in.json\",\n          \"/content/drive/MyDrive/DNLP/data/splits/sarcasm__train_en-au.json\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"testsets_index_csv\",\n      \"properties\": {\n        \"dtype\": \"category\",\n        \"num_unique_values\": 2,\n        \"samples\": [\n          \"/content/drive/MyDrive/DNLP/data/processed/sentiment/testsets/index_testsets.csv\",\n          \"/content/drive/MyDrive/DNLP/data/processed/sarcasm/testsets/index_testsets.csv\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"n_train\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 1044,\n        \"min\": 825,\n        \"max\": 3585,\n        \"num_unique_values\": 9,\n        \"samples\": [\n          2667,\n          1411\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"n_val\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 261,\n        \"min\": 206,\n        \"max\": 895,\n        \"num_unique_values\": 9,\n        \"samples\": [\n          666,\n          352\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    }\n  ]\n}"}},"metadata":{},"execution_count":6}]},{"cell_type":"markdown","source":["## Cell 13 — Inspect the master index\n","\n","Quick view of all (task, setting) combinations created, along with train/val sizes.\n","This is the table your model notebooks should read to load the correct files."],"metadata":{"id":"KHQ5SZo03YEz"}},{"cell_type":"code","source":["pd.set_option(\"display.max_rows\", 200)\n","index_df[[\"task\",\"setting\",\"n_train\",\"n_val\"]]"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":331},"id":"oAnUY9kuI2Ed","executionInfo":{"status":"ok","timestamp":1769344096640,"user_tz":-60,"elapsed":23,"user":{"displayName":"DNLP Project","userId":"02984253218627728392"}},"outputId":"daee2d11-b081-4f8e-c01c-631e93b8b0dd"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["        task      setting  n_train  n_val\n","0    sarcasm         FULL     3585    895\n","1    sarcasm  TRAIN_en-AU     1411    352\n","2    sarcasm  TRAIN_en-IN     1349    337\n","3    sarcasm  TRAIN_en-UK      825    206\n","4  sentiment       Google     3529    882\n","5  sentiment       Reddit     3564    891\n","6  sentiment  TRAIN_en-AU     2167    542\n","7  sentiment  TRAIN_en-IN     2667    666\n","8  sentiment  TRAIN_en-UK     2259    565"],"text/html":["\n","  <div id=\"df-e2481d16-c909-4c14-80a6-051c67186b25\" class=\"colab-df-container\">\n","    <div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>task</th>\n","      <th>setting</th>\n","      <th>n_train</th>\n","      <th>n_val</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>sarcasm</td>\n","      <td>FULL</td>\n","      <td>3585</td>\n","      <td>895</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>sarcasm</td>\n","      <td>TRAIN_en-AU</td>\n","      <td>1411</td>\n","      <td>352</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>sarcasm</td>\n","      <td>TRAIN_en-IN</td>\n","      <td>1349</td>\n","      <td>337</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>sarcasm</td>\n","      <td>TRAIN_en-UK</td>\n","      <td>825</td>\n","      <td>206</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>sentiment</td>\n","      <td>Google</td>\n","      <td>3529</td>\n","      <td>882</td>\n","    </tr>\n","    <tr>\n","      <th>5</th>\n","      <td>sentiment</td>\n","      <td>Reddit</td>\n","      <td>3564</td>\n","      <td>891</td>\n","    </tr>\n","    <tr>\n","      <th>6</th>\n","      <td>sentiment</td>\n","      <td>TRAIN_en-AU</td>\n","      <td>2167</td>\n","      <td>542</td>\n","    </tr>\n","    <tr>\n","      <th>7</th>\n","      <td>sentiment</td>\n","      <td>TRAIN_en-IN</td>\n","      <td>2667</td>\n","      <td>666</td>\n","    </tr>\n","    <tr>\n","      <th>8</th>\n","      <td>sentiment</td>\n","      <td>TRAIN_en-UK</td>\n","      <td>2259</td>\n","      <td>565</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>\n","    <div class=\"colab-df-buttons\">\n","\n","  <div class=\"colab-df-container\">\n","    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-e2481d16-c909-4c14-80a6-051c67186b25')\"\n","            title=\"Convert this dataframe to an interactive table.\"\n","            style=\"display:none;\">\n","\n","  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n","    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n","  </svg>\n","    </button>\n","\n","  <style>\n","    .colab-df-container {\n","      display:flex;\n","      gap: 12px;\n","    }\n","\n","    .colab-df-convert {\n","      background-color: #E8F0FE;\n","      border: none;\n","      border-radius: 50%;\n","      cursor: pointer;\n","      display: none;\n","      fill: #1967D2;\n","      height: 32px;\n","      padding: 0 0 0 0;\n","      width: 32px;\n","    }\n","\n","    .colab-df-convert:hover {\n","      background-color: #E2EBFA;\n","      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n","      fill: #174EA6;\n","    }\n","\n","    .colab-df-buttons div {\n","      margin-bottom: 4px;\n","    }\n","\n","    [theme=dark] .colab-df-convert {\n","      background-color: #3B4455;\n","      fill: #D2E3FC;\n","    }\n","\n","    [theme=dark] .colab-df-convert:hover {\n","      background-color: #434B5C;\n","      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n","      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n","      fill: #FFFFFF;\n","    }\n","  </style>\n","\n","    <script>\n","      const buttonEl =\n","        document.querySelector('#df-e2481d16-c909-4c14-80a6-051c67186b25 button.colab-df-convert');\n","      buttonEl.style.display =\n","        google.colab.kernel.accessAllowed ? 'block' : 'none';\n","\n","      async function convertToInteractive(key) {\n","        const element = document.querySelector('#df-e2481d16-c909-4c14-80a6-051c67186b25');\n","        const dataTable =\n","          await google.colab.kernel.invokeFunction('convertToInteractive',\n","                                                    [key], {});\n","        if (!dataTable) return;\n","\n","        const docLinkHtml = 'Like what you see? Visit the ' +\n","          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n","          + ' to learn more about interactive tables.';\n","        element.innerHTML = '';\n","        dataTable['output_type'] = 'display_data';\n","        await google.colab.output.renderOutput(dataTable, element);\n","        const docLink = document.createElement('div');\n","        docLink.innerHTML = docLinkHtml;\n","        element.appendChild(docLink);\n","      }\n","    </script>\n","  </div>\n","\n","\n","    </div>\n","  </div>\n"],"application/vnd.google.colaboratory.intrinsic+json":{"type":"dataframe","summary":"{\n  \"name\": \"index_df[[\\\"task\\\",\\\"setting\\\",\\\"n_train\\\",\\\"n_val\\\"]]\",\n  \"rows\": 9,\n  \"fields\": [\n    {\n      \"column\": \"task\",\n      \"properties\": {\n        \"dtype\": \"category\",\n        \"num_unique_values\": 2,\n        \"samples\": [\n          \"sentiment\",\n          \"sarcasm\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"setting\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 6,\n        \"samples\": [\n          \"FULL\",\n          \"TRAIN_en-AU\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"n_train\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 1044,\n        \"min\": 825,\n        \"max\": 3585,\n        \"num_unique_values\": 9,\n        \"samples\": [\n          2667,\n          1411\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"n_val\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 261,\n        \"min\": 206,\n        \"max\": 895,\n        \"num_unique_values\": 9,\n        \"samples\": [\n          666,\n          352\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    }\n  ]\n}"}},"metadata":{},"execution_count":7}]}]}